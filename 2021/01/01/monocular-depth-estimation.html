<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Unsupervised Learning of Depth and Ego-Motion from Video | Isaac Kargar</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Unsupervised Learning of Depth and Ego-Motion from Video" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My posts about Machine Learning" />
<meta property="og:description" content="My posts about Machine Learning" />
<link rel="canonical" href="https://kargarisaac.github.io/blog/2021/01/01/monocular-depth-estimation.html" />
<meta property="og:url" content="https://kargarisaac.github.io/blog/2021/01/01/monocular-depth-estimation.html" />
<meta property="og:site_name" content="Isaac Kargar" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-01T00:00:00-06:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kargarisaac.github.io/blog/2021/01/01/monocular-depth-estimation.html"},"description":"My posts about Machine Learning","@type":"BlogPosting","url":"https://kargarisaac.github.io/blog/2021/01/01/monocular-depth-estimation.html","headline":"Unsupervised Learning of Depth and Ego-Motion from Video","dateModified":"2021-01-01T00:00:00-06:00","datePublished":"2021-01-01T00:00:00-06:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://kargarisaac.github.io/blog/feed.xml" title="Isaac Kargar" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','G-7C8WW0BBJ4','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Unsupervised Learning of Depth and Ego-Motion from Video | Isaac Kargar</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Unsupervised Learning of Depth and Ego-Motion from Video" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My posts about Machine Learning" />
<meta property="og:description" content="My posts about Machine Learning" />
<link rel="canonical" href="https://kargarisaac.github.io/blog/2021/01/01/monocular-depth-estimation.html" />
<meta property="og:url" content="https://kargarisaac.github.io/blog/2021/01/01/monocular-depth-estimation.html" />
<meta property="og:site_name" content="Isaac Kargar" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-01T00:00:00-06:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kargarisaac.github.io/blog/2021/01/01/monocular-depth-estimation.html"},"description":"My posts about Machine Learning","@type":"BlogPosting","url":"https://kargarisaac.github.io/blog/2021/01/01/monocular-depth-estimation.html","headline":"Unsupervised Learning of Depth and Ego-Motion from Video","dateModified":"2021-01-01T00:00:00-06:00","datePublished":"2021-01-01T00:00:00-06:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://kargarisaac.github.io/blog/feed.xml" title="Isaac Kargar" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','G-7C8WW0BBJ4','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Isaac Kargar</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Unsupervised Learning of Depth and Ego-Motion from Video</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-01-01T00:00:00-06:00" itemprop="datePublished">
        Jan 1, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/kargarisaac/blog/tree/master/_notebooks/2021-01-01-monocular-depth-estimation.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/kargarisaac/blog/master?filepath=_notebooks%2F2021-01-01-monocular-depth-estimation.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/kargarisaac/blog/blob/master/_notebooks/2021-01-01-monocular-depth-estimation.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-01-01-monocular-depth-estimation.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>"Self-Supervised Monocular Depth Estimation in Autonomous Driving""</p>
<blockquote><p>""</p>
</blockquote>
<ul>
<li>toc:True- branch: master</li>
<li>badges: true</li>
<li>comments: true</li>
<li>categories: [deep learning, autonomous driving, jupyter]</li>
<li>image: images/some_folder/your_image.png</li>
<li>hide: false</li>
<li>search_exclude: true</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is easy for humans to estimate depth in a scene, but what about machines? Typically, robots and self-driving cars use LiDAR sensors to gauge the depth of a scene. However, LiDAR is an expensive sensor that is beyond the reach of many personal vehicles. Robo-Taxis may be reasonable in business models that provide service across a city, but not for personal vehicles. As a result, some companies are using camera-only approaches to infer depth information from monocular images. I will discuss some of the state-of-the-art approaches for monocular depth estimation in this post.</p>
<p><a href="https://arxiv.org/pdf/2003.06620.pdf">Several approaches are usually used for depth estimation</a>:</p>
<blockquote><ul>
<li>Geometry-based methods:Geometric constraints are used to recover 3D structures from images. In 3D reconstruction and Simultaneous Localization and Mapping (SLAM) , structure from motion (SfM) is an effective method of estimating 3D structures from a series of 2D image sequences. The accuracy of depth estimation depends heavily on exact feature matching and high-quality image sequences. SfM suffers from monocular scale ambiguity as well. Stereo vision matching is also capable of recovering 3D structures of scenes from two viewpoints. It simulates the way human eyes work by using two cameras, and a cost function is used to calculate disparity maps of images. Due to the calibration of the transformation between the two cameras, the scale factor is incorporated into depth estimation during stereo vision matching.&gt; - Sensor-based methods:This approach uses sensors such as RGB-D and LiDAR. There are sevaral disadvantages for this method such as cost, power consumption, size of sensor.&gt; - Deep learning-based methods:The pixel-level depth map can be recovered from a single image in an end-to-end manner based on deep learning. It can be done in supervised, semi-supervised, or self-supervised manner.
In this post, we just consider the self-supervised methods in which the geometric constraints between frames are regarded as the supervisory signal during the training process. There are several types of self-supervised learning methods for estimating depth using images, such as stereo-based and monocular videos. Using methods based on monocular videos presents its own challenges. Along with estimating depth, the model also requires estimating ego-motion between pairs of temporal images during training. The process involves training a pose estimation network, which takes a finite sequence of frames as input and outputs the corresponding camera transformations. Stereo data, however, make the camera-pose estimation a one-time offline calibration but may introduce occlusion and texture-copy artifacts.</li>
</ul>
</blockquote>
<p>One of the interesting use cases for depth estimation is to use it as an auxiliary task for end-to-end policy learning. It can lead to better representation learning and help the policy to learn some information about the geometric and the depth of the scene. Other tasks, such as optical flow, semantic segmentation, object detection, motion prediction, etc, can also be used to improve representation learning. For example, the following image shows a model from Wayve.ai, a self-driving car company in the UK working on end-to-end autonomous driving, which tries to use multi-task learning to improve representation learning and driving policy learning.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/1.png" alt="" />
<em><a href="https://wayve.ai/blog/driving-intelligence-with-end-to-end-deep-learning/">source</a></em></p>
<p>There are also some works that try to learn multiple tasks jointly. Sometimes there is some information in other related tasks that help the networks to learn better. For example, the following work tries to learn optical flow, motion segmentation, camera motion estimation, and depth estimation together:</p>
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/2.png" alt="" />
<em><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Ranjan_Competitive_Collaboration_Joint_Unsupervised_Learning_of_Depth_Camera_Motion_Optical_CVPR_2019_paper.pdf">source</a></em></p>
<p>Or the following work that tries to learn optical flow and depth together:</p>
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/3.png" alt="" />
<em><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper.pdf">source</a></em></p>
<p>Here in this post, we just want to understand how depth estimation works, and then it would be more straightforward to mix it with other tasks. Maybe in the future, I write other blog posts on other techniques and tasks. In the rest of this post, I will review two related papers for self-supervised monocular depth estimation which use monocular videos and stereo images methods. Let’s get started!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>They use the view synthesis idea as the supervision signal for depth and pose prediction CNNs: given one input view of a scene, synthesize a new image of the scene seen from a different camera pose. This synthesis process can be implemented in a fully differentiable manner with CNNs as the geometry and pose estimation modules.</p>
<p>Let’s consider &lt;I_1, …, I_N&gt; as a training image sequence with one of the frames I_t as the target view and the rest as the source views I_s(1 ≤ s ≤ N, s≠t). The view synthesis objective can be formulated as:</p>
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/7.png" alt="" />
<em><a href="https://arxiv.org/pdf/1704.07813.pdf">source</a></em></p>
<p>where p indexes over pixel coordinates and Iˆ_s is the source view I_s warped to the target coordinate frame based on a depth image-based rendering module (described in the following), taking the predicted depth Dˆ<em>t, the predicted 4×4 camera transformation matrix1 Tˆ</em>(t→s) and the source view I_s as input.</p>
<p>The differentiable depth image-based renderer reconstructs the target view I_t by sampling pixels from a source view I_s based on the predicted depth map Dˆ<em>t and the relative pose Tˆ</em>(t→s).</p>
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/8.png" alt="" />
<em><a href="https://arxiv.org/pdf/1704.07813.pdf">source</a></em></p>
<p>Let p_t denote the homogeneous coordinates of a pixel in the target view, and K denote the camera intrinsics matrix. The p_t’s projected coordinates onto the source view p_s (which is a continuous value) can be obtained by:</p>
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/9.png" alt="" />
<em><a href="https://arxiv.org/pdf/1704.07813.pdf">source</a></em></p>
<p>Then a differentiable bilinear sampling mechanism is used to linearly interpolate the values of the 4-pixel neighbors (top-left, top-right, bottom-left, and bottom-right) of p_s to approximate I_s(p_s).</p>
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/10.png" alt="" />
<em><a href="https://arxiv.org/pdf/1704.07813.pdf">source</a></em></p>
<p>where w^ij is linearly proportional to the spatial proximity between p_s and p^ij_s, and Sum(w^ij)=1.</p>
<p>The above view synthesis formulation implicitly assumes 1) the scene is static without moving objects; 2) there is no occlusion/disocclusion between the target view and the source views; 3) the surface is Lambertian so that the photo-consistency error is meaningful. To improve the robustness of the learning pipeline to these factors, an explainability prediction network (jointly and simultaneously with the depth and pose networks) is trained that outputs a per-pixel soft mask Eˆ_s for each target-source pair, indicating the network’s belief in where direct view synthesis will be successfully modeled for each target pixel.</p>
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/11.png" alt="" />
<em><a href="https://arxiv.org/pdf/1704.07813.pdf">source</a></em></p>
<p>Since there is no direct supervision for Eˆ_s, training with the above loss would result in predicting Eˆ_s to be zero. To prevent this, a regularization term L_reg(Eˆ_s) is considered that encourages nonzero predictions.</p>
<p>The other issue is that the gradients are mainly derived from the pixel intensity difference between I(p_t) and the four neighbors of I(p_s), which would inhibit training if the correct p_s (projected using the ground-truth depth and pose) is located in a low-texture region or far from the current estimation. To alleviate this problem, they use an explicit multi-scale and smoothness loss (the L1 norm of the second-order gradients for the predicted depth maps) that allows gradients to be derived from larger spatial regions directly.</p>
<p>The final loss function they used for training is as follows:</p>
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/12.png" alt="" />
<em><a href="https://arxiv.org/pdf/1704.07813.pdf">source</a></em></p>
<p>And finally, the results are as follows:</p>
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/13.png" alt="" />
<em><a href="https://arxiv.org/pdf/1704.07813.pdf">source</a></em></p>
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/14.png" alt="" />
<em><a href="https://arxiv.org/pdf/1704.07813.pdf">source</a></em></p>
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/1.gif" alt="" />
<em><a href="https://github.com/tinghuiz/SfMLearner">source</a></em></p>
<p>paper: <a href="https://arxiv.org/pdf/1704.07813.pdf">https://arxiv.org/pdf/1704.07813.pdf</a></p>
<p>code: <a href="https://github.com/tinghuiz/SfMLearner">https://github.com/tinghuiz/SfMLearner</a></p>
<p>Presentation at NeurIPS 2017:

<center>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/HWu39YkGKvI" frameborder="0" allowfullscreen=""></iframe>
</center>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Digging-Into-Self-Supervised-Monocular-Depth-Estimation">Digging Into Self-Supervised Monocular Depth Estimation<a class="anchor-link" href="#Digging-Into-Self-Supervised-Monocular-Depth-Estimation"> </a></h1><p>This paper uses both monocular videos and stereo pairs for depth estimation.</p>
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/15.png" alt="" />
<em><a href="https://arxiv.org/pdf/1806.01260.pdf">source</a></em></p>
<p>The problem is formulated as the minimization of a photometric reprojection error at training time. The relative pose for each source view I_t’, with respect to the target image I<em>t’s pose, is shown as T</em>(t→t’). Then a dense depth map D_t is predicted in which it minimizes the photometric reprojection error L_p:</p>
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/16.png" alt="" />
<em><a href="https://arxiv.org/pdf/1806.01260.pdf">source</a></em></p>
<p>Here pe is a photometric reconstruction error, e.g. the L1 distance in pixel space; proj() is the resulting 2D coordinates of the projected depths D_t in I_t’ and &lt;&gt; is the sampling operator. Bilinear sampling is used to sample the source images, which is locally sub-differentiable, and L1 and SSIM are used to make the photometric error function pe:</p>
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/17.png" alt="" />
<em><a href="https://arxiv.org/pdf/1806.01260.pdf">source</a></em></p>
<p>where α = 0.85. They also used edge-aware smoothness:</p>
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/18.png" alt="" />
<em><a href="https://arxiv.org/pdf/1806.01260.pdf">source</a></em></p>
<p>where $d*_t$ is the mean-normalized inverse depth to discourage shrinking of the estimated depth.</p>
<p>In stereo training, the source image I_t’ is the second view in the stereo pair to I_t, which has a known relative pose. But for monocular sequences, this pose is not known and a neural network is trained jointly with the depth estimation network to minimize L<em>p and to do pose estimation, T</em>(t→t’).</p>
<p>For monocular training, two frames temporally adjacent to I_t are used as source frames, i.e. I<em>t’ ∈ {I</em>(t−1), I_(t+1)}. In mixed training (MS), I_t’ includes the temporally adjacent frames and the opposite stereo view.</p>
<p>The image below shows the overview of the proposed approach and in this paper.</p>
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/19.png" alt="" />
<em><a href="https://arxiv.org/pdf/1806.01260.pdf">source</a></em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>They also propose three architectural and loss innovations which, when combined, lead to large improvements in monocular depth estimation when training with monocular video, stereo pairs, or both:</p>
<ul>
<li><p>A novel appearance matching loss to address the problem of occluded pixels that occur when using monocular supervision.</p>
</li>
<li><p>A novel and simple auto-masking approach to ignore pixels where no relative camera motion is observed in monocular training.</p>
</li>
<li><p>A multi-scale appearance matching loss that performs all image sampling at the input resolution, leading to a reduction in depth artifacts</p>
</li>
</ul>
<p>We just explained the main part of the method. To read more about the details of the above innovations, please read the paper. The final loss function is as follows:</p>
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/20.png" alt="" />
<em><a href="https://arxiv.org/pdf/1806.01260.pdf">source</a></em></p>
<p>And the results are as follows:</p>
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/21.png" alt="" />
<em><a href="https://arxiv.org/pdf/1806.01260.pdf">source</a></em></p>
<p><img src="/blog/images/copied_from_nb/images/monocular-depth/2.gif" alt="" />
<em><a href="https://github.com/nianticlabs/monodepth2">source</a></em></p>
<p>paper: <a href="https://arxiv.org/pdf/1806.01260.pdf">https://arxiv.org/pdf/1806.01260.pdf</a></p>
<p>code: <a href="https://github.com/nianticlabs/monodepth2">https://github.com/nianticlabs/monodepth2</a></p>
<p>video:

<center>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/sIN1Tp3wIbQ" frameborder="0" allowfullscreen=""></iframe>
</center>
</p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/blog/2021/01/01/monocular-depth-estimation.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My posts about Machine Learning</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/kargarisaac" target="_blank" title="kargarisaac"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/eshagh-kargar" target="_blank" title="eshagh-kargar"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/kargarisaac" target="_blank" title="kargarisaac"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
