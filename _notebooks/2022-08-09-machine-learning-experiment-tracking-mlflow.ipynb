{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd6c6e5c",
   "metadata": {},
   "source": [
    "# \"Machine Learning Experiment Tracking Using MLflow\"\n",
    "> \"Experiment tracking in machine learning model development.\"\n",
    "\n",
    "- toc: True\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [mlops]\n",
    "- image: images/some_folder/your_image.png\n",
    "- hide: false\n",
    "- search_exclude: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9eeb1",
   "metadata": {},
   "source": [
    "In this series of blog posts, I will describe the entire procedure for developing a machine learning service, from development to deployment to monitoring. This is the final project for the phenomenal [MLOps zoomcamp course](https://www.youtube.com/playlist?list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK). This series aims to train a simple sentiment analysis model, deploy it to Google Cloud, and utilize MLOps as much as possible. The program receives an E-commerce clothing review and forecasts if the customer would recommend the product to her friends. You can find the dataset [here](https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews).\n",
    "\n",
    "We do not focus that much on the model development here. I use [Kaggle](https://www.kaggle.com/code/granjithkumar/nlp-with-women-clothing-reviews) notebook as the reference and will try to do experiment tracking using MLflow. You can check the main notebook for further data preprocessing and visualizations.\n",
    "\n",
    "So, what is experiment tracking? Experiment tracking is the process of keeping track of all the relevant information from an ML experiment, which includes:\n",
    "\n",
    "- source code\n",
    "- environment\n",
    "- data\n",
    "- model\n",
    "- hyperparameters\n",
    "- metrics\n",
    "- ...\n",
    "\n",
    "And why experiment tracking is important?\n",
    "\n",
    "- reproducibilty\n",
    "- organization\n",
    "- optimization\n",
    "\n",
    "MLflow is an open source platform for managing the end-to-end machine learning lifecycle, and we will use it here. It tackles four primary functions:[[source](https://www.mlflow.org/docs/latest/index.html)]\n",
    "\n",
    "- Tracking experiments to record and compare parameters and results (MLflow Tracking). The MLflow Tracking component is an API and UI for logging parameters, code versions, metrics, and output files when running your machine learning code and for later visualizing the results\n",
    "\n",
    "- Packaging ML code in a reusable, reproducible form in order to share with other data scientists or transfer to production (MLflow Projects). \n",
    "\n",
    "- Managing and deploying models from a variety of ML libraries to a variety of model serving and inference platforms (MLflow Models).\n",
    "\n",
    "- Providing a central model store to collaboratively manage the full lifecycle of an MLflow Model, including model versioning, stage transitions, and annotations (MLflow Model Registry).\n",
    "\n",
    "You can learn more about MLflow by reading their documentations and also watching the videos from the MLOps zoomcamp course. \n",
    "\n",
    "\n",
    "> youtube: https://youtu.be/MiA7LQin9c8\n",
    "\n",
    "\n",
    "\n",
    "In this post, we train three different ML models for the sentiment analysis task: Bag of Words, TF-IDF, and a simple neural network model.\n",
    "\n",
    "First you need to install MLflow using pip. You can check more [here](https://www.mlflow.org/docs/latest/quickstart.html).\n",
    "\n",
    "You can then run the MLflow UI using:\n",
    "\n",
    "```bash\n",
    "mlflow ui --backend-store-uri sqlite:///mlflow.db\n",
    "```\n",
    "\n",
    "You can then see the UI in this address: `http://127.0.0.1:5000/`\n",
    "\n",
    "> youtube: https://youtu.be/cESCQE9J3ZE\n",
    "\n",
    "\n",
    "It is also possible to run MLflow on Cloud to store models and metadata there. I wrote a blog post about how to set up MLflow on GCP. You can read it [here](https://kargarisaac.github.io/blog/mlops/jupyter/2022/06/15/MLFlow-on-GCP.html).\n",
    "\n",
    "For hyperparameter optimization you can use [`hyperopt`](http://hyperopt.github.io/hyperopt/) library. Currently three algorithms are implemented in hyperopt:\n",
    "\n",
    "- Random Search\n",
    "- Tree of Parzen Estimators (TPE)\n",
    "- Adaptive TPE\n",
    "\n",
    "Hyperopt has been designed to accommodate Bayesian optimization algorithms based on Gaussian processes and regression trees, but these are not currently implemented.\n",
    "\n",
    "We don't have that much parameters here, so we don't use `hyperopt` but you can check it out. \n",
    "Let's start with the BoW model:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d273f",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "os.environ[\"NLTK_DATA\"] = \"./corpora\"\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"customer-sentiment-analysis\")\n",
    "\n",
    "## data loading\n",
    "data = pd.read_csv('Womens Clothing E-Commerce Reviews.csv',index_col =[0])\n",
    "\n",
    "## preprocess text\n",
    "data = data[~data['Review Text'].isnull()]  #Dropping columns which don't have any review\n",
    "X = data[['Review Text']]\n",
    "X.index = np.arange(len(X))\n",
    "\n",
    "y = data['Recommended IND']\n",
    "\n",
    "corpus =[]\n",
    "for i in range(len(X)):\n",
    "    review = re.sub('[^a-zA-z]',' ',X['Review Text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review =[ps.stem(i) for i in review if not i in set(stopwords.words('english'))]\n",
    "    review =' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "cv  = CV(max_features = 3000,ngram_range=(1,1))\n",
    "X_cv = cv.fit_transform(corpus).toarray()\n",
    "y = y.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cv, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "with mlflow.start_run():\n",
    "\n",
    "    mlflow.set_tag(\"developer\", \"Isaac\")\n",
    "    mlflow.set_tag(\"algorithm\", \"BernoulliNB\")\n",
    "    mlflow.log_param(\"train-data\", \"Womens Clothing E-Commerce Reviews\")\n",
    "    \n",
    "    alpha = 1\n",
    "    mlflow.log_param(\"alpha\", alpha)\n",
    "\n",
    "    classifier = BernoulliNB(alpha = alpha)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "\n",
    "    print(\"accuracy on test data:\", acc)\n",
    "\n",
    "    model_name = \"model_bow.bin\"\n",
    "    with open(\"models/\" + model_name, 'wb') as fout:\n",
    "        pickle.dump((cv, classifier), fout)\n",
    "\n",
    "    mlflow.log_artifact(local_path=\"models/\" + model_name, artifact_path=\"models_pickle\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62796b27",
   "metadata": {},
   "source": [
    "You can see how easy it is to use MLflow to keep track of the ML model development process.\n",
    "\n",
    "Let's do it for TF-IDF too:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "# nltk.download('stopwords')\n",
    "os.environ[\"NLTK_DATA\"] = \"./corpora\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TV\n",
    "import pickle\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"customer-sentiment-analysis\")\n",
    "\n",
    "## data loading\n",
    "data = pd.read_csv('Womens Clothing E-Commerce Reviews.csv',index_col =[0])\n",
    "\n",
    "## preprocess text\n",
    "data = data[~data['Review Text'].isnull()]  #Dropping columns which don't have any review\n",
    "X = data[['Review Text']]\n",
    "X.index = np.arange(len(X))\n",
    "\n",
    "y = data['Recommended IND']\n",
    "\n",
    "corpus =[]\n",
    "for i in range(len(X)):\n",
    "    review = re.sub('[^a-zA-z]',' ',X['Review Text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review =[ps.stem(i) for i in review if not i in set(stopwords.words('english'))]\n",
    "    review =' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "tv  = TV(ngram_range =(1,1),max_features = 3000)\n",
    "X_tv = tv.fit_transform(corpus).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tv, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "with mlflow.start_run():\n",
    "\n",
    "    mlflow.set_tag(\"developer\", \"Isaac\")\n",
    "    mlflow.set_tag(\"algorithm\", \"MultinomialNB\")\n",
    "    mlflow.log_param(\"train-data\", \"Womens Clothing E-Commerce Reviews\")\n",
    "\n",
    "    alpha = 1\n",
    "    mlflow.log_param(\"alpha\", alpha)\n",
    "\n",
    "    classifier = MultinomialNB(alpha = alpha)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "\n",
    "    print(\"accuracy on test data:\", acc)\n",
    "\n",
    "    model_name = \"model_tfidf.bin\"\n",
    "    with open(\"models/\" + model_name, 'wb') as fout:\n",
    "        pickle.dump((tv, classifier), fout)\n",
    "\n",
    "    mlflow.log_artifact(local_path=\"models/\" + model_name, artifact_path=\"models_pickle\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a58352",
   "metadata": {},
   "source": [
    "And finally, the deep learning model:\n",
    "\n",
    "```python\n",
    "# ref: https://www.kaggle.com/code/granjithkumar/nlp-with-women-clothing-reviews/data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "# nltk.download('stopwords')\n",
    "os.environ[\"NLTK_DATA\"] = \"./corpora\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "import pickle\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"customer-sentiment-analysis\")\n",
    "\n",
    "## data loading\n",
    "data = pd.read_csv('Womens Clothing E-Commerce Reviews.csv',index_col =[0])\n",
    "\n",
    "## preprocess text\n",
    "data = data[~data['Review Text'].isnull()]  #Dropping columns which don't have any review\n",
    "X = data[['Review Text']]\n",
    "X.index = np.arange(len(X))\n",
    "\n",
    "y = data['Recommended IND']\n",
    "\n",
    "corpus =[]\n",
    "for i in range(len(X)):\n",
    "    review = re.sub('[^a-zA-z]',' ',X['Review Text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review =[ps.stem(i) for i in review if not i in set(stopwords.words('english'))]\n",
    "    review =' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "## tokenization and dataset creation\n",
    "tokenizer = Tokenizer(num_words = 3000)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    ## model definition\n",
    "    embedding_dim = 32\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(3000, embedding_dim),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(6, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    ## training\n",
    "    num_epochs = 50\n",
    "    batch_size = 32\n",
    "    callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=0,\n",
    "        patience=2,\n",
    "        verbose=0,\n",
    "        mode=\"auto\",\n",
    "        baseline=None,\n",
    "        restore_best_weights=False,\n",
    "    )\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    mlflow.set_tag(\"developer\", \"Isaac\")\n",
    "    mlflow.set_tag(\"algorithm\", \"Deep Learning\")\n",
    "    mlflow.log_param(\"train-data\", \"Womens Clothing E-Commerce Reviews\")\n",
    "    mlflow.log_param(\"embedding-dim\", embedding_dim)\n",
    "\n",
    "    print(\"Fit model on training data\")\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        callbacks=callback,\n",
    "        # We pass some validation for\n",
    "        # monitoring validation loss and metrics\n",
    "        # at the end of each epoch\n",
    "        validation_data=(X_test, y_test),\n",
    "    )\n",
    "\n",
    "    ## save model and tokenizer\n",
    "    mlflow.keras.log_model(model, 'models/model_dl')\n",
    "\n",
    "    with open('models/tf_tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    mlflow.log_artifact(local_path=\"models/tf_tokenizer.pickle\", artifact_path=\"tokenizer_pickle\")\n",
    "\n",
    "    # Evaluate the model on the test data using `evaluate`\n",
    "    print(\"Evaluate on test data\")\n",
    "    results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "    print(\"test loss, test acc:\", results)\n",
    "    mlflow.log_metric(\"loss\", results[0])\n",
    "    mlflow.log_metric(\"accuracy\", results[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac21f0",
   "metadata": {},
   "source": [
    "As you can see, we can use `autolog()` for TensorFlow and Scikit Learn to log parameters (automatically) or we can also log whatever we want manually. Finally you will see something like below after training different models with different hyperparameters:\n",
    "\n",
    "![](images/experiment-tracking-mlflow/1.png)\n",
    "<!-- *[source](https://github.com/evidentlyai/evidently)* -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc631efc",
   "metadata": {},
   "source": [
    "You can then easily go to each one of the experiments and see more details for each run and compare them. You can select one model based on different parameters like training time, accuracy, etc.. \n",
    "\n",
    "Note that we can log a model using one of the following two ways:\n",
    "\n",
    "1. save the model based on the framework, and then use `mlflow.log_artifact(local_path=<local path to saved model>, artifact_path=<name of the folder you want the model to be saved in mlruns>)`. We saved the tokenizer using this method in the deep learning version above.\n",
    "\n",
    "2. second, you can use `mlflow.<framework>.log_model(...)`. We saved the Keras model using this method in the deep learning version above.\n",
    "\n",
    "Let's take a look at what is saved for one of the runs. For example one of the deep learning runs:\n",
    "\n",
    "![](images/experiment-tracking-mlflow/3.png)\n",
    "<!-- *[source](https://github.com/evidentlyai/evidently)* -->\n",
    "\n",
    "As you see, the model, tokenizer, all the information about the required python packages, and many more metadata are saved. Each run has a unique `run id`, which we will use it later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b09fd",
   "metadata": {},
   "source": [
    "Now let's go for the next stage called Model Management which is about how we can manage the trained models.\n",
    "\n",
    "![](images/experiment-tracking-mlflow/2.png)\n",
    "*[source](https://neptune.ai/experiment-tracking)*\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fdce82e",
   "metadata": {},
   "source": [
    "As you can see from the codes above, we save models for every run. Then, there should be model versioning followed by deployment. Let's see how MLflow can be used for model management.\n",
    "\n",
    "Following the previous step, we will have several model runs and trained models. The data scientist must then select a subset of these models based on a number of metrics and register them in the MLflow model registry. The model registry contains multiple stages/labels, including staging, production, and archive. The model will enter the staging area first. The MLflow Model Registry is a centralized model repository, APIs, and user interface for managing the whole lifecycle of an MLflow Model. It includes model lineage (which MLflow experiment and run created the model), model versioning, stage transitions (such as staging to production), and annotations [[source](https://www.mlflow.org/docs/latest/model-registry.html)].\n",
    "\n",
    "The deployment engineer or team can then begin working on the models in the model registry and staging and decide, based on parameters such as the size of the model or the inference time, which model will go to the production stage.\n",
    "\n",
    "![](images/experiment-tracking-mlflow/4.png)\n",
    "*[source](https://www.databricks.com/fr/blog/2020/04/15/databricks-extends-mlflow-model-registry-with-enterprise-features.html)*\n",
    "\n",
    "To register a model in the model registry, choose one of the runs and then the model folder under 'Artifacts'. Then, the 'Register Model' button will become visible. You must choose a model name that contains all of the versions. We created the model name \"customer-sentiment-analysis\" and registered three models, one from each method, based on the accuracy metric.\n",
    "\n",
    "![](images/experiment-tracking-mlflow/5.png)\n",
    "<!-- *[source](https://www.databricks.com/fr/blog/2020/04/15/databricks-extends-mlflow-model-registry-with-enterprise-features.html)* -->\n",
    "\n",
    "Then if you go to the `Models` tab on top of the screen, you will see all the versions. You can select each version and then set the stage. Here we set all to the `staging` state. It's a good practice to add a description about the data the stage has changed and also the name of the developer for each version.\n",
    "\n",
    "![](images/experiment-tracking-mlflow/6.png)\n",
    "<!-- *[source](https://www.databricks.com/fr/blog/2020/04/15/databricks-extends-mlflow-model-registry-with-enterprise-features.html)* -->\n",
    "\n",
    "![](images/experiment-tracking-mlflow/7.png)\n",
    "<!-- *[source](https://www.databricks.com/fr/blog/2020/04/15/databricks-extends-mlflow-model-registry-with-enterprise-features.html)* -->\n",
    "\n",
    "You can also use the API and `MlflowClient` and do these steps in code. Check the API [here](https://www.mlflow.org/docs/latest/model-registry.html) and the following video for more details. I prefer the UI.\n",
    "\n",
    "Then, you can compare models in the staging phase and choose one for production deployment. In our example, the accuracy of the deep learning model is higher, thus we advance it to the production stage. When you transition a model to the \"Production\" stage, the model registry simply assigns a label to that model version and does not actually deploy the model to production. Complement the registry with CI/CD code that does actual deployments.\n",
    "\n",
    "![](images/experiment-tracking-mlflow/8.png)\n",
    "\n",
    "We can then use the run id for this version and download the model and other required files like the tokenizer and deploy it. In our case, we want to load the Keras model. Check the [documentation](https://www.mlflow.org/docs/latest/python_api/mlflow.keras.html) for more details:\n",
    "\n",
    "```python\n",
    "mlflow.keras.load_model(model_uri, dst_path=None)\n",
    "```\n",
    "\n",
    "where `models:/<model_name>/<stage or version>`. `model_name` is `customer sentiment-analysis` and `stage` is `production`:\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"customer-sentiment-analysis\")\n",
    "\n",
    "mlflow.keras.load_model(\"models:/customer-sentiment-analysis/production\", dst_path=None)\n",
    "or \n",
    "mlflow.keras.load_model(\"models:/customer-sentiment-analysis/3\", dst_path=None)\n",
    "```\n",
    "\n",
    "You can set the `dst_path` if you want to save the model locally too. Let's also download the tokenizer:\n",
    "\n",
    "```python\n",
    "from mlflow.tracking import MlflowClient\n",
    "client = MlflowClient(tracking_uri=\"sqlite:///mlflow.db\")\n",
    "client.download_artifacts(run_id=\"d3ebd0c0b590443e824cde73fe041a6e\", path='tokenizer', dst_path='.')\n",
    "```\n",
    "\n",
    "The `run_id` is for the model in production that we can get it from mlflow UI.\n",
    "\n",
    "We can also read the model and artifacts from Google storage or Amazon S3. Check the documentation for more details.\n",
    "\n",
    "> youtube: https://youtu.be/TKHU7HAvGH8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511a6b1d",
   "metadata": {},
   "source": [
    "You can check the following video and also my [blog post](https://kargarisaac.github.io/blog/mlops/jupyter/2022/06/15/MLFlow-on-GCP.html) to see how you can setup MLflow on AWS or GCP.\n",
    "\n",
    "> youtube: https://www.youtube.com/watch?v=1ykg4YmbFVA&list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK&index=16 \n",
    "\n",
    "\n",
    "We will see more about using the model in the production stage in the next blog posts. That's it for the first blog post. In the next blog post will go for Orchestration. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
