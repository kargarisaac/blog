<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Model Based Reinforcement Learning (MBRL) | Isaac Kargar</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Model Based Reinforcement Learning (MBRL)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is a summary of MBRL tutorial from ICML-2020." />
<meta property="og:description" content="This is a summary of MBRL tutorial from ICML-2020." />
<link rel="canonical" href="https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/26/mbrl.html" />
<meta property="og:url" content="https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/26/mbrl.html" />
<meta property="og:site_name" content="Isaac Kargar" />
<meta property="og:image" content="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-26T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"This is a summary of MBRL tutorial from ICML-2020.","@type":"BlogPosting","headline":"Model Based Reinforcement Learning (MBRL)","url":"https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/26/mbrl.html","datePublished":"2020-10-26T00:00:00-05:00","dateModified":"2020-10-26T00:00:00-05:00","image":"https://kargarisaac.github.io/blog/images/some_folder/your_image.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/26/mbrl.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://kargarisaac.github.io/blog/feed.xml" title="Isaac Kargar" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Model Based Reinforcement Learning (MBRL) | Isaac Kargar</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Model Based Reinforcement Learning (MBRL)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is a summary of MBRL tutorial from ICML-2020." />
<meta property="og:description" content="This is a summary of MBRL tutorial from ICML-2020." />
<link rel="canonical" href="https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/26/mbrl.html" />
<meta property="og:url" content="https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/26/mbrl.html" />
<meta property="og:site_name" content="Isaac Kargar" />
<meta property="og:image" content="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-26T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"This is a summary of MBRL tutorial from ICML-2020.","@type":"BlogPosting","headline":"Model Based Reinforcement Learning (MBRL)","url":"https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/26/mbrl.html","datePublished":"2020-10-26T00:00:00-05:00","dateModified":"2020-10-26T00:00:00-05:00","image":"https://kargarisaac.github.io/blog/images/some_folder/your_image.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/26/mbrl.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://kargarisaac.github.io/blog/feed.xml" title="Isaac Kargar" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Isaac Kargar</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Model Based Reinforcement Learning (MBRL)</h1><p class="page-description">This is a summary of MBRL tutorial from ICML-2020.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-10-26T00:00:00-05:00" itemprop="datePublished">
        Oct 26, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      42 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/kargarisaac/blog/tree/master/_notebooks/2020-10-26-mbrl.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/kargarisaac/blog/master?filepath=_notebooks%2F2020-10-26-mbrl.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/kargarisaac/blog/blob/master/_notebooks/2020-10-26-mbrl.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Introduction-and-Motivation">Introduction and Motivation </a></li>
<li class="toc-entry toc-h2"><a href="#Problem-Statement">Problem Statement </a></li>
<li class="toc-entry toc-h2"><a href="#What-is-a-model?">What is a model? </a></li>
<li class="toc-entry toc-h2"><a href="#How-to-use-a-model?">How to use a model? </a></li>
<li class="toc-entry toc-h2"><a href="#How-to-learn-a-model?">How to learn a model? </a>
<ul>
<li class="toc-entry toc-h3"><a href="#state-transition-models">state-transition models </a></li>
<li class="toc-entry toc-h3"><a href="#observation-transition-models">observation-transition models </a></li>
<li class="toc-entry toc-h3"><a href="#latent-state-transition-models">latent state-transition models </a></li>
<li class="toc-entry toc-h3"><a href="#Structured-latent-state-transition-models">Structured latent state-transition models </a></li>
<li class="toc-entry toc-h3"><a href="#Recurrent-value-models">Recurrent value models </a></li>
<li class="toc-entry toc-h3"><a href="#Non-Parametric-models">Non-Parametric models </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Model-based-control-and-how-to-use-a-model?">Model-based control and how to use a model? </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Simulating-the-environment">Simulating the environment </a></li>
<li class="toc-entry toc-h3"><a href="#Assisting-the-learning-algorithm">Assisting the learning algorithm </a></li>
<li class="toc-entry toc-h3"><a href="#Strengthening-the-policy">Strengthening the policy </a>
<ul>
<li class="toc-entry toc-h4"><a href="#What-is-the-difference-between-background-and-decision-time-planning?">What is the difference between background and decision-time planning? </a></li>
<li class="toc-entry toc-h4"><a href="#What-is-the-difference-between-discrete-and-continuous-planning?">What is the difference between discrete and continuous planning? </a>
<ul>
<li class="toc-entry toc-h5"><a href="#MCTS-(monte-carlo-tree-search)">MCTS (monte carlo tree search) </a></li>
<li class="toc-entry toc-h5"><a href="#Trajectory-Optimization">Trajectory Optimization </a></li>
</ul>
</li>
<li class="toc-entry toc-h4"><a href="#Variety-and-motivations-of-continuous-planning-methods">Variety and motivations of continuous planning methods </a>
<ul>
<li class="toc-entry toc-h5"><a href="#Sensitivity-and-poor-conditioning">Sensitivity and poor conditioning </a></li>
<li class="toc-entry toc-h5"><a href="#Only-reaches-local-optimum">Only reaches local optimum </a></li>
<li class="toc-entry toc-h5"><a href="#Slow-convergence">Slow convergence </a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Model-based-control-in-the-loop">Model-based control in the loop </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Gathering-data-to-train-models">Gathering data to train models </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Fixed-off-line-datasets">Fixed off-line datasets </a></li>
<li class="toc-entry toc-h4"><a href="#Data-augmentation">Data augmentation </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Acting-under-imperfect-models">Acting under imperfect models </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Replan-via-model-predictive-control">Replan via model-predictive control </a></li>
<li class="toc-entry toc-h4"><a href="#Plan-conservatively">Plan conservatively </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Estimating-model-uncertainty">Estimating model uncertainty </a></li>
<li class="toc-entry toc-h3"><a href="#Combining-planning-and-learning">Combining planning and learning </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Distillation">Distillation </a></li>
<li class="toc-entry toc-h4"><a href="#Terminal-value-functions-(value-of-the-terminal-state)">Terminal value functions (value of the terminal state) </a></li>
<li class="toc-entry toc-h4"><a href="#Planning-as-policy-improvement">Planning as policy improvement </a></li>
<li class="toc-entry toc-h4"><a href="#Implicit-planning">Implicit planning </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#What-else-can-models-be-used-for?">What else can models be used for? </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Exploration">Exploration </a></li>
<li class="toc-entry toc-h3"><a href="#Hierarchical-reasoning">Hierarchical reasoning </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Subgoal-based-approaches">Subgoal-based approaches </a></li>
<li class="toc-entry toc-h4"><a href="#Skill-based-approaches">Skill-based approaches </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#representation-Learning">representation Learning </a></li>
<li class="toc-entry toc-h3"><a href="#Adaptivity-&-generalization">Adaptivity &amp; generalization </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Adapting-the-planner-in-new-states">Adapting the planner in new states </a></li>
<li class="toc-entry toc-h4"><a href="#Adapting-the-planner-to-new-rewards">Adapting the planner to new rewards </a></li>
<li class="toc-entry toc-h4"><a href="#Adapting-the-model-to-new-dynamics">Adapting the model to new dynamics </a></li>
</ul>
</li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-10-26-mbrl.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This post is a summary of the model-based RL tutorial at ICML-2020. You can find the videos <a href="https://sites.google.com/view/mbrl-tutorial">here</a>. The pictures are from the slides in the talk.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction-and-Motivation">
<a class="anchor" href="#Introduction-and-Motivation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction and Motivation<a class="anchor-link" href="#Introduction-and-Motivation"> </a>
</h2>
<p>Having access to a world model, and using it for decision-making is a powerful idea. 
There are a lot of applications of MBRL in different areas like robotics (manipulation- what will happen by doing an action), 
self-driving cars (having a model of other agents decisions and future motions and act accordingly),
games (AlphaGo- search over different possibilities), Science ( chemical use-cases),
and operation research and energy applications (allocate renewable energy at different points in time to meet the demand).</p>
<h2 id="Problem-Statement">
<a class="anchor" href="#Problem-Statement" aria-hidden="true"><span class="octicon octicon-link"></span></a>Problem Statement<a class="anchor-link" href="#Problem-Statement"> </a>
</h2>
<p>In sequential decision making, the agent will interact with the world by doing action $a$ and getting the next state $s$ and reward $r$.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/rl.png" alt="" style="max-width: 400px">
    
    
</figure>
</p>
<p>We can write this problem as a Markov Decision Process (MDP) as follows:</p>
<ul>
<li>States $S \epsilon R^{d_S}$</li>
<li>Actions $A \epsilon R^{d_A}$</li>
<li>Reward function $R: S \times A \rightarrow R$</li>
<li>Transition function $T: S \times A \rightarrow S$</li>
<li>Discount $\gamma \epsilon (0,1)$</li>
<li>Policy $\pi: S \rightarrow A$</li>
</ul>
<p>The goal is to find a policy which maximizes the sum of discounted future rewards:
$$
\text{argmax}_{\pi} \sum_{t=0}^\infty \gamma^t R(s_t, a_t)
$$
subject to
$$
a_t = \pi(s_t) , s_{t+1}=T(s_t, a_t)
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>How to solve this optimization problem?!</p>
<ul>
<li>Collect data $D= \{ s_t, a_t, r_{t+1}, s_{t+1} \}_{t=0}^T$.</li>
<li>Model-free: learn policy directly from data</li>
</ul>
<p>
$$ D \rightarrow \pi \quad \text{e.g. Q-learning, policy gradient}$$
</p>
<ul>
<li>Model-based: learn model, then use it to <strong>learn</strong> or <strong>improve</strong> a policy </li>
</ul>
<p>
$$ D \rightarrow f \rightarrow \pi$$
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-is-a-model?">
<a class="anchor" href="#What-is-a-model?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is a model?<a class="anchor-link" href="#What-is-a-model?"> </a>
</h2>
<p>a model is a representation that explicitly encodes knowledge about the structure of the environment and task.</p>
<p>This model can take a lot of different forms:</p>
<ul>
<li>A transition/dynamic model: $s_{t+1} = f_s(s_t, a_t)$</li>
<li>A model of rewards: $r_{t+1} = f_r(s_t, a_t)$</li>
<li>An inverse transition/dynamics model (which tells you what is the action to take and go from one state to the next state): $a_t = f_s^{-1}(s_t, s_{t+1})$</li>
<li>A model of distance of two states: $d_{ij} = f_d(s_i, s_j)$</li>
<li>A model of future returns: $G_t = Q(s_t, a_t)$ or $G_t = V(s_t)$</li>
</ul>
<p>Typically when someone says MBRL, he/she means the firs two items.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/model.png" alt="">
    
    
</figure>
</p>
<p>Sometimes we know the ground truth dynamics and rewards. Might as well use them! Like game environments or simulators like Mujoco, Carla, and so on.</p>
<p>But we don't have access to the model in all cases, so we need to learn the model. In cases like in robots, complex physical dynamics, and interaction with humans.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-to-use-a-model?">
<a class="anchor" href="#How-to-use-a-model?" aria-hidden="true"><span class="octicon octicon-link"></span></a>How to use a model?<a class="anchor-link" href="#How-to-use-a-model?"> </a>
</h2>
<p>In model-free RL agent, we have a policy and learning algorithm like the figure below:</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/rl2.png" alt="">
    
    
</figure>
</p>
<p>In model-based RL we can use the model in three different ways:</p>
<ul>
<li>simulating the environment: replacing the environment with a model and use it to generate data and use it to update the policy.</li>
<li>Assisting the learning algorithm: modify the learning algorithm to use the model to interpret the data it is getting differently. </li>
<li>Strengthening the policy: allow the agent at test time to use the model to try out different actions before it commits to one of them (taking action in the real world).</li>
</ul>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbrl.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In general, to compare model-free and model-based:</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbrl_vs_mfrl.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-to-learn-a-model?">
<a class="anchor" href="#How-to-learn-a-model?" aria-hidden="true"><span class="octicon octicon-link"></span></a>How to learn a model?<a class="anchor-link" href="#How-to-learn-a-model?"> </a>
</h2>
<p>Two different dimensions are useful to pay attention to:</p>
<ul>
<li>
<p>representation of the features for the states that the model is being learned over them</p>
</li>
<li>
<p>representation of the transition between states</p>
</li>
</ul>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To continue, we take a look at different transition models.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="state-transition-models">
<a class="anchor" href="#state-transition-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>state-transition models<a class="anchor-link" href="#state-transition-models"> </a>
</h3>
<p>We know equations of motion and dynamics in some cases, but we don't know the exact parameters like mass. We can use system identification to estimate unknown parameters like mass. But these sorts of cases require having a lot of domain knowledge about how exactly the system works.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model2.png" alt="">
    
    
</figure>

<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model3.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In some cases that we don't know the dynamics of motion, we can use an MLP to get a concatenation of $s_t, a_t$, and output the next state $s_{t+1}$.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model4.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In cases that we have some, not perfect, domain knowledge about the environment, we can use graph neural networks (GNNs) to model the agent (robot). For example, in Mujoco, we can model a robot (agent) with nodes as its body parts and edges as joint and learn the physics engine.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model5.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="observation-transition-models">
<a class="anchor" href="#observation-transition-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>observation-transition models<a class="anchor-link" href="#observation-transition-models"> </a>
</h3>
<p>In these cases, we don't have access to states (low-level states like joint angles), but we have access to images. The MDP for these cases would be like this:</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model6.png" alt="">
    
    
</figure>
</p>
<p>So what can we do with this?</p>
<ul>
<li>Directly predict transitions between observations (observation-transition models)</li>
</ul>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model7.png" alt="">
    
    
</figure>
</p>
<ul>
<li>Reconstruct observation at every timestep: Using sth like LSTMs. Here we need to reconstruct the whole observation in each timestep. The images can be blurry in these cases.</li>
</ul>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model8.png" alt="">
    
    
</figure>
</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model88.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="latent-state-transition-models">
<a class="anchor" href="#latent-state-transition-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>latent state-transition models<a class="anchor-link" href="#latent-state-transition-models"> </a>
</h3>
<p>Another option when we have just access to observation is to instead of making transition between observations we can infere a latent state and then make transitions in that latent space (latent state-transition models) not in the observation space. It would be much faster than reconstructing the observation on every timestep. We take our initial observation or perhaps the last couple of observations and embed them into the latent state and then unroll it in time and do predictions in $z$ instead of $o$.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model9.png" alt="">
    
    
</figure>
</p>
<p>Usually we use the observation and reconstruct it during training but at test time we can unroll it very quickly. we can also reconstruct observation at each timestep we want (not necessarily in all timesteps).</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model10.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Structured-latent-state-transition-models">
<a class="anchor" href="#Structured-latent-state-transition-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Structured latent state-transition models<a class="anchor-link" href="#Structured-latent-state-transition-models"> </a>
</h3>
<p>Another thing that you can do if you have a little bit more domain knowledge is to add a little bit of structure into your latent state. For example, if you know that the scene that you are trying to model consists of objects, you can try to actually explicitly detect those objects, segment them out and then learn those transitions between objects.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model11.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Recurrent-value-models">
<a class="anchor" href="#Recurrent-value-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Recurrent value models<a class="anchor-link" href="#Recurrent-value-models"> </a>
</h3>
<p>The idea is that when you unroll your latent-state, you additionally predict the value of the state at each point of the future, in addition to reward. We can train the model without necessarily needing to train using observations, but just training it by predicting the value progressing toward actual observed values when you roll it out in the real environment.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model12.png" alt="">
    
    
</figure>
</p>
<p>Why is this useful? Because some types of planners only need you to predict values rather than predicting states lime MCTS (Monte Carlo tree search).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Non-Parametric-models">
<a class="anchor" href="#Non-Parametric-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Non-Parametric models<a class="anchor-link" href="#Non-Parametric-models"> </a>
</h3>
<p>So far, we talked about parametric ways of learning the model. We can also use non-parametric methods like graphs.</p>
<p>For example, the replay buffer that we use in off-policy methods can be seen as an approximation to a type of model, where if you have enough data in your replay buffer, you can sample from the buffer and basically access the density model over your transitions. You can use extra replay to get the same level performances you would get using a model-based method that learns a parametric model.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model13.png" alt="">
    
    
</figure>
</p>
<p>We can also use data in the buffer to use data points and learn the transition between them and interpolate to find states between those states in the buffer. Somehow learning distribution and use it to generate new data points.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model14.png" alt="">
    
    
</figure>
</p>
<p>Another form of non-parametric transition is a symbolic description popular in the planning community, not in the deep learning community.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model15.png" alt="">
    
    
</figure>
</p>
<p>The other form of non-parametric models is gaussian processes, which give us strong predictions using a very small amount of data. PILCO is one example of these algorithms.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model16.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Model-based-control-and-how-to-use-a-model?">
<a class="anchor" href="#Model-based-control-and-how-to-use-a-model?" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model-based control and how to use a model?<a class="anchor-link" href="#Model-based-control-and-how-to-use-a-model?"> </a>
</h2>
<p>We will be using this landscape of various methods and categories that exist, including some representative algorithms:</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc1.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As we saw earlier, we can use the model in three different ways. In continue, we will see some examples of each case.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc.png" alt="" style="max-width: 300px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Simulating-the-environment">
<a class="anchor" href="#Simulating-the-environment" aria-hidden="true"><span class="octicon octicon-link"></span></a>Simulating the environment<a class="anchor-link" href="#Simulating-the-environment"> </a>
</h3>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc2.png" alt="" style="max-width: 150px">
    
    
</figure>
</p>
<p>One way is to mix the real data with model-generated experience and then apply traditional model-free algorithms like Q-learning, policy gradient, etc. In these cases, the model offers a larger and augmented training dataset.</p>
<p><strong>Dyna-Q</strong> is an example that uses Q-learning with a learned model. Dyna does the traditional Q-learning updates on real transitions and uses a model to create fictitious imaginary transitions from the real states and perform exactly the same Q-learning updates on those. So it's basically just a way to augment the experience.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc3.png" alt="">
    
    
</figure>
</p>
<p>This can also be applied to policy learning. We don't need to perform just a single step but multiple steps according to the <strong>model</strong> to generate experience even further away from the real data and do policy parameter updates entirely on these fictitious experiences.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc4.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Assisting-the-learning-algorithm">
<a class="anchor" href="#Assisting-the-learning-algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>Assisting the learning algorithm<a class="anchor-link" href="#Assisting-the-learning-algorithm"> </a>
</h3>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc5.png" alt="" style="max-width: 150px">
    
    
</figure>
</p>
<p>One important way that this can be done is to allow end-to-end training through our models. End-to-end training has recently been very successful in improving and simplifying supervised learning methods in computer vision, NLP, etc.</p>
<p>The question is, "can we apply the same type of end-to-end approaches to RL?"</p>
<p>One example is just the policy gradient algorithm. Let's say we want to maximize the sum of the discounted future reward of some parametric policy. We can write the objective function with respect to the policy parameters $\theta$</p>
$$
 J(\theta) = \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t)  , \quad a_t = \pi_{\theta}(s_t) , \quad s_{t+1} = T(s_t, a_t)
$$<p>Now we need to apply gradient ascent (for maximization) on policy gradient with respect to policy parameters $\theta  \rightarrow  \nabla_{\theta}J$.</p>
<p>So how can we calculate this $\nabla_{\theta}J$ ?</p>
<p>Sampling-based methods have been proposed, like REINFORCE, to estimate this gradient. But the problem with them is that they cannot have very high variance and often require the policy to have some randomness to make decisions. This can be unfavorable.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc6.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Accurate and smooth models, aside from imaginary experiences, offer derivatives:</p>
$$
s_{t+1} = f_s(s_t, a_t) \quad  r_t = f_r(s_t, a_t)
$$$$
\nabla_{s_t}(s_{t+1}), \quad \nabla_{a_t}(s_{t+1}), \quad \nabla_{s_t}(r_t), \quad \nabla_{a_t}(r_t), \quad ...
$$<p>And they are able to answer questions such as: <em>how do small changes in action change next state or reward any of other quantities?</em></p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc7.png" alt="" style="max-width: 150px">
    
    
</figure>
</p>
<p>Why is this useful? This is useful because it will allow us to do this type of end-to-end differentiation algorithms like <strong>back-propagation</strong>.</p>
<p>Let's rewrite our objective function using models:</p>
$$
 J(\theta) \approx \sum_{t=0}^{H} \gamma^t r_t  , \quad a_t = \pi_{\theta}(s_t) , \quad s_{t+1} = f_s(s_t, a_t), \quad r_t=f_r(s_t,a_t)
$$<p>So how can we use these derivatives to calculate $\nabla_{\theta}J$ ?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc8.png" alt="">
    
    
</figure>
</p>
<p>The highlighted derivatives are easy to calculate using some libraries like PyTorch or TensorFlow.</p>
<p>By calculating $\nabla_{\theta}J$ in this way:</p>
<p><strong>pros</strong>:</p>
<ul>
<li>The policy gradient that we get is actually a deterministic quantity, and there is no variance to it. </li>
<li>It can support potentially much longer-term credit assignment</li>
</ul>
<p><strong>cons</strong>:</p>
<ul>
<li>It is prone to local minima</li>
<li>Poor conditioning (vanishing/exploding gradients)</li>
</ul>
<p>Here are two examples to use model-based back-propagation (derivatives) either along real or model-generated trajectories to do end to end training:</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc9.png" alt="">
    
    
</figure>
</p>
<ul>
<li>real trajectories are safer but need to be from the current policy parameters (so it’s less sample-efficient)</li>
<li>model-generated trajectories allow larger policy changes without interacting with the real world but might suffer more from model inaccuracies</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Strengthening-the-policy">
<a class="anchor" href="#Strengthening-the-policy" aria-hidden="true"><span class="octicon octicon-link"></span></a>Strengthening the policy<a class="anchor-link" href="#Strengthening-the-policy"> </a>
</h3>
<p>So far, we talked about the first two ways of using a model in RL. These two ways are in the category of <strong>Background Planning</strong>.</p>
<p>There is another category based on the <em>Sutton and Barto (2018)- Reinforcement Learning: An Introduction</em> categorization, called <strong>Decision-Time Planning</strong>, which is a unique option we have available in model-based settings.</p>
<h4 id="What-is-the-difference-between-background-and-decision-time-planning?">
<a class="anchor" href="#What-is-the-difference-between-background-and-decision-time-planning?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is the difference between background and decision-time planning?<a class="anchor-link" href="#What-is-the-difference-between-background-and-decision-time-planning?"> </a>
</h4>
<p>In background planning, we can think of it as answering the question, "how do I learn how to act in any possible situation to succeed and reach the goal?"</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc10.png" alt="" style="max-width: 300px">
    
    
</figure>
</p>
<ul>
<li>The optimization variables are parameters of a policy or value function or ..., and are trained using expectation over all possible situations.</li>
<li>Conceptually, we can think of background planning as learning a set of habits that we could reuse.</li>
<li>We can think of background planning as learning a fast type of thinking.</li>
</ul>
<p>In decision-time planning, we want to answer the question, "what is the best sequence of actions just for my current situation to succeed or reach the goal?"</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc11.png" alt="" style="max-width: 300px">
    
    
</figure>
</p>
<ul>
<li>The optimization parameters are just a sequence of actions or states.</li>
<li>Conceptually, we can think of decision-time planning as finding our consciously improvising just for the particular situation that we find ourselves in.</li>
<li>We can think of decision-time planning as learning a slow type of thinking.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Why use one over the other?</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc12.png" alt="">
    
    
</figure>
</p>
<ul>
<li>
<p><em>Act on the most recent state of the world</em>: decision-time planning is just concerned about the current state in finding the sequence of actions. You can act based on the most recent state of the world. By contrast, in background planning, the habits may be stale and might take a while to get updated as the world's changes.</p>
</li>
<li>
<p><em>Act without any learning</em>: decision-time planning allows us to act without any learning at all. There is no need for policy or value networks before we can start making decisions. It is just an optimization problem as long as you have the model.</p>
</li>
<li>
<p><em>Competent in unfamiliar situations</em>: if you find yourself in situations that are far away from where you were training, your set of habits or policy network might not have the competence (the ability to do something successfully or efficiently) there. So you don't have any information to act or are very uncertain, or even in the worst case, it will with confidence make decisions that just potentially make no sense. This is out of distribution and generalization problem. In these cases, decision-time planning would be more beneficial.</p>
</li>
<li>
<p><em>Independent of observation space</em>: another advantage of decision-time planning is that it is also independent of the observation space that you decide on. In background methods, we need to consider some encoding or description of the state, joint angles, or pixels or graphs into our policy function. These decisions may play a large role in the total learning performance. When something is not working, you will not really know that is it because of the algorithm or state-space, which doesn't contain enough information. In contrast, decision-time planning avoids this confounded, which in practice can actually be quite useful when you're prototyping new methods.</p>
</li>
</ul>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc13.png" alt="" style="max-width: 500px">
    
    
</figure>
</p>
<ul>
<li>
<p><em>Partial observability</em>: decision-time plannings have some issues with it. They assume that you know the full state of the world when you're making the plan. So it's hard to hide information from decision-time planners. It is possible, but it is more costly.</p>
</li>
<li>
<p><em>Fast computation at deployment</em>: decision-time planners require more computation. It is not just evaluating a habit, but it needs more thinking.</p>
</li>
<li>
<p><em>Predictability and coherence</em>: decision-time planners do some actions which are not necessarily predictable or coherent. Because you are consciously thinking about each footstep, you might not have exactly the same plan. So you may have a very chaotic behavior that still succeeds. In contrast, background planning, because it learns a set of habits, it can perform a very regular behavior.</p>
</li>
<li>
<p><em>Same for discrete and continuous actions</em>: background planning has a very unified treatment of discrete and continuous actions, which is conceptually simpler. In decision-time planning, there are different algorithms for discrete and continuous actions. We will see in the following sections more about them.</p>
</li>
</ul>
<p>We can also mix and match the background and decision-time plannings.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="What-is-the-difference-between-discrete-and-continuous-planning?">
<a class="anchor" href="#What-is-the-difference-between-discrete-and-continuous-planning?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is the difference between discrete and continuous planning?<a class="anchor-link" href="#What-is-the-difference-between-discrete-and-continuous-planning?"> </a>
</h4>
<p>It depends on the problem which you want to solve. So it is not a choice that you can make. For example, in controlling a robot, the actions might be the torques for the motors (continuous), or in biomechanical settings, it might be muscle excitations (continuous), or in medical problems, the treatment that should be applied (discrete).</p>
<p>The distinction between discrete and continuous actions is not significant for background planning methods.</p>
<ul>
<li>You just learn stochastic policies that sample either from discrete or continuous distributions.</li>
</ul>
$$
a \sim \pi(.|s) \quad \leftarrow Gaussian, categorical, ...
$$<ul>
<li>Backpropagation is still possible via some reparametrization techniques. See <em>Jang et al (2016). Categorical reparametrization with Gumbel-Softmax</em> for an example.</li>
</ul>
<p>In either of these cases (continuous and discrete in background planning methods), your final objective and optimization problem is still smooth wrt the policy parameters because you are optimizing over expectations.</p>
$$
J(\theta) = E_{\pi}[\sum_t r_t], \quad a_t \sim \pi(.|s_t, \theta)
$$<p>But for decision-time planning, this distinction leads to specialized methods for discrete and continuous actions: discrete search or continuous trajectory optimization.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's see some examples to be able to compare them.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc14.png" alt="" style="max-width: 400px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="MCTS-(monte-carlo-tree-search)">
<a class="anchor" href="#MCTS-(monte-carlo-tree-search)" aria-hidden="true"><span class="octicon octicon-link"></span></a>MCTS (monte carlo tree search)<a class="anchor-link" href="#MCTS-(monte-carlo-tree-search)"> </a>
</h5>
<p>This algorithm is in a discrete action group and is used in alpha-go and alpha-zero. You keep track of Q-value, which is long term reward, for all states and actions that you want to consider. And also the number of times that the state and action have been previously visited.</p>
<ol>
<li>Initialize $Q_0(s, a) = 0, N_0(s, a)=0, k=0$</li>
</ol>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc15.png" alt="" style="max-width: 150px">
    
    
</figure>
</p>
<ol>
<li>Expansion: Starting from the current situation and expand nodes and selecting actions according to a search policy: </li>
</ol>
<p>
$$\pi_k(s) = Q_k(s,a)$$
</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc16.png" alt="" style="max-width: 150px">
    
    
</figure>
</p>
<ol>
<li>Evaluation: When a new node is reached, estimate its long-term value using Monte-Carlo rollouts</li>
</ol>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc17.png" alt="" style="max-width: 200px">
    
    
</figure>
</p>
<ol>
<li>Backup: Propagate the Q-values to parent nodes:</li>
</ol>
$$
Q_{k+1}(s, a) = \frac{Q_k(s,a) N_k(s,a) + R}{N_k(s,a)+1}
$$$$
N_{k+1}(s,a) = N_k(s,a)+1
$$<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc18.png" alt="" style="max-width: 300px">
    
    
</figure>
</p>
<ol>
<li>Repeat Steps 2-4 until the search budget is exhausted.
$$
k = k + 1
$$</li>
</ol>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc19.png" alt="" style="max-width: 300px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Trajectory-Optimization">
<a class="anchor" href="#Trajectory-Optimization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Trajectory Optimization<a class="anchor-link" href="#Trajectory-Optimization"> </a>
</h5>
<p>Instead of keeping track of a tree of many possibilities, you keep track of one possible action sequence.</p>
<ol>
<li>Initialize $a_0, ..., a_H$ from guess</li>
</ol>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc20.png" alt="" style="max-width: 200px">
    
    
</figure>
</p>
<ol>
<li>
<strong>Expansion</strong>: execute sequence of actions $a = a_0, ..., a_H$ to get a sequence of states $s_1, ..., s_H$</li>
</ol>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc21.png" alt="" style="max-width: 200px">
    
    
</figure>
</p>
<ol>
<li>
<p><strong>Evaluation</strong>: get trajectory reward $J(a) = \sum_{t=0}^H r_t$</p>
</li>
<li>
<p><strong>Back-propagation</strong>: because everything is differentiable, you can just calculate the gradient of the reward via back-propagation using reward model derivatives and transition model derivatives.</p>
</li>
</ol>
$$
\nabla_a J = \sum_{t=0}^H \nabla_a r_t
$$$$
\nabla_a r_t = \nabla_s f_r(s_t, a_t) \nabla_a s_t + \nabla_a f_r (s_t, a_t)
$$$$
\nabla_a s_t = \nabla_a f_s(s_{t-1}, a_{t-1}) + \nabla_s f_s(s_{t-1}, a_{t-1})\nabla_a s_{t-1}
$$$$
\nabla_a s_{t-1} = ...
$$<ol>
<li>Update all actions via gradient ascent $ a \leftarrow a + \nabla_a J$ and repeat steps 2-5.</li>
</ol>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc22.png" alt="" style="max-width: 200px">
    
    
</figure>
</p>
<p>The differences between discrete and continuous actions can be summarized as follows:</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc23.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The continuous example we saw above can be categorized in <strong>shooting methods</strong>.</p>
<h4 id="Variety-and-motivations-of-continuous-planning-methods">
<a class="anchor" href="#Variety-and-motivations-of-continuous-planning-methods" aria-hidden="true"><span class="octicon octicon-link"></span></a>Variety and motivations of continuous planning methods<a class="anchor-link" href="#Variety-and-motivations-of-continuous-planning-methods"> </a>
</h4>
<p>Why so many variations? They all try to mitigate the issues we looked at like:</p>
<ul>
<li>Sensitivity and poor conditioning</li>
<li>Only reaches local optimum</li>
<li>Slow convergence</li>
</ul>
<p>Addressing each leads to a different class of methods.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc24.png" alt="" style="max-width: 200px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Sensitivity-and-poor-conditioning">
<a class="anchor" href="#Sensitivity-and-poor-conditioning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Sensitivity and poor conditioning<a class="anchor-link" href="#Sensitivity-and-poor-conditioning"> </a>
</h5>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc24-2.png" alt="" style="max-width: 200px">
    
    
</figure>
</p>
<p><strong>Shooting methods</strong> that we have seen have this particular issue that small changes in early actions lead to very large changes downstream.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc25.png" alt="" style="max-width: 200px">
    
    
</figure>
</p>
<p>By expanding the objective function, this can be understood more clearly.</p>
$$
\max_{a_0,...,a_H} \sum_{t=0}^H r(s_t, a_t), \quad s_{t+1} = f(s_t, a_t)
$$$$
\sum_{t=0}^H r(s_t, a_t) = r(s_0, a_0) + r(f(s_0, a_0), a_1)+...+r(f(f(...),...), a_H)
$$<p>It means that each state implicitly is dependent on all actions that came before it. This is similar to the exploding/vanishing gradient problem in RNNs that hurts long-term credit assignment. But unlike the RNN training, we cannot change the transition function because it is dictated to us by the environment.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc26.png" alt="" style="max-width: 400px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To address this problem, <strong>Collocation</strong> is introduced, which is optimizing for states and/or actions <em>directly</em>, instead of actions only. So we have a different set of parameters that we are optimizing over.</p>
$$
\max_{s_0,a_0,...,s_H,a_H} \sum_{t=0}^H r(s_t, a_t), \quad ||s_{t+1} - f(s_t, a_t) || = 0 \leftarrow \text{explicit optimization constraint}
$$<p>It is an explicit constrained optimization problem, rather than just beeng satisfied by construction as in shooting methods.</p>
<p>As a result, you only have pairwise dependencies between variables, unlike the dense activity graph in the previous figure for shooting methods.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc27.png" alt="" style="max-width: 400px">
    
    
</figure>
</p>
<p>These methods have:</p>
<ul>
<li>Good conditioning: changing $s_0, a_0$ has a similar effect as changing $s_H, a_H$.</li>
<li>Larger but easier to optimize search space. It is useful for contact-rich problems such as some robotics applications.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Only-reaches-local-optimum">
<a class="anchor" href="#Only-reaches-local-optimum" aria-hidden="true"><span class="octicon octicon-link"></span></a>Only reaches local optimum<a class="anchor-link" href="#Only-reaches-local-optimum"> </a>
</h5>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc28.png" alt="" style="max-width: 200px">
    
    
</figure>
</p>
<p>Some approaches try to avoid local optima like sampling-based methods: Cross-Entropy Methods (CEM) and $\text{PI}^2$.</p>
<p>For example, in CEMs, instead of just maintaining the optimal trajectory, it maintains the optimal trajectory's mean and covariance.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc29.png" alt="" style="max-width: 500px">
    
    
</figure>

<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc30.png" alt="" style="max-width: 500px">
    
    
</figure>

<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc31.png" alt="" style="max-width: 500px">
    
    
</figure>
</p>
<p>Despite being very simple, this works surprisingly well and has very nice guarantees on performance.</p>
<p>Why does this work?</p>
<ul>
<li>Search space of decision-time plans much smaller than space of policy parameters: ex. 30x32 vs 32x644x32</li>
<li>More feasible plans than policy parameters</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Slow-convergence">
<a class="anchor" href="#Slow-convergence" aria-hidden="true"><span class="octicon octicon-link"></span></a>Slow convergence<a class="anchor-link" href="#Slow-convergence"> </a>
</h5>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc32.png" alt="" style="max-width: 200px">
    
    
</figure>
</p>
<p>Gradient descent is too slow to converge, and we need to wait for thousands-millions of iterations to train a policy. But this is too long for a one-time plan that we want to through it away after.</p>
<p>Can we do something like Newton’s method for trajectory optimization, like non-linear optimization? YES!</p>
<p>We can approximate transitions with linear functions and rewards with quadratics:</p>
$$
\max_{a_0,...,a_H} \sum_{t=0}^H r_t, \quad s_{t+1} = f_s(s_t, a_t), \quad r_t=f_r(s_t, a_t)
$$$$
f_s(s_t, a_t) \approx As_t + Ba_t, \quad f_r(s_t, a_t) \approx s_t^TQs_t + a_t^TRa_t
$$<p>Then it becomes the Linear-Quadratic Regulator (LQR) problem and can be solved exactly.</p>
<p>For iLQR, locally approximate the model around the current solution, solve the LQR problem to update the solution, and repeat.</p>
<p>For Differential dynamic programming (DDP), it is similar, but with a higher-order expansion of $f_s$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Model-based-control-in-the-loop">
<a class="anchor" href="#Model-based-control-in-the-loop" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model-based control in the loop<a class="anchor-link" href="#Model-based-control-in-the-loop"> </a>
</h2>
<p>We want to answer this question of how to both learn the model and act based on that simultaneously?</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc33.png" alt="" style="max-width: 300px">
    
    
</figure>
</p>
<h3 id="Gathering-data-to-train-models">
<a class="anchor" href="#Gathering-data-to-train-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gathering data to train models<a class="anchor-link" href="#Gathering-data-to-train-models"> </a>
</h3>
<p>How can we gather data to train the model? this is a chicken or the egg problem. Bad policy leads to a bad experience, leads to a bad model, leads to bad policy ...</p>
<p>This leads to some training stability issues in practice. There are some recent works in game theory to provide criteria for stability. See <em>Rajeswaran et al (2020). A Game Theoretic Framework for Model Based Reinforcement Learning.</em> for example.</p>
<h4 id="Fixed-off-line-datasets">
<a class="anchor" href="#Fixed-off-line-datasets" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fixed off-line datasets<a class="anchor-link" href="#Fixed-off-line-datasets"> </a>
</h4>
<p>Another way to address this in the loop issues is to see if we can actually train from a fixed experience that is not related to the policy. Some options that we have are:</p>
<ul>
<li>Human demonstration</li>
<li>Manually-engineered policy rollouts</li>
<li>Another (sub-optimal) policy</li>
</ul>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc34.png" alt="" style="max-width: 400px">
    
    
</figure>
</p>
<p>This leads to a recent popular topic <em>model-based offline reinforcement learning</em>. You can see some recent works like <em>Kidambi et al (2020). MOReL: Model-Based Offline Reinforcement Learning.</em>, 
<em>Yu et al (2020). MOPO: Model-based Offline Policy Optimization.
See also: Levine et al (2020).</em>, and <em>Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Data-augmentation">
<a class="anchor" href="#Data-augmentation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data augmentation<a class="anchor-link" href="#Data-augmentation"> </a>
</h4>
<p>Another way to generate data is to use the model to generate data to train itself. For example, in <em>Venkatraman et al (2014). Data as Demonstrator</em>. You might have some trajectory of a real experiment that you got by taking certain actions; then you roll out the model and train to pull its predicted next states to true next states.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc35.png" alt="" style="max-width: 200px">
    
    
</figure>
</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc36.png" alt="" style="max-width: 400px">
    
    
</figure>
</p>
<p>There are also some adversarial approaches to generate data to self-audit the model like <em>Lin et al (2020). Model-based Adversarial Meta-Reinforcement Learning.</em> and <em>Du et al (2019). Model-Based Planning with Energy Models.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>But even if we do all of these works, models are not going to be perfect. We cannot have experience everywhere, and there will be some approximation errors always. These small errors propagate and compound. We may end up in some states that are a little bit further away from true data, which might be an unfamiliar situation. So it might end up making even bigger errors next time around and so on and so forth that the model rollouts might actually land very far away over time from where you would expect them to be.</p>
<p>What's worse is that the planner might actually intentionally <em>exploit</em> these model errors to achieve the goal.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc37.png" alt="" style="max-width: 200px">
    
    
</figure>
</p>
<p>This leads to a longer model rollouts to be less reliable.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc38.png" alt="" style="max-width: 400px">
    
    
</figure>
</p>
<p>You can check <em>Janner et al (2019). When to Trust Your Model:
Model-Based Policy Optimization</em> for more details.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Acting-under-imperfect-models">
<a class="anchor" href="#Acting-under-imperfect-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Acting under imperfect models<a class="anchor-link" href="#Acting-under-imperfect-models"> </a>
</h3>
<p>The question is that "Can we still act with imperfect models?" the answer is yes!</p>
<h4 id="Replan-via-model-predictive-control">
<a class="anchor" href="#Replan-via-model-predictive-control" aria-hidden="true"><span class="octicon octicon-link"></span></a>Replan via model-predictive control<a class="anchor-link" href="#Replan-via-model-predictive-control"> </a>
</h4>
<p>The first approach is to not commit to just one single plan (open-loop control) but continually re-plan as you go along (closed-loop control).</p>
<p>Let's see one example.</p>
<p>You might start at some initial state and create an imaginary plan using the trajectory optimization methods like CEM or other methods. Then apply just the first action of this plan. That might take you to some state that might not in the practice match with your model imagined you would end up with. But it's ok! You can just re-plan from this new state, again and again, take the first action and ... and by doing this, there is a good chance to end up near the goal.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc39.png" alt="">
    
    
</figure>

<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc40.png" alt="">
    
    
</figure>

<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc41.png" alt="">
    
    
</figure>

<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc42.png" alt="">
    
    
</figure>

<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc43.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By doing this, the errors don't accumulate. So you don't need a perfect model; just one pointing in the right direction is enough. This re-planning might be expensive, but one solution is to reuse solutions from previous steps as initial guesses for the next plan.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc44.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Plan-conservatively">
<a class="anchor" href="#Plan-conservatively" aria-hidden="true"><span class="octicon octicon-link"></span></a>Plan conservatively<a class="anchor-link" href="#Plan-conservatively"> </a>
</h4>
<p>We have'seen that longer rollouts become more unreliable. One solution would be just to keep the rollouts short. So we don't deviate too far from where we have real data. And as we saw in Dyna, just one single rollout can be also very helpful to improve learning.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc45.png" alt="">
    
    
</figure>
</p>
<p>The other option to plan conservatively is to consider a distribution over your models and plan for either the average or worst case wrt distribution over your model or model uncertainty.</p>
$$
\max_{\theta} E_{f \sim F} [\sum_t \gamma^t r_t], \quad a_t=\pi_{\theta}(s_t), \quad s_{t+1}=f_s(s_t, a_t), \quad r_t=f_r(s_t, a_t)
$$<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc46.png" alt="" style="max-width: 400px">
    
    
</figure>
</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc47.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Another option for conservative planning is to try to stay close to states where the model is certain. There are a couple of ways to do this:</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc48.png" alt="" style="max-width: 150px">
    
    
</figure>
</p>
<ul>
<li>
<p>Implicitly: stay close to past policy that generated the real data</p>
<ul>
<li>Peters et al (2012). Relative Entropy Policy Search</li>
<li>Levine et al (2014). Guided Policy Search under Unknown Dynamics.</li>
</ul>
</li>
<li>
<p>Explicitly: add penalty to reward or cost function for going into unknown region</p>
<ul>
<li>Kidambi et al (2020). MOReL: Model-Based Offline Reinforcement Learning.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the last two options for conservative planning, we need uncertainty. So how do we get this model uncertainty?</p>
<h3 id="Estimating-model-uncertainty">
<a class="anchor" href="#Estimating-model-uncertainty" aria-hidden="true"><span class="octicon octicon-link"></span></a>Estimating model uncertainty<a class="anchor-link" href="#Estimating-model-uncertainty"> </a>
</h3>
<p>Model uncertainty, if necessary for conservative planning, but it has other applications too that we will see later.</p>
<p>We consider two sources of uncertainty:</p>
<ol>
<li>Epistemic uncertainty<ul>
<li>Model's lack of knowledge about the world</li>
<li>Distribution over beliefs</li>
<li>Reducible by gathering more experience about the world</li>
<li>Changes with learning</li>
</ul>
</li>
<li>Aleatoric uncertainty/Risk<ul>
<li>World's inherent stochasticity</li>
<li>Distribution over outcomes</li>
<li>Irreducible</li>
<li>Static as we keep learning</li>
</ul>
</li>
</ol>
<p>There are multiple approaches to estimate these uncertainties, which are listed as follows:</p>
<ul>
<li>
<p>Probabilistic neural networks that try to model distributions over the outputs of your model.</p>
<ul>
<li>
<p>Model explicitly outputs means and variances (typically Gaussian)</p>
<p>$$ p(s_{t+1}|s_t, a_t) = N(\mu_{\theta}(s_t, a_t), \sigma_{\theta}(s_t, a_t))$$</p>
</li>
<li>
<p>Simple and reliable (supervised learning)</p>
</li>
<li>Only captures aleatoric uncertainty/risk</li>
<li>No guarantees for reasonable outputs outside of training data</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>Bayesian neural network</p>
<ul>
<li>
<p>Model has a distribution over neural network weights</p>
<p>$$ p(s_{t+1}|s_t, a_t) = E_{\theta}[p(s_{t+1}|s_t, a_t, \theta)]$$</p>
</li>
<li>
<p>Captures epistemic and aleatoric uncertainty</p>
</li>
<li>Factorized approximations can underestimate uncertainty</li>
<li>Can be hard to train (but an active research area)</li>
</ul>
</li>
</ul>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc49.png" alt="" style="max-width: 500px">
    
    
</figure>
</p>
<ul>
<li>Gaussian processes <ul>
<li>Captures epistemic uncertainty</li>
<li>Explicitly control state distance metric</li>
<li>Can be hard to scale (but an active research area)</li>
</ul>
</li>
<li>Pseudo-counts<ul>
<li>Count or hash states you already visited</li>
<li>Captures epistemic uncertainty</li>
<li>Can be sensitive to state space in which you count</li>
</ul>
</li>
</ul>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc50.png" alt="" style="max-width: 500px">
    
    
</figure>
</p>
<ul>
<li>Ensembles<ul>
<li>Train multiple models independently and combine predictions across models</li>
<li>Captures epistemic uncertainty</li>
<li>Simple to implement and applicable in many contexts</li>
<li>Can be sensitive to state space and network architecture</li>
</ul>
</li>
</ul>
<p>For discussion in the context of reinforcement learning, see <em>Osband et al (2018). Randomized Prior Functions for Deep Reinforcement Learning.</em></p>
<p>Between the above options, Ensembles are currently popular due to simplicity and flexibility.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Combining-planning-and-learning">
<a class="anchor" href="#Combining-planning-and-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Combining planning and learning<a class="anchor-link" href="#Combining-planning-and-learning"> </a>
</h3>
<p>We compared these two methods in previous sections and saw that background and decision-time planning have complementary strengths and weaknesses.</p>
<p>How to combine decision-time planning and background planning methods and get the benefits of both?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Distillation">
<a class="anchor" href="#Distillation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Distillation<a class="anchor-link" href="#Distillation"> </a>
</h4>
<p>In this approach, we gather a collection of initial states and run our decision-time planner for each initial state and get a collection of trajectories that succeed at reaching the goal. Once we collected this collection of optimal trajectories, we can use a supervised learning algorithm to train either policy function or any other function to map states to actions. This is similar to Behavioral Cloning (BC).</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/comb1.png" alt="">
    
    
</figure>

<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/comb2.png" alt="">
    
    
</figure>
</p>
<p>Some issues that can arise:</p>
<ul>
<li>
<strong>What is the learned policies that have compounding errors?</strong> If we rollout the policy from one of the states, it does something different than what we intended to do.</li>
</ul>
<ol>
<li>Create new decision-time plans from these states that have been visited by the policy.</li>
<li>Add these trajectories (new decision-time plans) to the distillation dataset (expand dataset where policy makes errors)</li>
</ol>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/comb3.png" alt="" style="max-width: 300px">
    
    
</figure>
</p>
<p>This is the idea of Dagger algorithm:</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/comb4.png" alt="" style="max-width: 300px">
    
    
</figure>
</p>
<ul>
<li>
<strong>What if the plans are not consistent?</strong> There are several ways to achieving a goal, and we've seen that by changing the initial condition only a little bit, the decision-time planner can give us pretty different solutions to reach a single goal. This chaotic behavior might be hard to distill into the policy</li>
</ul>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/comb5.png" alt="">
    
    
</figure>
</p>
<ol>
<li>we can make it so that the policy function that we are learning actually feeds back and influences our planner. </li>
<li>To do this, we can add an additional term in our cost that says stay close to the policy. $D$ in the below cost function is the distance between actions of the planner, $a_t$, and the policy outputs, $\pi(s_t)$. </li>
</ol>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/comb6.png" alt="">
    
    
</figure>

<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/comb7.png" alt="" style="max-width: 300px">
    
    
</figure>

<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/comb8.png" alt="" style="max-width: 400px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Terminal-value-functions-(value-of-the-terminal-state)">
<a class="anchor" href="#Terminal-value-functions-(value-of-the-terminal-state)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Terminal value functions (value of the terminal state)<a class="anchor-link" href="#Terminal-value-functions-(value-of-the-terminal-state)"> </a>
</h4>
<p>One of the issues with many trajectory optimizations or discrete search approaches is that the planning horizon is typically finite. This may lead to myopic or greedy behavior.</p>
$$
J^H = \sum_{t=0}^H \gamma^t r_t
$$<p>To solve this problem, we can use the value function at the terminal state and add it to the objective function. This learned value function guides plans to good long-term states. So the objective function would be infinite horizon:</p>
$$
J^{\infty} = \sum_{t=0}^{\infty} \gamma^t r_t = \sum_{t=0}^H \gamma^t r_t + \gamma^H V(s_H)
$$<p>This is another kind of combining decision-time planning (optimization problem) with background planning (learned value function).</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/comb9.png" alt="" style="max-width: 200px">
    
    
</figure>
</p>
<p>This can be used in both discrete and continuous action spaces:</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/comb10.png" alt="" style="max-width: 300px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Planning-as-policy-improvement">
<a class="anchor" href="#Planning-as-policy-improvement" aria-hidden="true"><span class="octicon octicon-link"></span></a>Planning as policy improvement<a class="anchor-link" href="#Planning-as-policy-improvement"> </a>
</h4>
<p>So far, we used policy (background) or decision-time planner to make a decision and generate trajectory and actions.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/comb11.png" alt="" style="max-width: 400px">
    
    
</figure>
</p>
<p>But we can combine them and use the planner as policy improvement. We can use the policy to provide some information for the planner. For example, the policy can output its set of trajectories, and the planner can use it as a warm start or initialization to improve upon. We would like to train the policy such that the improvement proposed by the planner has no effect. So the policy trajectory is the best that we can do. I think we can see the planner as a teacher for the policy.</p>
<p>Some related papers are listed here:</p>
<ul>
<li><em>Silver et al (2017). Mastering the game of Go without human knowledge.</em></li>
<li><em>Levine et al (2014). Guided Policy Search under Unknown Dynamics.</em></li>
<li><em>Anthony et al (2017). Thinking Fast and Slow with Deep Learning and Tree Search.</em></li>
</ul>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/comb12.png" alt="" style="max-width: 500px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Implicit-planning">
<a class="anchor" href="#Implicit-planning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Implicit planning<a class="anchor-link" href="#Implicit-planning"> </a>
</h4>
<p>In addition, to use a planner to improve policy trajectory, we can put the planner as a component <em>inside</em> the policy network and train end-to-end.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/comb13.png" alt="" style="max-width: 400px">
    
    
</figure>
</p>
<p>The advantage of doing this is that the policy network dictates abstract state/action spaces to plan in. But the downside of this is that it requires differentiating through the planning algorithm. But the good news is that multiple algorithms we've seen have been made differentiable and amenable to integrating into such a planner.</p>
<p>some examples are as follows:</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/comb14.png" alt="">
    
    
</figure>

<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/comb15.png" alt="">
    
    
</figure>

<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/comb16.png" alt="">
    
    
</figure>
</p>
<p>There are also some works that show the planning could <em>emerge</em> in generic black-box policy network and model-free RL training.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/comb17.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-else-can-models-be-used-for?">
<a class="anchor" href="#What-else-can-models-be-used-for?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What else can models be used for?<a class="anchor-link" href="#What-else-can-models-be-used-for?"> </a>
</h2>
<p>Consider we have a model of the world. We can use the model in a lot of different ways like:</p>
<ul>
<li>Exploration</li>
<li>Hierarchical Reasoning</li>
<li>Adaptivity &amp; Generalization</li>
<li>Representation Learning</li>
<li>Reasoning about other agents</li>
<li>Dealing with partial observability</li>
<li>Language understanding</li>
<li>Commonsense reasoning</li>
<li>and more!</li>
</ul>
<p>Here we're gonna just focus on the first four ways that we can use the model to encourage better behavior.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mu1.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Exploration">
<a class="anchor" href="#Exploration" aria-hidden="true"><span class="octicon octicon-link"></span></a>Exploration<a class="anchor-link" href="#Exploration"> </a>
</h3>
<p>One of the good things about having a model of the world is that you can reset to any state in the world that you might care about. It's not possible in all environments to reset like a continual learning problem. But if you have the model of the world, you can reset to any state you want.</p>
<p>We can also consider resetting to intermediate states in the middle of the episode as a starting point. The idea is to keep track of one of the interesting states and does exploration from there. So if you have the world's model, you can again reset to that state and efficiently perform additional explorations.</p>
<p>You can also reset from the final state rather than the initial state. This can be useful in situations where there is only a single goal state like Rubik's Cube. In this case, there is only one goal but maybe several possible starting states. So it would be useful to reset to the final state and explore backward from there rather than starting from the initial state.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mu2.png" alt="">
    
    
</figure>
</p>
<p>Another way that models can be used to facilitate exploration is by using <em>intrinsic reward</em>. In these cases, we want to explore places that we haven't been much so that we can gather data in those locations and learn more about them. One way to identify where we haven't been is to use model prediction error as a proxy. Basically, we learn a world model, then we predict what the next state is going to be and then take action and observe the next state and compare it with the predicted state and calculate the model error. We can then use this prediction error as a signal in the intrinsic reward to encourage the agent to explore the locations we haven't visited often to learn more about them.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mu3.png" alt="">
    
    
</figure>
</p>
<p>In addition to the above approach, we can also <em>plan to explore</em>. In <em>POLO</em> paper, rather than using the error from your prediction model, they use the error across an ensemble of value functions and use it as an intrinsic reward. Actually, at each state, we compute a bunch of different values from our ensemble of value functions, then take softmax over them to give us an optimistic estimate of what the value is going to be. We can use this optimistic value estimate as an intrinsic reward. We plan to maximize this optimistic value estimate, and then this allows us to basically, during planning, identify places that we should direct our behavior towards are more surprising or more interesting.</p>
<ul>
<li>Compute intrinsic reward during (decision-time) planning to direct the agent into new regions of state-space</li>
<li>Intrinsic reward = softmax across an ensemble of value functions</li>
</ul>
$$
\hat{V}(s) = log(\sum_{k=1}^K exp(k\hat{V}_{\theta_k}(s)))
$$<p>
</p>
<center>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/003rQ5vUcek?t=3" frameborder="0" allowfullscreen=""></iframe>
</center>

<ul>
<li><em>Lowrey et al. (2019). Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control. ICLR 2019.</em></li>
</ul>
<p>We can also use the same idea, but instead of using a set of disagreement across on ensemble of value functions, we can compute disagreement across transition functions. Now because we are just using state transitions, this turns into a task agnostic exploration problem. We can then plan where there is a disagreement between our transition functions and direct behavior towards those regions of space to learn a really robust world model. And then use this model of the world to learn new tasks either using zero-shot or few-shot (examples of experience).</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mu5.png" alt="">
    
    
</figure>
</p>
<p>Finally, another form of exploration is that if we have a model of possible states that we might find ourselves in, not necessarily a transition model but a density model over goals, we can sample possible goals from this density model and then train our agent achieve the goals.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mu6.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Hierarchical-reasoning">
<a class="anchor" href="#Hierarchical-reasoning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hierarchical reasoning<a class="anchor-link" href="#Hierarchical-reasoning"> </a>
</h3>
<p>A very classic way of doing hierarchical reasoning is what's called <em>task and motion planning (TAMP)</em> in robotics. You jointly plan symbolically at the task level, and then you also plan in the continuous space and do motion planning at the low-level—you sort of doing these things jointly in order to solve relatively long-horizon and multi-step tasks. For example, in the following figure, to control a robot arm and to get block $A$ and put it in the washer, wash it, and then put it in storage. In order to do this, you first have to move $C$ and $B$ out of the way and put $A$ into the washer, then move $D$ out of the way and then put $A$ into the storage. By leveraging symbolic representation, like PDDL from the beginning of the post, allows you to be able to jointly solve these hierarchical tasks.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mu7.png" alt="">
    
    
</figure>
</p>
<p>The other example of this is the OpenAI Rubik's Cube solver. The idea is that you use a high-level symbolic algorithm, Kociemba's algorithm, to generate the solution (plan) of high-level actions, for example, which faces should be rotated, and then you have a low-level neural network policy that generates the controls needed to achieve these high-level actions. This low-level control policy is quite challenging to learn.

</p>
<center>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/kVmp0uGtShk" frameborder="0" allowfullscreen=""></iframe>
</center>

<ul>
<li><em>OpenAI et al. (2019). Solving Rubik's Cube with a Robot Hand. arXiv.</em></li>
</ul>
<p>The question that might arise is that where does this high-level state-space come from?</p>
<p>We don't want to hand-code symbolically on these high-level roles that we want to achieve. Some model-free works try to answer this, but we focus on some MBRL approaches here for this problem.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mu8.png" alt="">
    
    
</figure>
</p>
<h4 id="Subgoal-based-approaches">
<a class="anchor" href="#Subgoal-based-approaches" aria-hidden="true"><span class="octicon octicon-link"></span></a>Subgoal-based approaches<a class="anchor-link" href="#Subgoal-based-approaches"> </a>
</h4>
<p>We can consider that any state you might find yourself in in the world as a subgoal. We don't want to construct a super long sequence of states to go through, but a small sequence. So the idea would be which states do we pick as a subgoal. Rather than learning a forward state transition model, we can learn a universal value function approximator, $V(s, g)$, that tells us the value of going from state $s$ to goal state $g$. We can train these value functions between our subgoals to estimate how good a particular plan of length $k$ is. A plan of length $k$ is then given by maximizing:</p>
$$
\text{arg}\max_{\{s_i\}_{i=1}^k} (V(s_0, s_1) + V(s_k, s_g) + \sum_{i=1}^{k-1} V(s_i, s_{i+1}))
$$<p>The figure below shows the idea. If you start from state $s_0$ and you want to go to $s_{\infty}$, you can break up this long plan of length one into a plan of length two by inserting a subgoal. You can do this recursively multiple times to end up with a plan of length $k$ or, in this case, a plan of length three.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mu9.png" alt="" style="max-width: 300px">
    
    
</figure>
</p>
<p>When we use a planner to identify which of these subgoals we should choose in order to maximize the above equation, in the figure below, you see which white subgoal it is considering as subgoal in order to find a path between the green and the blue points.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/maze.gif" alt="" style="max-width: 200px">
    
    
</figure>
</p>
<ul>
<li><em>Nasiriany et al. (2019). Planning with Goal-Conditioned Policies. NeurIPS.</em></li>
<li><em>Jurgenson et al. (2019). Sub-Goal Trees -- A Framework for Goal-Directed Trajectory Prediction and Optimization. arXiv.</em></li>
<li><em>Parascandolo, Buesing, et al. (2020). Divide-and-Conquer Monte Carlo Tree Search For Goal-Directed Planning. arXiv.</em></li>
</ul>
<h4 id="Skill-based-approaches">
<a class="anchor" href="#Skill-based-approaches" aria-hidden="true"><span class="octicon octicon-link"></span></a>Skill-based approaches<a class="anchor-link" href="#Skill-based-approaches"> </a>
</h4>
<p>Here, rather than identifying discrete states as subgoals that we want to try to achieve, what we want to do is to learn a set of skills that sort of fully parametrize the space of possible trajectories that we might want to execute. So, for example, in the Ant environment, a nice parametrization of skills would be to say a particular direction that you want to get to move in. So the approach taken by this paper is to learn a set of skills those outcomes are both (1) easy to predict, so if you train a dynamics model to predict the outcome of executing the skill, and (2) the skills are diverse from one another. That's why you get this nice diversity of the ant moving in different directions. This works very well for zero-shot adaptation to new sequences of goals. As you can see on the bottom, this is an ant going to a few different locations in space, and it is doing this by just pure planning using this set of skills that it is learned during the unsupervised training phase.</p>
<ul>
<li>Learn a set of skills whose outcomes are (1) easy to predict and (2) diverse</li>
<li>Learn dynamics model over skills, and plan with MPC</li>
<li>Can solve long-horizon sequences of high-level goals with no additional learning</li>
</ul>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/ant.gif" alt="" style="max-width: 500px">
    
    
</figure>
</p>
<ul>
<li><em>Sharma et al. (2020). Dynamics-Aware Unsupervised Discovery of Skills. ICLR.</em></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="representation-Learning">
<a class="anchor" href="#representation-Learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>representation Learning<a class="anchor-link" href="#representation-Learning"> </a>
</h3>
<p>Beyond just using models for prediction, they can be used as regularizers for training other types of representations that then you can train a policy on.</p>
<p>One way is to learn a model as an <em>auxiliary loss</em>. For example, if you have an A2C algorithm and add an auxiliary loss to predict the reward it's gonna achieve, in some cases, you can get a large improvement in performance by just adding this auxiliary loss. By considering this loss during training, we are actually forcing it to learn the future and capture the structure of the world, which is useful. We also don't use this learned model in planning and just for representation learning.</p>
<ul>
<li><em>Jaderberg et al. (2017). Reinforcement learning with unsupervised auxiliary tasks. ICLR 2017.</em></li>
</ul>
<p>The other same idea is to use a <em>contrastive loss</em>, like CPC paper (below), that tries to predict what observations it might encounter in the future, and by adding this additional loss during training, we see improvement in performance.</p>
<ul>
<li><em>van den Oord, Li, &amp; Vinyals (2019). Representation Learning with Contrastive Predictive Coding. arXiv.</em></li>
</ul>
<p>Another idea is <em>plannable representations</em> that make it much easier to plan in. For example, if we are in a continuous space, we can discretize it in an intelligent way that might make it easy to use some of these discrete search methods, like MCTS, to rapidly come up with a good plan of actions. Or maybe we can come up with a representation for our state space such that moving along a direction in the latent state space corresponds to planning. So you can basically just interpolate between states in order to come up with a plan.</p>
<ul>
<li>Learn an embedding of states that is easier to plan in, e.g.<ul>
<li>Discretized</li>
<li>States that can be transitioned between should be near to each other in latent space!</li>
</ul>
</li>
<li>Related to notions in hierarchical RL (state abstraction)</li>
</ul>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mu10.png" alt="" style="max-width: 300px">
    
    
</figure>
</p>
<ul>
<li><em>Corneil et al. (2018). Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation. ICML.</em></li>
<li><em>Kurutach et al. (2018). Learning Plannable Representations with Causal InfoGAN. NeurIPS.</em></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Adaptivity-&amp;-generalization">
<a class="anchor" href="#Adaptivity-&amp;-generalization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adaptivity &amp; generalization<a class="anchor-link" href="#Adaptivity-&amp;-generalization"> </a>
</h3>
<p>Models of the world can also be used for fast adaptation and generalization.</p>
<p>The world can be changed in two different ways:</p>
<ul>
<li>
<strong>Change in rewards</strong>. So we're being asked to do a new task, but the dynamics are the same.</li>
<li>
<strong>Change in dynamics</strong>. </li>
</ul>
<p>Based on the above changes, we can do different things in response to them.</p>
<p>In a model-free approach, we just adapt to the policy. But this tends to be relatively slow because it's hard to quickly adapt changes in rewards to the same dynamics and vice versa because they are sort of entangled with each other.</p>
<p>If we have an explicit model of the world, we can update our behavior differently. One option would be that we can adapt the planner, but we can also adapt the model itself, or we can do both.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mu11.png" alt="">
    
    
</figure>
</p>
<h4 id="Adapting-the-planner-in-new-states">
<a class="anchor" href="#Adapting-the-planner-in-new-states" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adapting the planner in new states<a class="anchor-link" href="#Adapting-the-planner-in-new-states"> </a>
</h4>
<p>A pre-trained policy may not generalize to all states (especially in combinatorial spaces). So some states that we might find ourselves in might be required harder or more reasoning, and others may require less.
We have to try to detect when planning is required, and they adapt the amount of planning depending on the difficulty of the task. For example, in the following gifs, in the upper case, the n-body agent can easily solve the task and reach the center's goal using just a couple of simulations. But in the bottom case, it is much harder to reason about because it starts on one of the planets, which requires many more simulations. We can adaptively change this amount of computation as needed. Save the computation on easy scenes and then spend it more on the hard ones.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/easy.gif" alt="" style="max-width: 300px">
    
    
</figure>

<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/hard.gif" alt="" style="max-width: 300px">
    
    
</figure>
</p>
<ul>
<li><em>Hamrick et al. (2017). Metacontrol for adaptive imagination-based optimization. ICLR 2017.</em></li>
<li><em>Pascanu, Li, et al. (2017). Learning model-based planning from scratch. arXiv.</em></li>
</ul>
<h4 id="Adapting-the-planner-to-new-rewards">
<a class="anchor" href="#Adapting-the-planner-to-new-rewards" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adapting the planner to new rewards<a class="anchor-link" href="#Adapting-the-planner-to-new-rewards"> </a>
</h4>
<p>Here is another same idea in a life-long learning setup where the reward can suddenly change, and either the agents can observe the change in the reward, or they just have to infer the reward has changed. Because of changes in reward, it needs more planning because the prior policy is less reliable, and more planning allows you to better explore these different options for the reward function. In the video below, as you can see in the bottom agent after the reward is changed, the agent needs to do more planning to have a nice movement compared to the other two agents. 

</p>
<center>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/3T3QuKregt0" frameborder="0" allowfullscreen=""></iframe>
</center>

<ul>
<li><em>Lu, Mordatch, &amp; Abbeel (2019). Adaptive Online Planning for Continual Lifelong Learning. NeurIPS Deep RL Workshop.</em></li>
</ul>
<h4 id="Adapting-the-model-to-new-dynamics">
<a class="anchor" href="#Adapting-the-model-to-new-dynamics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adapting the model to new dynamics<a class="anchor-link" href="#Adapting-the-model-to-new-dynamics"> </a>
</h4>
<p>For the times that the dynamics change, it could be very useful to adapt the model. One way to approach this is to train the model using the meta-learning objective so that during training, you're always training it to adapt  to a slightly different environment around you, and at the test time, you actually see a new unobserved environment that you never saw before, you can take a few gradient steps to adapt the model to deal with these new situations. Here is an example where the agent, half cheetah, has been trained to walk along some terrain, but it's never seen as a little hill before. Therefore, the baseline methods that cannot adapt their model cannot get the agent to go up the hill, where this meta-learning version can get the cheetah to go up the hill. 

</p>
<center>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/ejG2nzCNdZ8?t=144" frameborder="0" allowfullscreen=""></iframe>
</center>

<ul>
<li><em>Nagabandi et al. (2019). Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning. ICLR.</em></li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="kargarisaac/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/fastpages/jupyter/2020/10/26/mbrl.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My posts about Machine Learning</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/kargarisaac" title="kargarisaac"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/eshagh-kargar" title="eshagh-kargar"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/kargarisaac" title="kargarisaac"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
