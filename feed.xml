<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://kargarisaac.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kargarisaac.github.io/blog/" rel="alternate" type="text/html" /><updated>2021-05-24T06:25:48-05:00</updated><id>https://kargarisaac.github.io/blog/feed.xml</id><title type="html">Isaac Kargar</title><subtitle>My posts about Machine Learning</subtitle><entry><title type="html">Offline Reinforcement Learning</title><link href="https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/29/offline-rl.html" rel="alternate" type="text/html" title="Offline Reinforcement Learning" /><published>2020-10-29T00:00:00-05:00</published><updated>2020-10-29T00:00:00-05:00</updated><id>https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/29/offline-rl</id><content type="html" xml:base="https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/29/offline-rl.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-10-29-offline-rl.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;what-makes-modern-machine-learning-work?&quot;&gt;what makes modern machine learning work?&lt;a class=&quot;anchor-link&quot; href=&quot;#what-makes-modern-machine-learning-work?&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;so to start off let's start with a big question what is it that makes machine learning work you know i'm going to give a
simple answer to a big question this is
going to be
maybe somewhat controversial but perhaps
many of you will agree with this at
least at a very very high level
i think that what makes machine learning
work today is really
the combination of large and highly
diverse data sets
and large and high capacity models and
what we've seen time and time again
is that across a range of domains from
image recognition
to machine translation to text
recognition and speech recognition
this kind of formula seems to be the
formula that leads to good results
if you collect a large data set like
imagenet or ms coco
and then train a huge model with dozens
or even hundreds of layers
that's going to be the thing that leads
to very good performance and arguably
the widespread enthusiasm about machine
learning in recent years
has really been spurred on by
applications
that follow this basic formula
so what about reinforcement learning
what about uh using
learning to figure out how to make
decisions
well uh reinforcement learning is
fundamentally
at least in the textbook setting an
active learning framework where you have
an agent
that interacts with the world collects
some experience
uses that experience to update its model
policy or value function
and then collects more experience and
this process is repeated many many times
i and we've seen that this basic recipe
does lead to good results across
a range of domains from from playing
video games to basic robotic
locomotion and manipulation skills and
even to play in the game of go
however when you want
learning systems that generalize
effectively to large-scale real-world
settings
like the ones that i showed on the
previous slide you still have to collect
large and diverse data sets and in an
active learning framework
this means that each time the agent
interacts with the world they need to
collect
a breadth of experience that covers the
sort of situations they might see in the
world
for example if you imagine using
reinforcement learning to train an
autonomous driving system
now this system might need to practice
driving in
many many different cities each time it
updates the model
so it goes and drives in san francisco
new york berlin
london updates the model and goes back
to san francisco new york berlin and
london
now this very quickly becomes
impractical and indeed
if we look at the kind of domains in
which modern reinforcement learning
algorithms have been successful
and then contrasted side by side with
the kind of domains where we've seen the
success of supervised learning methods
that can leverage very large and diverse
data sets
we see that there's a really big gulf
it's not that the reinforcing learning
algorithms are not capable they're
learning very sophisticated strategies
very sophisticated behaviors
but they're not exhibiting the same kind
of open world generalization
all of the domains that i'm showing on
the left can all be characterized as
closed world settings
we know exactly what the rules of go are
we know how the emulator for the video
game works even the robotics application
is a laboratory setting
whereas all the applications shown on
the right are open world domains
images mined from flickr from all over
the world
natural text natural speech collected
from real human beings
actually speaking or writing text in the
real world
so if we want to bridge this gulf of
generalization
what can we do how can we bring
reinforcement learning into these open
world settings
to address the kind of applications that
we actually want
well i'm going to posit that in order to
enable this we really need to develop
data-driven reinforcement learning
methods and data driven
means that you need to be able to use
large and diverse previously collected
data sets
in the same way that supervised learning
algorithms can utilize large and diverse
previously collected data sets
the classic textbook version of
reinforcement learning
is really an on policy formulation in an
on policy reinforcement learning
algorithm
you have a setting where each time the
agent updates its policy
it has to go and collect new data
a very common area of study in
reinforcement learning is also to study
off policy reinforcement learning
algorithms and these algorithms can
utilize previous data
but they typically still learn in an
active online fashion
meaning that the agent interacts with
the world collects some data
adds that data to its buffer and then
uses the latest data and all pass data
to improve its policy
and we might think that we could simply
cut that connection we could simply use
prior data
using the same exact algorithms but as
i'll discuss in today's talk
that actually doesn't work very well and
if we really want data driven
rl algorithms then we need to come up
with some new algorithmic techniques to
make this possible
and develop a new class of what i'm
going to call offline reinforcement
learning methods
these have also been called batch
reinforcement methods in the literature
the idea here is that a data set was
collected previously
with some behavior policy that i'm going
to note pi beta in general you might not
even know what pi beta was maybe it was
a person driving a car a person
controlling the robot
maybe it was the robot's own experience
and other tasks whatever it was
it collected a buffer of data a large
and diverse data set
that covers many different open world
situations and now you have to learn the
best policy you can
for your current task using just that
data without the
luxury of interacting with the world
further to try out uh different methods
now of course in reality you might want
something more like a hybrid in reality
maybe
what we have is we have a big data set
from all the past interactions that our
agent has had
maybe you have a robot that has cleaned
the kitchen and mocked the floors and
did many other things and that's sort of
it's it's foundational data it's it's
imagenet
style data set that it's going to use to
get generalizable skills
we're going to use that data set in an
offline rl fashion
and then perhaps we'll actually come
back and interact with the world a
little bit more
just to fine-tune that skill just to
collect task specific
data in small amounts and i'll actually
discuss some methods that can
do this as well but the main component
the main challenge of this recipe is
really the first part is using large and
diverse previous data sets
to get the best policy you can without
having to revisit all those diverse and
varied situations
without having the car go back and drive
in san francisco and new york and berlin
and london each time its policy changes
and if we can accomplish this if we can
build algorithms that have this
capability
then we will not only be able to train
better policies for
robots and things like that but we'll
also be able to apply reinforcement
learning
to domains where conventionally it has
been very difficult to use
for instance we could imagine using past
data to use reinforcement learning
to train a policy for advising doctors
on treatment plans and diagnoses
it would be pretty crazy to imagine an
active exploration procedure for doing
this because that would result in
enormous costs and liability but if you
can use previously expected offline data
to get the best decision making system
you can to support doctors in a medical
setting
well that seems like a pretty good
recipe you can imagine the same
procedure being applied to scientific
discovery problems
controlling power plants chemical plants
operations research
logistics and inventory management
finance in all of these domains
there's ample previously collected
historical data
but it's extremely difficult and costly
to run active online data collection so
we would expect that these would be the
domains that could be revolutionized
with effective offline reinforcement
learning algorithms
so in today's talk i'm going to cover
the following topics
first i'll discuss why offline
reinforcement learning is difficult
then i'll talk about some basic recipes
for designing offline rl algorithms and
a little bit about recent progress in
this area i'll talk about
some of our recent work on model based
offline rl
and then i'll discuss a new algorithm
that we're pretty excited about called
conservative q learning
which seems to be a way to do offline
url that works quite a bit better
than many of the previous methods and
then i'll conclude with a discussion
of how we should be evaluating our
offline reinforcement learning
algorithms
and discuss a benchmark that we've
developed recently called d4rl
all right but let's start with the first
topic let's talk about what makes
offline reinforcement learning hard
and why we can't use our standard off
policy rl algorithms
to solve it so first
let's start with a quick primer on off
policy reinforcement learning
in a standard reinforced learning
problem you have an agent that interacts
with the world by selecting actions
and the world responds with states and
the rewards the consequences of those
actions
what the agent needs to do is select a
policy which i'm going to denote as pi
of a given s
and that policy needs to be selected so
as to maximize
the reinforced learning objective which
is the sum of expected rewards over all
time
so the agent doesn't just take the
action that yields the highest reward
right now
it's supposed to take the action that
yields the highest rewards
in the long run now a very useful object
for maximizing this reinforcement
learning objective
is the q function the q function tells
you
if you start a particular state take a
particular action
and then follow your current policy pi
what is the total reward that you will
accumulate
and if you can somehow determine the q
function for a given policy
then you can always improve that policy
by taking an action with probability one
if it is the r max of the q function and
zero otherwise
so this greedy policy with respect to q
pi will always be at least as good or
better than the previous pi
and you can also skip the middleman and
directly enforce
the bellman optimality equation for all
state action tuples
so if you may if you can make qsa equal
to rsa
plus the max of the q value at the next
time step then you can also show
that recovering the greedy policy from
this will get you the optimal policy
and one way you could do this is you
could enforce this equation at all
states
by minimizing the difference between its
left-hand side and right-hand side
now this is all fairly basic stuff this
is kind of what we learned
uh you know when we first learned about
reinforcement learning it's textbook
rl but you know as many of you might
already know
the nice thing about this equation is
that you don't necessarily need
on policy data to enforce it now the
stock is going to focus pretty much
entirely on approximate dynamic
programming methods basically
q learning style methods or after critic
methods there are many other methods for
offline rl
based more around policy gradient style
estimators and i'm not going to talk
about this so we'll basically start with
q learning and actor critic and sort of
stay there just as a as a warning
okay so this procedure that i've laid
out here this classic q learning
approach
is an off policy algorithm which means
that you don't need on policy data
to make the left-hand side and
right-hand side of the bellman
optimality equation
equal to each other so you could imagine
an off policy
include q learning method or a theta q
iteration procedure that has the
following basic recipe this is sort of
classic stuff from literature collect
the data set of transitions
using some policy add it to a buffer
sample a batch from that buffer minimize
the difference between the left hand
side and right hand side
of the bellman optimality equation and
then repeat this process some number of
times
and then periodically you could go out
and collect more data
you could also think of this graphically
you interacted with the world
collected a data set of transitions
you're going to run your q-learning
procedure on that data set
and then periodically go back and
explore and of course if we're doing
offline rl then this is the part that we
would omit we would just
take our buffer and just keep crunching
away with q learning on that buffer
so this is an off policy algorithm and
it can accommodate previously collected
data in principle
so does it solve the awful nrl problem
and if not what goes wrong
well we studied this question a little
bit so what i'm going to show next is
some results from a large-scale
reinforced learning project that we
actually did at google a couple of years
back called qt opt
our goal in this project is to develop a
highly generalizable and powerful
robotic grasping system that could learn
to grasp any object
directly from raw images but in the
process of doing this research
we actually evaluated a little bit how
these q learning style methods compare
in the fully offline regime
versus the regime where they're allowed
to collect additional online data
now here there's you know everything is
kind of scaled up so there are seven
different robots that are all collecting
lots of data in parallel they're pushing
it to a
decentralized uh distributed buffer
storing all the data from past
experiments and there are multiple
workers that are actually updating q
values
on everything in the buffer
and we looked at how the system worked
it actually worked really well
it could pick up novel objects heavy
objects awkward objects i could respond
to perturbations that i'd never seen
before so basically
when we scale up reinforcement learning
it does actually generalize something
is really working and we're seeing
generalization that kind of resembles
the kind of
uh open world generalization that we saw
in the supervised system
of course here one might argue it's not
entirely open world it's still in the
laboratory
but it is generalizing to never before
seen objects
so we evaluated this method on a test
set of objects of the policy never saw
during training we actually bought
entirely new objects just to ensure that
they were not in the lab
during the training process uh and
and then we compared the algorithm train
in fully offline mode
meaning using just the data collected
before trainable policy and see how it
does and this was fairly good offline
data it came from past rl experiments
and we also evaluated the algorithm in
fine-tuned mode where
the policy was allowed to collect a
little bit of additional data and
fine-tune online the offline data set
consisted of
580 000 uh episodes
so this is a very large grasping episode
the online fine tuning added another 28
000.
so the amount of additional online data
was pretty negligible
so it was clearly not the increase in
the data set size it was really that was
online
and we saw the following numbers the
success rate was 87
for the offline method and 96 with the
additional online fine-tuning
and while this might seem like a small
difference if we rewrite this as error
rates we have an error rate of 13
for the offline method and four percent
uh for the fine-tuned method so
less than a third of the number of
mistakes that's a pretty big difference
actually
so the system clearly works very well
but
you're getting three times fewer
mistakes
if you fine-tune you believe in a small
amount of data so something about the
fully offline setting
seems to be pretty hard now
more recently uh we actually studied
this question
in a paper called stabilizing off policy
queue learning via bootstrapping error
reduction
by overall kumar justin food george
tucker and myself
and we want to understand what what are
the reasons why this might be so
difficult
we had a few hypotheses that we wanted
to investigate one hypothesis we had was
maybe there's some kind of or fitting
effect maybe when you train on offline
data
if you train too long you sort of
overfit to that data and you start
seeing bad performance
but if it was an overfitting effect then
what we would expect
is that increasing the data set size
should decrease the problem
so what this plot shows is offline rl
performance
on the half cheetah benchmark and
different colors denote different data
sets so
blue is 1000 uh transitions and
red is 1 million transitions
and you can see that there is virtually
no difference between 1 000 and 1
million so
if there is an overfitting effect it is
not the conventional kind of overfitting
it doesn't seem to go away as you add
more data
now we also looked at how well the q
function thought it was doing
and meaning if you actually look at the
q values
for the current policy that tells you
how well the q function
thinks it's going to do when it's
executed in the world
and this is a plot showing that now
something to note here is that the y
axis here is actually a log scale so
what this is showing is
enormous overestimation actual
performance is below zero
estimated performance is between ten to
the seventh and ten to the twentieth
power
so the q function thinks it'll do
extremely well and it's doing extremely
poorly
well that's kind of weird um another
hypothesis we had as well
maybe the trouble is just the training
date is just not good like you know
maybe the best you can do with the state
is -250
now one way that you can evaluate that
hypothesis you guys should look at the
best
transitions in the data set if you just
copy the best transitions
how will we do we'll actually do
actually pretty well so
this is usually not the case in these
offline rl problems it's not that the
training data doesn't have good behavior
it's that somehow the q function becomes
excessively confident and optimistic
about actions that are not actually very
good
to understand what's happening here we
first have to understand distributional
shift
so here's distribution shift in a
nutshell
when we run supervised learning we're
typically solving what's called an
empirical risk minimization problem
so we have some data distribution p of x
we have some label distribution p of y
given x
our training set consists of samples
from p of x and p of y given x
and then we're going to minimize some
loss on those samples
and if we're doing vellum bellman error
minimization then we're minimizing the
squared error
between the values predicted by our
function f theta
and the target values y now when we run
empirical risk minimization
we could ask the question given some new
data point x star
is f theta of x star going to be correct
well one thing that we do know is if we
had enough samples
then the expected value of the error
under our training distribution should
be low that's what generalization means
generalization means the training error
which is the empirical risk is
representative of
generalization if you have a large
training set minimizing training error
is going to minimize generalization
error unless you overfit
but that doesn't necessarily mean that
we're going to make an accurate
prediction
on some other point x star
for example the expected value of our
error under some other distribution over
x
p bar of x is not in general going to be
low
unless p bar of x is equal to p of x so
our error under a different
input distribution might be in fact very
high
in fact even if x star was sampled from
p of x we're not guaranteed that error
on x star is low because we're
minimizing expected error
so we might still have some points with
higher we're not minimizing point wise
there or just expected error
now you might say that usually when we
train deep neural networks
we're not too worried about this kind of
distributional shift because deep
networks are really powerful they
generalize really really well
so maybe it's okay but what if we select
x star so as to maximize f theta of x
see we might have a function that fits
the true function really well
let's say that the green curve
represents the true function and the
blue curve represents our fit
mostly our fit is extremely good however
if we pick the largest value of f theta
of x
um which point are we going to land in
we're going to land exactly on that peak
we're going to find the point that has
the largest error in the positive
direction
so even if we're generalizing really
really well even if we're doing well for
x star points even ones outside of the
distribution
if we maximize the x we're going to get
big errors
so what does this have to do with q
learning well
let's look at our target values in q
learning it's r of s
a plus the max over a prime of q of s
prime comma a prime
and i'm going to rewrite this in kind of
a funny way i'm going to write this
as r plus the expected value of a prime
under the policy pi nu where pi nu is
this r max policy so pi nu
assigns the probability of 1 to the r
max now this is kind of just a really
weird way of writing the max
but i think it makes the distribution
shift problem much more apparent
so let's say that our target values are
called y
what is the objective for the q function
well it's to minimize
the empirical risk the empirical error
against y
under the distribution over states and
actions induced by our behavior policy
pi beta
which means that we would expect q to
give us accurate estimates of q values
under pi beta
right pi beta is the behavior policy
so we expect good accuracy when pi beta
is equal to pi nu so if pi nu is pi beta
then we're fine we're going we're going
to have lower estimates
but how often is that true pi news
chosen to maximize the q value so unless
pi beta
was actually also maximizing those same
q values before it even knew what they
were
these things are not going to match and
it's even worse
pioneer is selected to be the r max
and if you think back to the previous
slide when you select your point with a
max
you get some bad news and that's why we
see
on the safchita experiments that the
policy does poorly
whereas the q function thinks it's doing
really well because it's finding the
actions
with the largest error in the positive
direction and that's why just naively
applying off-pulse crl methods to the
offline setting
does not in general yield great results
so we have to somehow combat this
overestimation
issue we have to combat the
distributional shift
in the action space so let's talk about
how we can do this
how do we design offline rl algorithms
well one very large class of methods in
the literature and this is summarized in
a tutorial
that we assembled recently called
offline reinforcement tutorial review
and perspectives on open problems
one very large class of methods is what
we're going to call policy constraint
methods
so it's easiest to describe policy
constraint methods in the actor critics
setting but they can be applied in the q
learning setting too
so here we're going to update our q
function just like before using the
expected value under pi new
and then we'll update our policy not
just by taking the r
max but by taking the rmax subject to a
constraint
that some measure of divergence between
pi nu and pi beta
is bounded by epsilon so the idea is
that if
pi nu stays close to pi beta then the
distributional shift
will be bounded and therefore the
effective auto distribution actions will
be bounded
and then you repeat this process so it's
just like an actor critic algorithm
only with this additional constraint
against the behavior policy
so does a solid distributional shift
does it mean that we have no more
erroneous values
well to a degree it actually does so
there's a
pretty large class of methods that have
explored various kinds of policy
constraints
uh it's it's a very old idea we coined
the term policy constraint i think in
our tutorial
but it's this these kinds of approaches
have been called many things in
literature
including kl divergence control uh
maximum prl linearly cell blm dps all
sorts of things
and there are many uh researchers that
have studied this for decades
not just in the offline rl literature
but in the url literature more broadly
more recently this has been used
extensively for various off policy and
offline rl methods
here is just a collection of recent
citations kind of from the last five
years
that have done some variant of this
now this does have a number of problems
one problem is that
this kind of approach might be way too
conservative
imagine the following scenario let's say
that you have
a behavior policy that is
highly random in some places and almost
deterministic in other places
now when the behavior policy is highly
random you actually get pretty good
coverage of actions
and the effect of out of distribution is
actually minimized in fact if the
behavior policy was
uniformly random nothing would be out of
distribution because it has full support
but if you have a policy that is highly
random in some places and highly
deterministic in others
it's actually very difficult to pick a
value of epsilon that works
because if you if you pick a value of
epsilon that is too low
then in the places where the behavioral
policy is highly random you'll be able
to do all sorts of interesting things in
its support
but in the places where it's not highly
random you might take a really bad
action
right so imagine it's kind of close to
deterministic when it's
crossing a narrow bridge where with some
small probability it falls off
if you admit uh that amount of error you
might fall off the bridge
if you use a tighter constraint if you
limit the amount of deviation from the
behavior policy very strictly
then you won't fall off the bridge
you'll match it in that highly
deterministic region will you also be
forced
to match the highly random distribution
in the region where it's random and
that's just useless right
if the policy is being very random being
just as random
doesn't seem to give you anything like
if it's random you can just choose
within that support whatever you want
so it's hard to pick a single value of
epsilon that works better
now one thing that can mitigate this is
to use a support constraint
basically say it's not that you have to
match the distribution you have to match
the support of that distribution
so if the distribution is highly random
you can do sort of anything you want
within that support
but if it's highly deterministic you
really need to do whatever it did
because you don't have any wiggle room
to go out of support and we explored
this a little bit
uh in a in a paper from 2019 that
introduced an algorithm called bear
which uses support constraint the second
issue
which is a bit tougher to deal with and
actually pretty problematic in practice
is that estimating the behavior of the
the behavior policy itself
in order to enforce this constraint can
be very difficult
if you don't know what the behavior
policy was maybe the data was collected
by a human
you have to somehow figure out what
policy that human was following
and if you make a mistake in figuring it
out then your constraint might be
incorrect
so for example if you fit the behavior
policy with supervised learning
we know that supervised learning will
make certain mistakes it has kind of a
moment matching effect which means that
it will average across modes
which means you might have high
probability actions under your behavior
policy
that at a very low probability under the
true behavior policy
and when you enforce a constraint
against that the constraint will be
highly imperfect
and this can lead to very poor
performance in practice
now when is estimating the behavior
policy especially hard
well it's especially hard when the
behavior policy actually consists of a
mixture of multiple different behaviors
and this is actually exactly the setting
that we want because remember i
motivated all of this
by talking about the setting where you
have a large and highly diverse data set
and that's exactly what you would expect
that your
behavior policy would actually be a
mixture of many different policies
and at that point estimating it with a
single neural network is actually very
hard
so the easy case is when all the data
comes from the same markovian policy
but this is not very common or very
realistic uh you know if your data came
from humans they're certainly not going
to be markovian if it came from many
past tasks you've done
while each individual test might be
markovian the mixture might not be
so the hard case is where the data comes
from many different policies
and this is very common in reality and
it's also very common when you're doing
online fine-tuning so if you remember
when i motivated all this i also said
that we want to collect lots of data
from past behaviors train a policy with
offline rl and then maybe fine tune it
further with a little bit of online data
so let's use this online fine-tuning as
a kind of test case
in reality what we care about is the
setting where data comes from many
different policies but the online
fine-tuning situation
is a nice sort of petri dish in which to
explore this problem
so i'm going to talk about a method that
we've developed that specifically
targets that setting
where first you have offline training on
a large previously collected data
set and then you have some online
fine-tuning and during online
fine-tuning you're adding more data
from many different policies
so this is work by uh two of my students
astronautier and amazon gupta together
with an undergraduate student
named mortis and so the experiment is
that we do online fine-tuning from
offline initialization
and what i'm showing in this plot is the
log likelihood
of the behavior policy fit so this is
for one of these standard
policy constraint methods in this case
bayer and the left side shows the
offline training so you can see during
offline training we have pretty good log
likelihood for fitting our behavior
policy
but during online training when we have
these additional samples from
many different policies being pushed to
the buffer the likelihood
of the policy estimate drops
precipitously it drops very sharply for
the online data and even drops for the
old offline data because
you have to also model the new offline
data so you do worse on the offline
offline data so so our fit
gets bad and in fact we can see that uh
this method this is bare but this would
be true i think for for most policy
constraint methods
uh it doesn't do so well so this is just
showing the online fine tuning
and you're gonna have two choices you
can use a strict constraint or a loose
constraint
if you use a strict constraint then you
do a little bit better at the beginning
that's the yellow line
but you fail to improve because as the
behavior policy deteriorates
that strict constraint just basically
causes everything to get stuck
if you use a loose constraint then you
improve over the course of online
training but you start off in a really
bad place
and in this case it's no better than
just initializing from scratch
and in fact this general principle is
borne out in a few other papers for
example
this paper i have cited here called maq
shows that using a much more powerful
class of behavior policies
leads to substantial improvement
implying behavior policy modeling
is really a major bottleneck for these
policy constraint methods
so one of the things we could do is we
could actually enforce a constraint
without explicit modeling of the
behavior policy so this is a kind of
implicit constraint method
so here's the idea the problem we want
to solve
is this constrained argmax
it turns out that you can prove that the
solution to this constrained
optimization problem
can be expressed in closed form and the
way that you do this is you write down
the lagrangian
solve for the dual and then you can
actually write down the optimum for the
dual
and the optimal solution is given by one
over z times the behavior policy
times the exponential of the advantage
function
and the advantage function is multiplied
by one over lambda where lambda
is a lagrange multiplier and this is
straightforward to show using a duality
argument
uh we didn't actually come up with this
this is this is actually been brought up
in a number of previous papers including
the
reps papers by peters at all uh the
rolex at all sci learning paper and many
others this is kind of actually a very
well known identity
in kl regularized control
but the interesting thing about this
equation is that it shows
that we can approximate the solution to
the constrained optimization
using a weighted maximum likelihood
objective
and the way to do this is to note that
matching this pi
star is the same as
maximizing the likelihood of actions
taken from
pi star and you can do that by taking
action instead from pi
beta and weighting them by the
exponentiated advantage
so the if you can find the r
max of the expression i have written
here then you will get pi star
provided you can match pi star exactly
and the the cool thing about this
expression now is that
pi beta only shows up as the
distribution the expectation so even
though you need pi beta for this
in reality all you actually need is
samples from pi beta which means that
you do not need to actually know the
distribution pi beta
and samples from pi beta that's exactly
uh what our data set is composed of
so we no longer need to estimate the
behavioral policy we just need samples
from it and our data set already
contains those
and of course the advantage we get from
our critic so you could imagine an actor
critic algorithm
which updates the critic to get the
advantages and then updates the policy
using this weighted maximum likelihood
expression
all right so does this procedure work
well if we look at some results from the
paper we evaluated on these kind of
dexterous manipulation tasks
where we had a data set from human
demonstrations
uh with a with a data glove and then if
you look at what those human
demonstrations do they get a success
rate of about 24
on this door task with some online fine
tuning and get 88
success rate so this is doing much
better than if you just copy the
demonstrations
and you get meaningful fine tuning now
of course this is an offline rl method
so it doesn't actually need
demonstrations per se
that's just what we had in these
experiments we had other experiments
that also used random data
and quantitatively the method actually
does really well it's shown by this red
line here
the pen task is the easiest the door is
kind of medium and the relocate task is
the hardest and you can see that as the
tasks get harder
this method denoted as awac ends up
doing the best
so it kind of matches previous work on
the easy task and then greatly exceeds
it on the harder tasks
so this shows that something you know is
really working with these implicit
constraints
all right now let me take a little
detour to
delve a little bit into the world of
model based reinforcement learning
because
we can also develop effective offline rl
algorithms in the model based world too
so uh just a brief overview of
model-based rl
for those of you that aren't familiar
with it in model based rl
what we're going to do using data that
we collect from the environment is we're
going to fit a model we're going to fit
an
estimate to the transition probabilities
p hat of st plus 1
given st comma 80. and then we'll
somehow use that model
to train a policy pi of a t given st and
then typically repeat the process
so one way that you could imagine such a
model based rl algorithm working and
this is sort of a dyna style recipe
is that you collect real data denoted by
these block trajectories
you use that real data to fit your model
and then you're going to use that model
to make
kind of little short roll outs from
different states in your data set
and these little short rollouts will be
on policy using your latest policy
so the model kind of answers these
what-if questions if i were in this
state that i had seen before
what would have happened if i took a
different action and this of course lets
you train your policy much more
efficiently
and of course in offline rl we're going
to omit this error we're not going to
allow the algorithm to collect more data
it has to contend itself with the data
that it already has so what could go
wrong
well as you might have already guessed
the problem again is going to be
distributional shift
when we make these rollouts under the
model
if the policy that we're now evaluating
is very different from the policy they
originally collected the data
rolling out the model under that policy
will result in state visitations
that are very different from the states
under which the model was trained
and the model will make very large
mistakes and of course since the policy
is again being trained to maximize the
return
it'll learn to exploit the model to
essentially cheat so the model
erroneously produces good states
now in the in the literature one of the
ways that people have mitigated this
effect
is by just shortening these rollouts so
if you don't allow the policy to raw for
very many steps under the model there's
only so much exploiting that it can do
but in the fully offline setting of
course you don't want to do this because
your policy might deviate drastically
from the behavior policy so you want to
give it longer roll outs
so you can actually tell how well it's
doing
so the model's predictions will be
invalid if you get these out of
distribution states
very much like the problem we saw before
so
one solution we've developed and this is
uh joint work with
kenya you uh and garrett thomas who are
the co-first authors as well as a number
of collaborators
most of the team here is from stanford
university led by professors
chelsea finn and thank you mom and
there's also some concurrent work
by kadambi at all that also proposes a
very closely related algorithm called
moral
but i'm going to talk about our paper
called mopo
so the solution is to basically punish
the policy for exploiting these other
distribution states
so what we're going to do is we're going
to take our original reward function
and we'll subtract a penalty
mult with a coefficient lambda and this
penalty u
is essentially going to be some quantity
that is larger for states that are more
out of distribution or state action
tuples that are more out of distribution
so it's a kind of uncertainty penalty
and that's the only change we're going
to make we're just going to change our
reward like this
and then use some existing model based
rl algorithm the one that we used in
mopo is based on an algorithm called
mbpo model based policy optimization
so the idea now is when you visit one of
these outer distribution states
you get a really big penalty and that
results in the policy not wanting to
visit those other distribution states
instead of curving back
to the support of the data
okay so there's some pretty interesting
theoretical analysis that we can do for
this algorithm
and the theory this is kind of the
statement from the main theorem in the
paper
uh which states that under two technical
assumptions
the learned policy pi hat in the moba
algorithm satisfies the property that
the return of pi hat
is greater than or equal to the supremum
over pi
of uh the return of pi minus two times
lambda lambda
times epsilon u okay so what is what are
all these things
there are two technical assumptions one
is that we can basically represent
the value function so things will break
down badly if you have really bad
function approximation error and the
second one
is an assumption on you which says that
the true model error
is bounded above by you so this is this
is essentially saying that u is a
consistent estimator
for the error in the model and you need
this property
for basically because you need you to
mean something in practice the way that
we estimate mu
is by training an ensemble of models and
measuring their disagreement but any
estimator that upper bounds
the error in the model will do the job
so this is the true return of the policy
train under the model so not the return
the model thinks it has but the
return it actually has and
epsilon here is just the expected value
of u
so what this is saying is that the true
return of a policy we optimize under our
model
will actually have high will actually be
at least as high
as the best policy that still has to pay
this
error price another way of saying that
it'll be at least as good
as the best in distribution policy of
course you can get the optimal policy
because you don't know what happens out
of distribution we can at least have to
do at least as well
as the best in distribution policy
another way of saying this is that um
you can construct this policy pi delta
which is
the best policy whose error is bound
basically the best policy that
doesn't visit states with error larger
than delta
so this is under the true mdp but it's
saying you're going to
find the best policy under the true mdp
that doesn't visit states where the
model would have made mistakes
and this result says that the policy we
learn with model-based rl will be at
least as good as that policy
minus an error term that scales us two
times lambda times delta
so this basically shows that we'll get a
good policy within the support of our
data
so some implications of this this means
that we always improve over the behavior
policy
and we can actually quantify the
optimality gap against the optimal
policy in terms of model error so
basically
if our error on the states that the
optimal policy would visit is small
then we'll get close to the optimal
policy
empirically this method does very well
it outperforms regular mbpo it also
outperforms
quite a few of the previously proposed
policy constraint methods
all right so now let me discuss the last
algorithm i'm going to cover
which is called conservative q learning
conservative q learning takes a slightly
different approach
to offline rl from policy constraint
methods so
just to remind you the problem we saw
before is that the q function kept
thinking that it's going to do much
better than it's actually going to do
so it's going to overestimate like this
because when we take the r
max we get the point with the largest
positive error
so instead of trying to constrain the
policy what if we try to directly fix
the problem what if we directly push
down on erroneously high q values
one of the ways we could do this is we
can formulate a q
learning objective with a penalty so we
have the usual term which says
minimize development error and then we
have a penalty term which says
pick the actions with the high q values
and push down on those q values so
minimize the q values under this
distribution mu
which is chosen so that the q values
under mu are high
so this will basically find these
erroneous positive points and push them
down
it turns out that with this very simple
regularizer we can actually prove
that the q function we learn is a lower
bound on the true q function for the
policy pi
if you choose alpha the weight on the
regularizer appropriately
that's pretty cool we're actually
guaranteed not to overestimate if we do
this
um so this is work that was primarily
led by my student averal kumar
and the particular algorithm that
overall proposed is a kind of actor
critic style algorithm
where you learn the slower bound q
function for the current policy
uh so that q hat is less than or equal
to q
and then you update your policy now you
don't actually need to represent the
policy exactly it's just a little
explicitly you can still have a q
learning algorithm where it's implicit
where it's the r max policy
but it's a little easier to explain in
active critics setting we call this
conservative q learning though because
it's also very simple to instantiate as
a q learning method
now you can also derive a much better
bound for conservative q learning the
bound i had on the previous page
was actually too conservative it
actually pushed down the q values too
much
what you can do is you can push down on
the high q values
but you can compensate by also pushing
up on the q values in the data set
so you push down on the q values of the
q function thinks are high
and then you push up on the q values uh
for the actions in the data set
intuitively what this means that is that
if the high q values are all for actions
that are in the data set
then these right two terms will cancel
out and the regularizer goes away so
when you're in distribution
there is no regularization if you go
more out of distribution you get more
regularization
so these are the two error terms push
down on actions from you
push up on actions from d from the
dataset
now you're no longer guaranteed to have
a bound for all state action tuples if
you do this
but you are turns out still guaranteed
to have a bound in expectation under
your policy
so the expected q value under pi for q
hat will still be less than or equal to
the expected value
under the uh the true q function for pi
for all the states provided that alpha
is chosen appropriately of course
so um the full bound uh
since it's a the full conservative q
learning algorithm is shown here you
minimize the big q values and you
maximize
uh the q values under the data the full
bound is written out here
so the left side is the estimated value
function it's less than or equal to the
true value function
minus a positive pessimism term due to
the regularizer and then there's this
error term that accounts for sampling
error
so you just have to choose alpha so that
this positive pessimism term
is larger than the positive sampling
error term obviously if you have more
data then your sampling error is lower
and you don't have to worry as much if
you have high sampling error then you
need a higher alpha
to be conservative
okay so does this bound hold in practice
one of the things we did is we actually
empirically measured underestimation
versus overestimation on a simple
benchmark task
so what i'm going to be showing is
basically the expected value of the
learned q function
q hat minus the expected value of the
true q function
so if these values are negative that
means that q hat is less than
q and expectation if they're positive
that means we're overestimating
and we get the true q function by
actually rolling out the policy many
times and using monte carlo
so here are the results so the first
column shows this
the full cql method with both the
minimization and maximization term
the second column shows just the basic
method that has just the minimization
but no maximization
then we have four different ensembles so
an ensemble of two networks four
networks 10 networks and 28 networks
and then we have bayer which is a
representative policy constraint on it
now the first thing that you might note
is that all of the ensembles and the
policy constraint method
are overestimating massively despite the
policy constraint
the simple cq element that just has the
minimization
is underestimating but by quite a lot so
rewards here are on the order of a few
hundred
so getting minus 150 means that you're
very heavily underestimating
whereas the full cql method
underestimates but only by a little bit
so we are having we do have a lower
bound and the lower bound is pretty good
so the cqr always has negative errors
which means that it's pessimistic
all right now before i tell you how well
cql actually does empirically
i want to talk a little bit about how we
should evaluate offline rl methods
so how should we evaluate them well uh
ultimately what we're going to want to
do is train our offline rl methods using
large and highly diverse data sets
but in the meantime when we're just
prototyping algorithmic ideas
we need to somehow collect uh you know
some data to set up a benchmark task so
maybe one thing we could do is we could
collect some data using some online rl
algorithm and then use it to evaluate
offline rl
this is actually pretty typical in prior
work you train pi beta with online rl
and then you either collect data
throughout training or you take data
from the final policy
i'm going to claim that this is a really
bad idea this is a really terrible way
to evaluate offline reinforcement
learning methods and if you're doing
research on offline rl
you should not use data sets that have
just this kind of data
because if you already have a good
policy why bother with offline rl
but perhaps more importantly in the real
world that's not what your data is going
to look like
your data is not going to come from a
markovian policy it's going to come from
humans from many different sources from
your past
behavior it's going to be multitask it's
going to be diverse it's not going to
look like the replay buffer
for an online url run so human users and
engineer policies etc
so if you really want to value your
offline rl method you really have to use
data that is representative
of real-world settings and leaves plenty
of room for improvement
and then offline allows to learn
policies that are much better than the
behavior policy that are better than the
best thing in your data set
without testing for these properties you
can't really trust
that our offline rl algorithms are good
and
of course in past work from my group
we've also been guilty of doing this but
we're we're mending our ways we're not
going to do this anymore
so we developed a benchmark suite called
d4rl it stands for data sets for data
driven dprl
this was led by my student justin food
together with avril kumar
uh of your not true truman george tucker
and d4rl is actually rapidly uh you know
picking up it's rapidly becoming the
most popular benchmark for offline rl
because it really exercises the kinds of
properties you want in offline rl
algorithms
so what are some important principles to
keep in mind well you want data from
non-rl policies
including data from humans so we
included things like the dexterous
manipulation data that i showed before
this was based on data collected by
argentoswar and colleagues
you want to evaluate whether your
algorithm can put together different
parts of different trajectories we'll
call this stitching
so if you've seen for example you can go
from a to b and you've seen that you can
go from b to c
but it's more optimal to go from a to c
the data actually tells you everything
you need to know to figure that out
so you should be able to do this and do
better than the best trajectory in the
data set
uh so we evaluate this using some maze
navigation tasks both in a simple low
dimensional 2d setting
and a complex setting where you've got a
simulated four-legged robot actually
walk through the maze
so you never see full paths from the
start to the goal but you see paths
between different places in the maze in
your data center
and you also have to have realistic
tasks we included first person driving
data from images using the carlos
simulator
data manipulating objects in a kitchen
from paper by abhishek gupta called
relay policy learning
and traffic control data from professor
alex bines lab
that simulates the effect of autonomous
cars on traffic
so the set of uh d4l tasks includes the
standard
mujoco gym tasks
with some difficult data sets the
stitching tasks and the mazes
dexterous manipulation tasks with data
from real humans
robot manipulation tests in this kitchen
environment again using human data
traffic control data from a flow
simulator and
image-based driving in karla
now if we look at how cql compares on
this benchmark first let me show you the
prior methods
one of the things to note is on the
harder benchmark tasks
we actually see that first of all
nothing works on the harder stitching
task so
on the larger mazes previous methods
basically don't learn anything these
scores are all normalized between 0 and
100.
the most competitive baseline across all
of these harder tasks
is just simple behavior cloning which
suggests that previously proposed
offline reinforcement learning methods
which have primarily been tested
on data from other rl policies are not
actually doing very well
so nothing beats behavior cloning on
these harder tasks
if we look at the performance of cql it
achieves state-of-the-art results on
nearly all of these tasks
so i'm showing two variants of cql and
one of these two variants
is the best on all but one of the tasks
or tied for the best so it's up to five
times better on the harder dexterous
manipulation tasks
fifty uh to three hundred percent better
on on the
adjust the human data 10 to 30 better on
the kitchen tasks
and essentially infinitely better on the
larger mazes where it's the only
algorithm that's able to exhibit the
stitching behavior
we also evaluated the method on atari
data from paper by agrowall at all and
we saw there also that
cql is 50 to 600 percent better uh than
previously proposed algorithms
so this method is doing really well it
seems to work quite well across many
tasks
and we seem to know why it works because
of this lower bound property
of course there's still plenty of room
for improvement so if you want to
develop better offline rl methods
there there's plenty of work to do and
plenty of ways
in which you can improve the results all
right
so just to wrap up and conclude i talked
about how offline rl
is quite difficult but has enormous
promise and initial results suggest that
it can be extremely powerful
i talked about how effective dynamic
programming offline rl methods can be
implemented by imposing constraints on
the policy and perhaps implicit
constraints
can get around the need to model the
behavior policy and i talked about how
this
learning a lower bound on the q function
using conservative q learning
can substantially improve offline rl
performance thank you very much&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" /><media:content medium="image" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Model Based Reinforcement Learning (MBRL)</title><link href="https://kargarisaac.github.io/blog/reinforcement%20learning/mbrl/jupyter/2020/10/26/mbrl.html" rel="alternate" type="text/html" title="Model Based Reinforcement Learning (MBRL)" /><published>2020-10-26T00:00:00-05:00</published><updated>2020-10-26T00:00:00-05:00</updated><id>https://kargarisaac.github.io/blog/reinforcement%20learning/mbrl/jupyter/2020/10/26/mbrl</id><content type="html" xml:base="https://kargarisaac.github.io/blog/reinforcement%20learning/mbrl/jupyter/2020/10/26/mbrl.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-10-26-mbrl.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This post is a summary (almost!) of the model-based RL tutorial at ICML-2020 by &lt;a href=&quot;https://twitter.com/IMordatch&quot;&gt;Igor Mordatch&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/jhamrick&quot;&gt;Jess Hamrick&lt;/a&gt;. You can find the videos &lt;a href=&quot;https://sites.google.com/view/mbrl-tutorial&quot;&gt;here&lt;/a&gt;. The pictures are from the slides in the talk.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Introduction-and-Motivation&quot;&gt;Introduction and Motivation&lt;a class=&quot;anchor-link&quot; href=&quot;#Introduction-and-Motivation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Having access to a world model, and using it for decision-making is a powerful idea. 
There are a lot of applications of MBRL in different areas like robotics (manipulation- what will happen by doing an action), 
self-driving cars (having a model of other agents decisions and future motions and act accordingly),
games (AlphaGo- search over different possibilities), Science ( chemical use-cases),
and operation research and energy applications (allocate renewable energy at different points in time to meet the demand).&lt;/p&gt;
&lt;h2 id=&quot;Problem-Statement&quot;&gt;Problem Statement&lt;a class=&quot;anchor-link&quot; href=&quot;#Problem-Statement&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In sequential decision making, the agent will interact with the world by doing action $a$ and getting the next state $s$ and reward $r$.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/rl.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We can write this problem as a Markov Decision Process (MDP) as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;States $S \epsilon R^{d_S}$&lt;/li&gt;
&lt;li&gt;Actions $A \epsilon R^{d_A}$&lt;/li&gt;
&lt;li&gt;Reward function $R: S \times A \rightarrow R$&lt;/li&gt;
&lt;li&gt;Transition function $T: S \times A \rightarrow S$&lt;/li&gt;
&lt;li&gt;Discount $\gamma \epsilon (0,1)$&lt;/li&gt;
&lt;li&gt;Policy $\pi: S \rightarrow A$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The goal is to find a policy which maximizes the sum of discounted future rewards:
$$
\text{argmax}_{\pi} \sum_{t=0}^\infty \gamma^t R(s_t, a_t)
$$
subject to
$$
a_t = \pi(s_t) , s_{t+1}=T(s_t, a_t)
$$&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;How to solve this optimization problem?!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Collect data $D= \{ s_t, a_t, r_{t+1}, s_{t+1} \}_{t=0}^T$.&lt;/li&gt;
&lt;li&gt;Model-free: learn policy directly from data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
$$ D \rightarrow \pi \quad \text{e.g. Q-learning, policy gradient}$$
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model-based: learn model, then use it to &lt;strong&gt;learn&lt;/strong&gt; or &lt;strong&gt;improve&lt;/strong&gt; a policy &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
$$ D \rightarrow f \rightarrow \pi$$
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What-is-a-model?&quot;&gt;What is a model?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-a-model?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;a model is a representation that explicitly encodes knowledge about the structure of the environment and task.&lt;/p&gt;
&lt;p&gt;This model can take a lot of different forms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A transition/dynamic model: $s_{t+1} = f_s(s_t, a_t)$&lt;/li&gt;
&lt;li&gt;A model of rewards: $r_{t+1} = f_r(s_t, a_t)$&lt;/li&gt;
&lt;li&gt;An inverse transition/dynamics model (which tells you what is the action to take and go from one state to the next state): $a_t = f_s^{-1}(s_t, s_{t+1})$&lt;/li&gt;
&lt;li&gt;A model of distance of two states: $d_{ij} = f_d(s_i, s_j)$&lt;/li&gt;
&lt;li&gt;A model of future returns: $G_t = Q(s_t, a_t)$ or $G_t = V(s_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Typically when someone says MBRL, he/she means the firs two items.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/model.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Sometimes we know the ground truth dynamics and rewards. Might as well use them! Like game environments or simulators like Mujoco, Carla, and so on.&lt;/p&gt;
&lt;p&gt;But we don't have access to the model in all cases, so we need to learn the model. In cases like in robots, complex physical dynamics, and interaction with humans.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;How-to-use-a-model?&quot;&gt;How to use a model?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-use-a-model?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In model-free RL agent, we have a policy and learning algorithm like the figure below:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/rl2.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In model-based RL we can use the model in three different ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;simulating the environment: replacing the environment with a model and use it to generate data and use it to update the policy.&lt;/li&gt;
&lt;li&gt;Assisting the learning algorithm: modify the learning algorithm to use the model to interpret the data it is getting differently. &lt;/li&gt;
&lt;li&gt;Strengthening the policy: allow the agent at test time to use the model to try out different actions before it commits to one of them (taking action in the real world).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbrl.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In general, to compare model-free and model-based:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbrl_vs_mfrl.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;How-to-learn-a-model?&quot;&gt;How to learn a model?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-learn-a-model?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Two different dimensions are useful to pay attention to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;representation of the features for the states that the model is being learned over them&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;representation of the transition between states&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To continue, we take a look at different transition models.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;state-transition-models&quot;&gt;state-transition models&lt;a class=&quot;anchor-link&quot; href=&quot;#state-transition-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We know equations of motion and dynamics in some cases, but we don't know the exact parameters like mass. We can use system identification to estimate unknown parameters like mass. But these sorts of cases require having a lot of domain knowledge about how exactly the system works.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model2.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model3.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In some cases that we don't know the dynamics of motion, we can use an MLP to get a concatenation of $s_t, a_t$, and output the next state $s_{t+1}$.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model4.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In cases that we have some, not perfect, domain knowledge about the environment, we can use graph neural networks (GNNs) to model the agent (robot). For example, in Mujoco, we can model a robot (agent) with nodes as its body parts and edges as joint and learn the physics engine.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model5.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;observation-transition-models&quot;&gt;observation-transition models&lt;a class=&quot;anchor-link&quot; href=&quot;#observation-transition-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In these cases, we don't have access to states (low-level states like joint angles), but we have access to images. The MDP for these cases would be like this:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model6.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;So what can we do with this?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Directly predict transitions between observations (observation-transition models)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model7.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reconstruct observation at every timestep: Using sth like LSTMs. Here we need to reconstruct the whole observation in each timestep. The images can be blurry in these cases.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model8.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model88.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;latent-state-transition-models&quot;&gt;latent state-transition models&lt;a class=&quot;anchor-link&quot; href=&quot;#latent-state-transition-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Another option when we have just access to observation is to instead of making transition between observations we can infere a latent state and then make transitions in that latent space (latent state-transition models) not in the observation space. It would be much faster than reconstructing the observation on every timestep. We take our initial observation or perhaps the last couple of observations and embed them into the latent state and then unroll it in time and do predictions in $z$ instead of $o$.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model9.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Usually we use the observation and reconstruct it during training but at test time we can unroll it very quickly. we can also reconstruct observation at each timestep we want (not necessarily in all timesteps).&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model10.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Structured-latent-state-transition-models&quot;&gt;Structured latent state-transition models&lt;a class=&quot;anchor-link&quot; href=&quot;#Structured-latent-state-transition-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Another thing that you can do if you have a little bit more domain knowledge is to add a little bit of structure into your latent state. For example, if you know that the scene that you are trying to model consists of objects, you can try to actually explicitly detect those objects, segment them out and then learn those transitions between objects.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model11.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Recurrent-value-models&quot;&gt;Recurrent value models&lt;a class=&quot;anchor-link&quot; href=&quot;#Recurrent-value-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The idea is that when you unroll your latent-state, you additionally predict the value of the state at each point of the future, in addition to reward. We can train the model without necessarily needing to train using observations, but just training it by predicting the value progressing toward actual observed values when you roll it out in the real environment.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model12.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Why is this useful? Because some types of planners only need you to predict values rather than predicting states like MCTS (Monte Carlo tree search).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Non-Parametric-models&quot;&gt;Non-Parametric models&lt;a class=&quot;anchor-link&quot; href=&quot;#Non-Parametric-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;So far, we talked about parametric ways of learning the model. We can also use non-parametric methods like graphs.&lt;/p&gt;
&lt;p&gt;For example, the replay buffer that we use in off-policy methods can be seen as an approximation to a type of model, where if you have enough data in your replay buffer, you can sample from the buffer and basically access the density model over your transitions. You can use extra replay to get the same level performances you would get using a model-based method that learns a parametric model.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model13.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We can also use data in the buffer to use data points and learn the transition between them and interpolate to find states between those states in the buffer. Somehow learning distribution and use it to generate new data points.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model14.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Another form of non-parametric transition is a symbolic description popular in the planning community, not in the deep learning community.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model15.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The other form of non-parametric models is gaussian processes, which give us strong predictions using a very small amount of data. PILCO is one example of these algorithms.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model16.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Model-based-control-and-how-to-use-a-model?&quot;&gt;Model-based control and how to use a model?&lt;a class=&quot;anchor-link&quot; href=&quot;#Model-based-control-and-how-to-use-a-model?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We will be using this landscape of various methods and categories that exist, including some representative algorithms:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc1.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;As we saw earlier, we can use the model in three different ways. In continue, we will see some examples of each case.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Simulating-the-environment&quot;&gt;Simulating the environment&lt;a class=&quot;anchor-link&quot; href=&quot;#Simulating-the-environment&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc2.png&quot; alt=&quot;&quot; style=&quot;max-width: 150px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;One way is to mix the real data with model-generated experience and then apply traditional model-free algorithms like Q-learning, policy gradient, etc. In these cases, the model offers a larger and augmented training dataset.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dyna-Q&lt;/strong&gt; is an example that uses Q-learning with a learned model. Dyna does the traditional Q-learning updates on real transitions and uses a model to create fictitious imaginary transitions from the real states and perform exactly the same Q-learning updates on those. So it's basically just a way to augment the experience.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc3.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This can also be applied to policy learning. We don't need to perform just a single step but multiple steps according to the &lt;strong&gt;model&lt;/strong&gt; to generate experience even further away from the real data and do policy parameter updates entirely on these fictitious experiences.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc4.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Assisting-the-learning-algorithm&quot;&gt;Assisting the learning algorithm&lt;a class=&quot;anchor-link&quot; href=&quot;#Assisting-the-learning-algorithm&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc5.png&quot; alt=&quot;&quot; style=&quot;max-width: 150px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;One important way that this can be done is to allow end-to-end training through our models. End-to-end training has recently been very successful in improving and simplifying supervised learning methods in computer vision, NLP, etc.&lt;/p&gt;
&lt;p&gt;The question is, &quot;can we apply the same type of end-to-end approaches to RL?&quot;&lt;/p&gt;
&lt;p&gt;One example is just the policy gradient algorithm. Let's say we want to maximize the sum of the discounted future reward of some parametric policy. We can write the objective function with respect to the policy parameters $\theta$&lt;/p&gt;
$$
 J(\theta) = \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t)  , \quad a_t = \pi_{\theta}(s_t) , \quad s_{t+1} = T(s_t, a_t)
$$&lt;p&gt;Now we need to apply gradient ascent (for maximization) on policy gradient with respect to policy parameters $\theta  \rightarrow  \nabla_{\theta}J$.&lt;/p&gt;
&lt;p&gt;So how can we calculate this $\nabla_{\theta}J$ ?&lt;/p&gt;
&lt;p&gt;Sampling-based methods have been proposed, like REINFORCE, to estimate this gradient. But the problem with them is that they can have very high variance and often require the policy to have some randomness to make decisions. This can be unfavorable.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc6.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The next solution is when we have accurate and smooth models. Accurate and smooth models, aside from imaginary experiences, offer derivatives:&lt;/p&gt;
$$
s_{t+1} = f_s(s_t, a_t) \quad  r_t = f_r(s_t, a_t)
$$$$
\nabla_{s_t}(s_{t+1}), \quad \nabla_{a_t}(s_{t+1}), \quad \nabla_{s_t}(r_t), \quad \nabla_{a_t}(r_t), \quad ...
$$&lt;p&gt;And they are able to answer questions such as: &lt;em&gt;how do small changes in action change next state or reward any of other quantities?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc7.png&quot; alt=&quot;&quot; style=&quot;max-width: 150px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Why is this useful? This is useful because it will allow us to do this type of end-to-end differentiation algorithms like &lt;strong&gt;back-propagation&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let's rewrite our objective function using models:&lt;/p&gt;
$$
 J(\theta) \approx \sum_{t=0}^{H} \gamma^t r_t  , \quad a_t = \pi_{\theta}(s_t) , \quad s_{t+1} = f_s(s_t, a_t), \quad r_t=f_r(s_t,a_t)
$$&lt;p&gt;So how can we use these derivatives to calculate $\nabla_{\theta}J$ ?&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc8.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The highlighted derivatives are easy to calculate using some libraries like PyTorch or TensorFlow.&lt;/p&gt;
&lt;p&gt;By calculating $\nabla_{\theta}J$ in this way:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;pros&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The policy gradient that we get is actually a deterministic quantity, and there is no variance to it. &lt;/li&gt;
&lt;li&gt;It can support potentially much longer-term credit assignment&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;cons&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is prone to local minima&lt;/li&gt;
&lt;li&gt;Poor conditioning (vanishing/exploding gradients)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are two examples to use model-based back-propagation (derivatives) either along real or model-generated trajectories to do end to end training:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc9.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;real trajectories are safer but need to be from the current policy parameters (so its less sample-efficient)&lt;/li&gt;
&lt;li&gt;model-generated trajectories allow larger policy changes without interacting with the real world but might suffer more from model inaccuracies&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Strengthening-the-policy&quot;&gt;Strengthening the policy&lt;a class=&quot;anchor-link&quot; href=&quot;#Strengthening-the-policy&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;So far, we talked about the first two ways of using a model in RL. These two ways are in the category of &lt;strong&gt;Background Planning&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;There is another category based on the &lt;em&gt;Sutton and Barto (2018)- Reinforcement Learning: An Introduction&lt;/em&gt; categorization, called &lt;strong&gt;Decision-Time Planning&lt;/strong&gt;, which is a unique option we have available in model-based settings.&lt;/p&gt;
&lt;h4 id=&quot;What-is-the-difference-between-background-and-decision-time-planning?&quot;&gt;What is the difference between background and decision-time planning?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-the-difference-between-background-and-decision-time-planning?&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In background planning, we can think of it as answering the question, &quot;how do I learn how to act in any possible situation to succeed and reach the goal?&quot;&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc10.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The optimization variables are parameters of a policy or value function or ..., and are trained using expectation over all possible situations.&lt;/li&gt;
&lt;li&gt;Conceptually, we can think of background planning as learning a set of habits that we could reuse.&lt;/li&gt;
&lt;li&gt;We can think of background planning as learning a fast type of thinking.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In decision-time planning, we want to answer the question, &quot;what is the best sequence of actions just for my current situation to succeed or reach the goal?&quot;&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc11.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The optimization parameters are just a sequence of actions or states.&lt;/li&gt;
&lt;li&gt;Conceptually, we can think of decision-time planning as finding our consciously improvising just for the particular situation that we find ourselves in.&lt;/li&gt;
&lt;li&gt;We can think of decision-time planning as learning a slow type of thinking.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Why use one over the other?&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc12.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Act on the most recent state of the world&lt;/em&gt;: decision-time planning is just concerned about the current state in finding the sequence of actions. You can act based on the most recent state of the world. By contrast, in background planning, the habits may be stale and might take a while to get updated as the world's changes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Act without any learning&lt;/em&gt;: decision-time planning allows us to act without any learning at all. There is no need for policy or value networks before we can start making decisions. It is just an optimization problem as long as you have the model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Competent in unfamiliar situations&lt;/em&gt;: if you find yourself in situations that are far away from where you were training, your set of habits or policy network might not have the competence (the ability to do something successfully or efficiently) there. So you don't have any information to act or are very uncertain, or even in the worst case, it will with confidence make decisions that just potentially make no sense. This is out of distribution and generalization problem. In these cases, decision-time planning would be more beneficial.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Independent of observation space&lt;/em&gt;: another advantage of decision-time planning is that it is also independent of the observation space that you decide on. In background methods, we need to consider some encoding or description of the state, joint angles, or pixels or graphs into our policy function. These decisions may play a large role in the total learning performance. When something is not working, you will not really know that is it because of the algorithm or state-space, which doesn't contain enough information. In contrast, decision-time planning avoids this confounded, which in practice can actually be quite useful when you're prototyping new methods.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc13.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Partial observability&lt;/em&gt;: decision-time plannings have some issues with it. They assume that you know the full state of the world when you're making the plan. So it's hard to hide information from decision-time planners. It is possible, but it is more costly.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Fast computation at deployment&lt;/em&gt;: decision-time planners require more computation. It is not just evaluating a habit, but it needs more thinking.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Predictability and coherence&lt;/em&gt;: decision-time planners do some actions which are not necessarily predictable or coherent. Because you are consciously thinking about each footstep, you might not have exactly the same plan. So you may have a very chaotic behavior that still succeeds. In contrast, background planning, because it learns a set of habits, it can perform a very regular behavior.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Same for discrete and continuous actions&lt;/em&gt;: background planning has a very unified treatment of discrete and continuous actions, which is conceptually simpler. In decision-time planning, there are different algorithms for discrete and continuous actions. We will see in the following sections more about them.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also mix and match the background and decision-time plannings.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;What-is-the-difference-between-discrete-and-continuous-planning?&quot;&gt;What is the difference between discrete and continuous planning?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-the-difference-between-discrete-and-continuous-planning?&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;It depends on the problem which you want to solve. So it is not a choice that you can make. For example, in controlling a robot, the actions might be the torques for the motors (continuous), or in biomechanical settings, it might be muscle excitations (continuous), or in medical problems, the treatment that should be applied (discrete).&lt;/p&gt;
&lt;p&gt;The distinction between discrete and continuous actions is not significant for background planning methods.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You just learn stochastic policies that sample either from discrete or continuous distributions.&lt;/li&gt;
&lt;/ul&gt;
$$
a \sim \pi(.|s) \quad \leftarrow Gaussian, categorical, ...
$$&lt;ul&gt;
&lt;li&gt;Backpropagation is still possible via some reparametrization techniques. See &lt;em&gt;Jang et al (2016). Categorical reparametrization with Gumbel-Softmax&lt;/em&gt; for an example.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In either of these cases (continuous and discrete in background planning methods), your final objective and optimization problem is still smooth wrt the policy parameters because you are optimizing over expectations.&lt;/p&gt;
$$
J(\theta) = E_{\pi}[\sum_t r_t], \quad a_t \sim \pi(.|s_t, \theta)
$$&lt;p&gt;But for decision-time planning, this distinction leads to specialized methods for discrete and continuous actions: discrete search or continuous trajectory optimization.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Let's see some examples to be able to compare them.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc14.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;MCTS-(monte-carlo-tree-search)&quot;&gt;MCTS (monte carlo tree search)&lt;a class=&quot;anchor-link&quot; href=&quot;#MCTS-(monte-carlo-tree-search)&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;p&gt;This algorithm is in a discrete action group and is used in alpha-go and alpha-zero. You keep track of Q-value, which is long term reward, for all states and actions that you want to consider. And also the number of times that the state and action have been previously visited.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize $Q_0(s, a) = 0, N_0(s, a)=0, k=0$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc15.png&quot; alt=&quot;&quot; style=&quot;max-width: 150px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Expansion: Starting from the current situation and expand nodes and selecting actions according to a search policy: &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;
$$\pi_k(s) = Q_k(s,a)$$
&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc16.png&quot; alt=&quot;&quot; style=&quot;max-width: 150px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Evaluation: When a new node is reached, estimate its long-term value using Monte-Carlo rollouts&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc17.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Backup: Propagate the Q-values to parent nodes:&lt;/li&gt;
&lt;/ol&gt;
$$
Q_{k+1}(s, a) = \frac{Q_k(s,a) N_k(s,a) + R}{N_k(s,a)+1}
$$$$
N_{k+1}(s,a) = N_k(s,a)+1
$$&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc18.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Repeat Steps 2-4 until the search budget is exhausted.
$$
k = k + 1
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc19.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;Trajectory-Optimization&quot;&gt;Trajectory Optimization&lt;a class=&quot;anchor-link&quot; href=&quot;#Trajectory-Optimization&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;p&gt;Instead of keeping track of a tree of many possibilities, you keep track of one possible action sequence.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize $a_0, ..., a_H$ from guess&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc20.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Expansion&lt;/strong&gt;: execute sequence of actions $a = a_0, ..., a_H$ to get a sequence of states $s_1, ..., s_H$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc21.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: get trajectory reward $J(a) = \sum_{t=0}^H r_t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Back-propagation&lt;/strong&gt;: because everything is differentiable, you can just calculate the gradient of the reward via back-propagation using reward model derivatives and transition model derivatives.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
$$
\nabla_a J = \sum_{t=0}^H \nabla_a r_t
$$$$
\nabla_a r_t = \nabla_s f_r(s_t, a_t) \nabla_a s_t + \nabla_a f_r (s_t, a_t)
$$$$
\nabla_a s_t = \nabla_a f_s(s_{t-1}, a_{t-1}) + \nabla_s f_s(s_{t-1}, a_{t-1})\nabla_a s_{t-1}
$$$$
\nabla_a s_{t-1} = ...
$$&lt;ol&gt;
&lt;li&gt;Update all actions via gradient ascent $ a \leftarrow a + \nabla_a J$ and repeat steps 2-5.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc22.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The differences between discrete and continuous actions can be summarized as follows:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc23.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The continuous example we saw above can be categorized in &lt;strong&gt;shooting methods&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&quot;Variety-and-motivations-of-continuous-planning-methods&quot;&gt;Variety and motivations of continuous planning methods&lt;a class=&quot;anchor-link&quot; href=&quot;#Variety-and-motivations-of-continuous-planning-methods&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Why so many variations? They all try to mitigate the issues we looked at like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sensitivity and poor conditioning&lt;/li&gt;
&lt;li&gt;Only reaches local optimum&lt;/li&gt;
&lt;li&gt;Slow convergence&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Addressing each leads to a different class of methods.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc24.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;Sensitivity-and-poor-conditioning&quot;&gt;Sensitivity and poor conditioning&lt;a class=&quot;anchor-link&quot; href=&quot;#Sensitivity-and-poor-conditioning&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc24-2.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Shooting methods&lt;/strong&gt; that we have seen have this particular issue that small changes in early actions lead to very large changes downstream.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc25.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;By expanding the objective function, this can be understood more clearly.&lt;/p&gt;
$$
\max_{a_0,...,a_H} \sum_{t=0}^H r(s_t, a_t), \quad s_{t+1} = f(s_t, a_t)
$$$$
\sum_{t=0}^H r(s_t, a_t) = r(s_0, a_0) + r(f(s_0, a_0), a_1)+...+r(f(f(...),...), a_H)
$$&lt;p&gt;It means that each state implicitly is dependent on all actions that came before it. This is similar to the exploding/vanishing gradient problem in RNNs that hurts long-term credit assignment. But unlike the RNN training, we cannot change the transition function because it is dictated to us by the environment.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc26.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To address this problem, &lt;strong&gt;Collocation&lt;/strong&gt; is introduced, which is optimizing for states and/or actions &lt;em&gt;directly&lt;/em&gt;, instead of actions only. So we have a different set of parameters that we are optimizing over.&lt;/p&gt;
$$
\max_{s_0,a_0,...,s_H,a_H} \sum_{t=0}^H r(s_t, a_t), \quad ||s_{t+1} - f(s_t, a_t) || = 0 \leftarrow \text{explicit optimization constraint}
$$&lt;p&gt;It is an explicit constrained optimization problem, rather than just beeng satisfied by construction as in shooting methods.&lt;/p&gt;
&lt;p&gt;As a result, you only have pairwise dependencies between variables, unlike the dense activity graph in the previous figure for shooting methods.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc27.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;These methods have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Good conditioning: changing $s_0, a_0$ has a similar effect as changing $s_H, a_H$.&lt;/li&gt;
&lt;li&gt;Larger but easier to optimize search space. It is useful for contact-rich problems such as some robotics applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;Only-reaches-local-optimum&quot;&gt;Only reaches local optimum&lt;a class=&quot;anchor-link&quot; href=&quot;#Only-reaches-local-optimum&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc28.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Some approaches try to avoid local optima like sampling-based methods: Cross-Entropy Methods (CEM) and $\text{PI}^2$.&lt;/p&gt;
&lt;p&gt;For example, in CEMs, instead of just maintaining the optimal trajectory, it maintains the optimal trajectory's mean and covariance.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc29.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc30.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc31.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Despite being very simple, this works surprisingly well and has very nice guarantees on performance.&lt;/p&gt;
&lt;p&gt;Why does this work?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Search space of decision-time plans much smaller than space of policy parameters: ex. 30x32 vs 32x644x32&lt;/li&gt;
&lt;li&gt;More feasible plans than policy parameters&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;Slow-convergence&quot;&gt;Slow convergence&lt;a class=&quot;anchor-link&quot; href=&quot;#Slow-convergence&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc32.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Gradient descent is too slow to converge, and we need to wait for thousands-millions of iterations to train a policy. But this is too long for a one-time plan that we want to through it away after.&lt;/p&gt;
&lt;p&gt;Can we do something like Newtons method for trajectory optimization, like non-linear optimization? YES!&lt;/p&gt;
&lt;p&gt;We can approximate transitions with linear functions and rewards with quadratics:&lt;/p&gt;
$$
\max_{a_0,...,a_H} \sum_{t=0}^H r_t, \quad s_{t+1} = f_s(s_t, a_t), \quad r_t=f_r(s_t, a_t)
$$$$
f_s(s_t, a_t) \approx As_t + Ba_t, \quad f_r(s_t, a_t) \approx s_t^TQs_t + a_t^TRa_t
$$&lt;p&gt;Then it becomes the Linear-Quadratic Regulator (LQR) problem and can be solved exactly.&lt;/p&gt;
&lt;p&gt;For iLQR, locally approximate the model around the current solution, solve the LQR problem to update the solution, and repeat.&lt;/p&gt;
&lt;p&gt;For Differential dynamic programming (DDP), it is similar, but with a higher-order expansion of $f_s$.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Model-based-control-in-the-loop&quot;&gt;Model-based control in the loop&lt;a class=&quot;anchor-link&quot; href=&quot;#Model-based-control-in-the-loop&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We want to answer this question of how to both learn the model and act based on that simultaneously?&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc33.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&quot;Gathering-data-to-train-models&quot;&gt;Gathering data to train models&lt;a class=&quot;anchor-link&quot; href=&quot;#Gathering-data-to-train-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;How can we gather data to train the model? this is a chicken or the egg problem. Bad policy leads to a bad experience, leads to a bad model, leads to bad policy ...&lt;/p&gt;
&lt;p&gt;This leads to some training stability issues in practice. There are some recent works in game theory to provide criteria for stability. See &lt;em&gt;Rajeswaran et al (2020). A Game Theoretic Framework for Model Based Reinforcement Learning.&lt;/em&gt; for example.&lt;/p&gt;
&lt;h4 id=&quot;Fixed-off-line-datasets&quot;&gt;Fixed off-line datasets&lt;a class=&quot;anchor-link&quot; href=&quot;#Fixed-off-line-datasets&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Another way to address this in the loop issues is to see if we can actually train from a fixed experience that is not related to the policy. Some options that we have are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Human demonstration&lt;/li&gt;
&lt;li&gt;Manually-engineered policy rollouts&lt;/li&gt;
&lt;li&gt;Another (sub-optimal) policy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc34.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This leads to a recent popular topic &lt;em&gt;model-based offline reinforcement learning&lt;/em&gt;. You can see some recent works like &lt;em&gt;Kidambi et al (2020). MOReL: Model-Based Offline Reinforcement Learning.&lt;/em&gt;, 
&lt;em&gt;Yu et al (2020). MOPO: Model-based Offline Policy Optimization.
See also: Levine et al (2020).&lt;/em&gt;, and &lt;em&gt;Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems.&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Data-augmentation&quot;&gt;Data augmentation&lt;a class=&quot;anchor-link&quot; href=&quot;#Data-augmentation&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Another way to generate data is to use the model to generate data to train itself. For example, in &lt;em&gt;Venkatraman et al (2014). Data as Demonstrator&lt;/em&gt;. You might have some trajectory of a real experiment that you got by taking certain actions; then you roll out the model and train to pull its predicted next states to true next states.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc35.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc36.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;There are also some adversarial approaches to generate data to self-audit the model like &lt;em&gt;Lin et al (2020). Model-based Adversarial Meta-Reinforcement Learning.&lt;/em&gt; and &lt;em&gt;Du et al (2019). Model-Based Planning with Energy Models.&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;But even if we do all of these works, models are not going to be perfect. We cannot have experience everywhere, and there will be some approximation errors always. These small errors propagate and compound. We may end up in some states that are a little bit further away from true data, which might be an unfamiliar situation. So it might end up making even bigger errors next time around and so on and so forth that the model rollouts might actually land very far away over time from where you would expect them to be.&lt;/p&gt;
&lt;p&gt;What's worse is that the planner might actually intentionally &lt;em&gt;exploit&lt;/em&gt; these model errors to achieve the goal.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc37.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This leads to a longer model rollouts to be less reliable.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc38.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;You can check &lt;em&gt;Janner et al (2019). When to Trust Your Model:
Model-Based Policy Optimization&lt;/em&gt; for more details.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Acting-under-imperfect-models&quot;&gt;Acting under imperfect models&lt;a class=&quot;anchor-link&quot; href=&quot;#Acting-under-imperfect-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The question is that &quot;Can we still act with imperfect models?&quot; the answer is yes!&lt;/p&gt;
&lt;h4 id=&quot;Replan-via-model-predictive-control&quot;&gt;Replan via model-predictive control&lt;a class=&quot;anchor-link&quot; href=&quot;#Replan-via-model-predictive-control&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The first approach is to not commit to just one single plan (open-loop control) but continually re-plan as you go along (closed-loop control).&lt;/p&gt;
&lt;p&gt;Let's see one example.&lt;/p&gt;
&lt;p&gt;You might start at some initial state and create an imaginary plan using the trajectory optimization methods like CEM or other methods. Then apply just the first action of this plan. That might take you to some state that might not in the practice match with your model imagined you would end up with. But it's ok! You can just re-plan from this new state, again and again, take the first action and ... and by doing this, there is a good chance to end up near the goal.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc39.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc40.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc41.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc42.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc43.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;By doing this, the errors don't accumulate. So you don't need a perfect model; just one pointing in the right direction is enough. This re-planning might be expensive, but one solution is to reuse solutions from previous steps as initial guesses for the next plan.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc44.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Plan-conservatively&quot;&gt;Plan conservatively&lt;a class=&quot;anchor-link&quot; href=&quot;#Plan-conservatively&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;We have'seen that longer rollouts become more unreliable. One solution would be just to keep the rollouts short. So we don't deviate too far from where we have real data. And as we saw in Dyna, just one single rollout can be also very helpful to improve learning.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc45.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The other option to plan conservatively is to consider a distribution over your models and plan for either the average or worst case wrt distribution over your model or model uncertainty.&lt;/p&gt;
$$
\max_{\theta} E_{f \sim F} [\sum_t \gamma^t r_t], \quad a_t=\pi_{\theta}(s_t), \quad s_{t+1}=f_s(s_t, a_t), \quad r_t=f_r(s_t, a_t)
$$&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc46.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc47.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Another option for conservative planning is to try to stay close to states where the model is certain. There are a couple of ways to do this:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc48.png&quot; alt=&quot;&quot; style=&quot;max-width: 150px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Implicitly: stay close to past policy that generated the real data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Peters et al (2012). Relative Entropy Policy Search&lt;/li&gt;
&lt;li&gt;Levine et al (2014). Guided Policy Search under Unknown Dynamics.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Explicitly: add penalty to reward or cost function for going into unknown region&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kidambi et al (2020). MOReL: Model-Based Offline Reinforcement Learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In the last two options for conservative planning, we need uncertainty. So how do we get this model uncertainty?&lt;/p&gt;
&lt;h3 id=&quot;Estimating-model-uncertainty&quot;&gt;Estimating model uncertainty&lt;a class=&quot;anchor-link&quot; href=&quot;#Estimating-model-uncertainty&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Model uncertainty, if necessary for conservative planning, but it has other applications too that we will see later.&lt;/p&gt;
&lt;p&gt;We consider two sources of uncertainty:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Epistemic uncertainty&lt;ul&gt;
&lt;li&gt;Model's lack of knowledge about the world&lt;/li&gt;
&lt;li&gt;Distribution over beliefs&lt;/li&gt;
&lt;li&gt;Reducible by gathering more experience about the world&lt;/li&gt;
&lt;li&gt;Changes with learning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Aleatoric uncertainty/Risk&lt;ul&gt;
&lt;li&gt;World's inherent stochasticity&lt;/li&gt;
&lt;li&gt;Distribution over outcomes&lt;/li&gt;
&lt;li&gt;Irreducible&lt;/li&gt;
&lt;li&gt;Static as we keep learning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are multiple approaches to estimate these uncertainties, which are listed as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Probabilistic neural networks that try to model distributions over the outputs of your model.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Model explicitly outputs means and variances (typically Gaussian)&lt;/p&gt;
&lt;p&gt;$$ p(s_{t+1}|s_t, a_t) = N(\mu_{\theta}(s_t, a_t), \sigma_{\theta}(s_t, a_t))$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Simple and reliable (supervised learning)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Only captures aleatoric uncertainty/risk&lt;/li&gt;
&lt;li&gt;No guarantees for reasonable outputs outside of training data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Bayesian neural network&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Model has a distribution over neural network weights&lt;/p&gt;
&lt;p&gt;$$ p(s_{t+1}|s_t, a_t) = E_{\theta}[p(s_{t+1}|s_t, a_t, \theta)]$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Captures epistemic and aleatoric uncertainty&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Factorized approximations can underestimate uncertainty&lt;/li&gt;
&lt;li&gt;Can be hard to train (but an active research area)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc49.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gaussian processes &lt;ul&gt;
&lt;li&gt;Captures epistemic uncertainty&lt;/li&gt;
&lt;li&gt;Explicitly control state distance metric&lt;/li&gt;
&lt;li&gt;Can be hard to scale (but an active research area)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pseudo-counts&lt;ul&gt;
&lt;li&gt;Count or hash states you already visited&lt;/li&gt;
&lt;li&gt;Captures epistemic uncertainty&lt;/li&gt;
&lt;li&gt;Can be sensitive to state space in which you count&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc50.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ensembles&lt;ul&gt;
&lt;li&gt;Train multiple models independently and combine predictions across models&lt;/li&gt;
&lt;li&gt;Captures epistemic uncertainty&lt;/li&gt;
&lt;li&gt;Simple to implement and applicable in many contexts&lt;/li&gt;
&lt;li&gt;Can be sensitive to state space and network architecture&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For discussion in the context of reinforcement learning, see &lt;em&gt;Osband et al (2018). Randomized Prior Functions for Deep Reinforcement Learning.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Between the above options, Ensembles are currently popular due to simplicity and flexibility.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Combining-planning-and-learning&quot;&gt;Combining planning and learning&lt;a class=&quot;anchor-link&quot; href=&quot;#Combining-planning-and-learning&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We compared these two methods in previous sections and saw that background and decision-time planning have complementary strengths and weaknesses.&lt;/p&gt;
&lt;p&gt;How to combine decision-time planning and background planning methods and get the benefits of both?&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Distillation&quot;&gt;Distillation&lt;a class=&quot;anchor-link&quot; href=&quot;#Distillation&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In this approach, we gather a collection of initial states and run our decision-time planner for each initial state and get a collection of trajectories that succeed at reaching the goal. Once we collected this collection of optimal trajectories, we can use a supervised learning algorithm to train either policy function or any other function to map states to actions. This is similar to Behavioral Cloning (BC).&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb1.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb2.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Some issues that can arise:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;What is the learned policies that have compounding errors?&lt;/strong&gt; If we rollout the policy from one of the states, it does something different than what we intended to do.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Create new decision-time plans from these states that have been visited by the policy.&lt;/li&gt;
&lt;li&gt;Add these trajectories (new decision-time plans) to the distillation dataset (expand dataset where policy makes errors)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb3.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This is the idea of Dagger algorithm:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb4.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;What if the plans are not consistent?&lt;/strong&gt; There are several ways to achieving a goal, and we've seen that by changing the initial condition only a little bit, the decision-time planner can give us pretty different solutions to reach a single goal. This chaotic behavior might be hard to distill into the policy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb5.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;we can make it so that the policy function that we are learning actually feeds back and influences our planner. &lt;/li&gt;
&lt;li&gt;To do this, we can add an additional term in our cost that says stay close to the policy. $D$ in the below cost function is the distance between actions of the planner, $a_t$, and the policy outputs, $\pi(s_t)$. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb6.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb7.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb8.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Terminal-value-functions-(value-of-the-terminal-state)&quot;&gt;Terminal value functions (value of the terminal state)&lt;a class=&quot;anchor-link&quot; href=&quot;#Terminal-value-functions-(value-of-the-terminal-state)&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;One of the issues with many trajectory optimizations or discrete search approaches is that the planning horizon is typically finite. This may lead to myopic or greedy behavior.&lt;/p&gt;
$$
J^H = \sum_{t=0}^H \gamma^t r_t
$$&lt;p&gt;To solve this problem, we can use the value function at the terminal state and add it to the objective function. This learned value function guides plans to good long-term states. So the objective function would be infinite horizon:&lt;/p&gt;
$$
J^{\infty} = \sum_{t=0}^{\infty} \gamma^t r_t = \sum_{t=0}^H \gamma^t r_t + \gamma^H V(s_H)
$$&lt;p&gt;This is another kind of combining decision-time planning (optimization problem) with background planning (learned value function).&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb9.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This can be used in both discrete and continuous action spaces:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb10.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Planning-as-policy-improvement&quot;&gt;Planning as policy improvement&lt;a class=&quot;anchor-link&quot; href=&quot;#Planning-as-policy-improvement&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;So far, we used policy (background) or decision-time planner to make a decision and generate trajectory and actions.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb11.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;But we can combine them and use the planner as policy improvement. We can use the policy to provide some information for the planner. For example, the policy can output its set of trajectories, and the planner can use it as a warm start or initialization to improve upon. We would like to train the policy such that the improvement proposed by the planner has no effect. So the policy trajectory is the best that we can do. I think we can see the planner as a teacher for the policy.&lt;/p&gt;
&lt;p&gt;Some related papers are listed here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Silver et al (2017). Mastering the game of Go without human knowledge.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Levine et al (2014). Guided Policy Search under Unknown Dynamics.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Anthony et al (2017). Thinking Fast and Slow with Deep Learning and Tree Search.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb12.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Implicit-planning&quot;&gt;Implicit planning&lt;a class=&quot;anchor-link&quot; href=&quot;#Implicit-planning&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In addition, to use a planner to improve policy trajectory, we can put the planner as a component &lt;em&gt;inside&lt;/em&gt; the policy network and train end-to-end.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb13.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The advantage of doing this is that the policy network dictates abstract state/action spaces to plan in. But the downside of this is that it requires differentiating through the planning algorithm. But the good news is that multiple algorithms we've seen have been made differentiable and amenable to integrating into such a planner.&lt;/p&gt;
&lt;p&gt;some examples are as follows:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb14.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb15.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb16.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;There are also some works that show the planning could &lt;em&gt;emerge&lt;/em&gt; in generic black-box policy network and model-free RL training.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb17.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What-else-can-models-be-used-for?&quot;&gt;What else can models be used for?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-else-can-models-be-used-for?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Consider we have a model of the world. We can use the model in a lot of different ways like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exploration&lt;/li&gt;
&lt;li&gt;Hierarchical Reasoning&lt;/li&gt;
&lt;li&gt;Adaptivity &amp;amp; Generalization&lt;/li&gt;
&lt;li&gt;Representation Learning&lt;/li&gt;
&lt;li&gt;Reasoning about other agents&lt;/li&gt;
&lt;li&gt;Dealing with partial observability&lt;/li&gt;
&lt;li&gt;Language understanding&lt;/li&gt;
&lt;li&gt;Commonsense reasoning&lt;/li&gt;
&lt;li&gt;and more!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here we're gonna just focus on the first four ways that we can use the model to encourage better behavior.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu1.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Exploration&quot;&gt;Exploration&lt;a class=&quot;anchor-link&quot; href=&quot;#Exploration&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;One of the good things about having a model of the world is that you can reset to any state in the world that you might care about. It's not possible in all environments to reset like a continual learning problem. But if you have the model of the world, you can reset to any state you want.&lt;/p&gt;
&lt;p&gt;We can also consider resetting to intermediate states in the middle of the episode as a starting point. The idea is to keep track of one of the interesting states and does exploration from there. So if you have the world's model, you can again reset to that state and efficiently perform additional explorations.&lt;/p&gt;
&lt;p&gt;You can also reset from the final state rather than the initial state. This can be useful in situations where there is only a single goal state like Rubik's Cube. In this case, there is only one goal but maybe several possible starting states. So it would be useful to reset to the final state and explore backward from there rather than starting from the initial state.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu2.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Another way that models can be used to facilitate exploration is by using &lt;em&gt;intrinsic reward&lt;/em&gt;. In these cases, we want to explore places that we haven't been much so that we can gather data in those locations and learn more about them. One way to identify where we haven't been is to use model prediction error as a proxy. Basically, we learn a world model, then we predict what the next state is going to be and then take action and observe the next state and compare it with the predicted state and calculate the model error. We can then use this prediction error as a signal in the intrinsic reward to encourage the agent to explore the locations we haven't visited often to learn more about them.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu3.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In addition to the above approach, we can also &lt;em&gt;plan to explore&lt;/em&gt;. In &lt;em&gt;POLO&lt;/em&gt; paper, rather than using the error from your prediction model, they use the error across an ensemble of value functions and use it as an intrinsic reward. Actually, at each state, we compute a bunch of different values from our ensemble of value functions, then take softmax over them to give us an optimistic estimate of what the value is going to be. We can use this optimistic value estimate as an intrinsic reward. We plan to maximize this optimistic value estimate, and then this allows us to basically, during planning, identify places that we should direct our behavior towards are more surprising or more interesting.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute intrinsic reward during (decision-time) planning to direct the agent into new regions of state-space&lt;/li&gt;
&lt;li&gt;Intrinsic reward = softmax across an ensemble of value functions&lt;/li&gt;
&lt;/ul&gt;
$$
\hat{V}(s) = log(\sum_{k=1}^K exp(k\hat{V}_{\theta_k}(s)))
$$&lt;p&gt;
&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/003rQ5vUcek?t=3&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Lowrey et al. (2019). Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control. ICLR 2019.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also use the same idea, but instead of using a set of disagreement across on ensemble of value functions, we can compute disagreement across transition functions. Now because we are just using state transitions, this turns into a task agnostic exploration problem. We can then plan where there is a disagreement between our transition functions and direct behavior towards those regions of space to learn a really robust world model. And then use this model of the world to learn new tasks either using zero-shot or few-shot (examples of experience).&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu5.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Finally, another form of exploration is that if we have a model of possible states that we might find ourselves in, not necessarily a transition model but a density model over goals, we can sample possible goals from this density model and then train our agent achieve the goals.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu6.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Hierarchical-reasoning&quot;&gt;Hierarchical reasoning&lt;a class=&quot;anchor-link&quot; href=&quot;#Hierarchical-reasoning&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;A very classic way of doing hierarchical reasoning is what's called &lt;em&gt;task and motion planning (TAMP)&lt;/em&gt; in robotics. You jointly plan symbolically at the task level, and then you also plan in the continuous space and do motion planning at the low-levelyou sort of doing these things jointly in order to solve relatively long-horizon and multi-step tasks. For example, in the following figure, to control a robot arm and to get block $A$ and put it in the washer, wash it, and then put it in storage. In order to do this, you first have to move $C$ and $B$ out of the way and put $A$ into the washer, then move $D$ out of the way and then put $A$ into the storage. By leveraging symbolic representation, like PDDL from the beginning of the post, allows you to be able to jointly solve these hierarchical tasks.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu7.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The other example of this is the OpenAI Rubik's Cube solver. The idea is that you use a high-level symbolic algorithm, Kociemba's algorithm, to generate the solution (plan) of high-level actions, for example, which faces should be rotated, and then you have a low-level neural network policy that generates the controls needed to achieve these high-level actions. This low-level control policy is quite challenging to learn.

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/kVmp0uGtShk&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;OpenAI et al. (2019). Solving Rubik's Cube with a Robot Hand. arXiv.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The question that might arise is that where does this high-level state-space come from?&lt;/p&gt;
&lt;p&gt;We don't want to hand-code symbolically on these high-level roles that we want to achieve. Some model-free works try to answer this, but we focus on some MBRL approaches here for this problem.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu8.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&quot;Subgoal-based-approaches&quot;&gt;Subgoal-based approaches&lt;a class=&quot;anchor-link&quot; href=&quot;#Subgoal-based-approaches&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;We can consider that any state you might find yourself in in the world as a subgoal. We don't want to construct a super long sequence of states to go through, but a small sequence. So the idea would be which states do we pick as a subgoal. Rather than learning a forward state transition model, we can learn a universal value function approximator, $V(s, g)$, that tells us the value of going from state $s$ to goal state $g$. We can train these value functions between our subgoals to estimate how good a particular plan of length $k$ is. A plan of length $k$ is then given by maximizing:&lt;/p&gt;
$$
\text{arg}\max_{\{s_i\}_{i=1}^k} (V(s_0, s_1) + V(s_k, s_g) + \sum_{i=1}^{k-1} V(s_i, s_{i+1}))
$$&lt;p&gt;The figure below shows the idea. If you start from state $s_0$ and you want to go to $s_{\infty}$, you can break up this long plan of length one into a plan of length two by inserting a subgoal. You can do this recursively multiple times to end up with a plan of length $k$ or, in this case, a plan of length three.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu9.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;When we use a planner to identify which of these subgoals we should choose in order to maximize the above equation, in the figure below, you see which white subgoal it is considering as subgoal in order to find a path between the green and the blue points.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/maze.gif&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Nasiriany et al. (2019). Planning with Goal-Conditioned Policies. NeurIPS.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Jurgenson et al. (2019). Sub-Goal Trees -- A Framework for Goal-Directed Trajectory Prediction and Optimization. arXiv.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Parascandolo, Buesing, et al. (2020). Divide-and-Conquer Monte Carlo Tree Search For Goal-Directed Planning. arXiv.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;Skill-based-approaches&quot;&gt;Skill-based approaches&lt;a class=&quot;anchor-link&quot; href=&quot;#Skill-based-approaches&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Here, rather than identifying discrete states as subgoals that we want to try to achieve, what we want to do is to learn a set of skills that sort of fully parametrize the space of possible trajectories that we might want to execute. So, for example, in the Ant environment, a nice parametrization of skills would be to say a particular direction that you want to get to move in. So the approach taken by this paper is to learn a set of skills those outcomes are both (1) easy to predict, so if you train a dynamics model to predict the outcome of executing the skill, and (2) the skills are diverse from one another. That's why you get this nice diversity of the ant moving in different directions. This works very well for zero-shot adaptation to new sequences of goals. As you can see on the bottom, this is an ant going to a few different locations in space, and it is doing this by just pure planning using this set of skills that it is learned during the unsupervised training phase.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn a set of skills whose outcomes are (1) easy to predict and (2) diverse&lt;/li&gt;
&lt;li&gt;Learn dynamics model over skills, and plan with MPC&lt;/li&gt;
&lt;li&gt;Can solve long-horizon sequences of high-level goals with no additional learning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/ant.gif&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Sharma et al. (2020). Dynamics-Aware Unsupervised Discovery of Skills. ICLR.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;representation-Learning&quot;&gt;representation Learning&lt;a class=&quot;anchor-link&quot; href=&quot;#representation-Learning&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Beyond just using models for prediction, they can be used as regularizers for training other types of representations that then you can train a policy on.&lt;/p&gt;
&lt;p&gt;One way is to learn a model as an &lt;em&gt;auxiliary loss&lt;/em&gt;. For example, if you have an A2C algorithm and add an auxiliary loss to predict the reward it's gonna achieve, in some cases, you can get a large improvement in performance by just adding this auxiliary loss. By considering this loss during training, we are actually forcing it to learn the future and capture the structure of the world, which is useful. We also don't use this learned model in planning and just for representation learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Jaderberg et al. (2017). Reinforcement learning with unsupervised auxiliary tasks. ICLR 2017.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The other same idea is to use a &lt;em&gt;contrastive loss&lt;/em&gt;, like CPC paper (below), that tries to predict what observations it might encounter in the future, and by adding this additional loss during training, we see improvement in performance.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;van den Oord, Li, &amp;amp; Vinyals (2019). Representation Learning with Contrastive Predictive Coding. arXiv.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another idea is &lt;em&gt;plannable representations&lt;/em&gt; that make it much easier to plan in. For example, if we are in a continuous space, we can discretize it in an intelligent way that might make it easy to use some of these discrete search methods, like MCTS, to rapidly come up with a good plan of actions. Or maybe we can come up with a representation for our state space such that moving along a direction in the latent state space corresponds to planning. So you can basically just interpolate between states in order to come up with a plan.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn an embedding of states that is easier to plan in, e.g.&lt;ul&gt;
&lt;li&gt;Discretized&lt;/li&gt;
&lt;li&gt;States that can be transitioned between should be near to each other in latent space!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Related to notions in hierarchical RL (state abstraction)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu10.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Corneil et al. (2018). Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation. ICML.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Kurutach et al. (2018). Learning Plannable Representations with Causal InfoGAN. NeurIPS.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Adaptivity-&amp;amp;-generalization&quot;&gt;Adaptivity &amp;amp; generalization&lt;a class=&quot;anchor-link&quot; href=&quot;#Adaptivity-&amp;amp;-generalization&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Models of the world can also be used for fast adaptation and generalization.&lt;/p&gt;
&lt;p&gt;The world can be changed in two different ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Change in rewards&lt;/strong&gt;. So we're being asked to do a new task, but the dynamics are the same.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Change in dynamics&lt;/strong&gt;. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Based on the above changes, we can do different things in response to them.&lt;/p&gt;
&lt;p&gt;In a model-free approach, we just adapt to the policy. But this tends to be relatively slow because it's hard to quickly adapt changes in rewards to the same dynamics and vice versa because they are sort of entangled with each other.&lt;/p&gt;
&lt;p&gt;If we have an explicit model of the world, we can update our behavior differently. One option would be that we can adapt the planner, but we can also adapt the model itself, or we can do both.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu11.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&quot;Adapting-the-planner-in-new-states&quot;&gt;Adapting the planner in new states&lt;a class=&quot;anchor-link&quot; href=&quot;#Adapting-the-planner-in-new-states&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;A pre-trained policy may not generalize to all states (especially in combinatorial spaces). So some states that we might find ourselves in might be required harder or more reasoning, and others may require less.
We have to try to detect when planning is required, and they adapt the amount of planning depending on the difficulty of the task. For example, in the following gifs, in the upper case, the n-body agent can easily solve the task and reach the center's goal using just a couple of simulations. But in the bottom case, it is much harder to reason about because it starts on one of the planets, which requires many more simulations. We can adaptively change this amount of computation as needed. Save the computation on easy scenes and then spend it more on the hard ones.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/easy.gif&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/hard.gif&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Hamrick et al. (2017). Metacontrol for adaptive imagination-based optimization. ICLR 2017.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Pascanu, Li, et al. (2017). Learning model-based planning from scratch. arXiv.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;Adapting-the-planner-to-new-rewards&quot;&gt;Adapting the planner to new rewards&lt;a class=&quot;anchor-link&quot; href=&quot;#Adapting-the-planner-to-new-rewards&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Here is another same idea in a life-long learning setup where the reward can suddenly change, and either the agents can observe the change in the reward, or they just have to infer the reward has changed. Because of changes in reward, it needs more planning because the prior policy is less reliable, and more planning allows you to better explore these different options for the reward function. In the video below, as you can see in the bottom agent after the reward is changed, the agent needs to do more planning to have a nice movement compared to the other two agents. 

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/3T3QuKregt0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Lu, Mordatch, &amp;amp; Abbeel (2019). Adaptive Online Planning for Continual Lifelong Learning. NeurIPS Deep RL Workshop.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;Adapting-the-model-to-new-dynamics&quot;&gt;Adapting the model to new dynamics&lt;a class=&quot;anchor-link&quot; href=&quot;#Adapting-the-model-to-new-dynamics&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;For the times that the dynamics change, it could be very useful to adapt the model. One way to approach this is to train the model using the meta-learning objective so that during training, you're always training it to adapt  to a slightly different environment around you, and at the test time, you actually see a new unobserved environment that you never saw before, you can take a few gradient steps to adapt the model to deal with these new situations. Here is an example where the agent, half cheetah, has been trained to walk along some terrain, but it's never seen as a little hill before. Therefore, the baseline methods that cannot adapt their model cannot get the agent to go up the hill, where this meta-learning version can get the cheetah to go up the hill. 

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/ejG2nzCNdZ8?t=144&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Nagabandi et al. (2019). Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning. ICLR.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What's-missing-from-model-based-methods?&quot;&gt;What's missing from model-based methods?&lt;a class=&quot;anchor-link&quot; href=&quot;#What's-missing-from-model-based-methods?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Humans are ultimate model-based reasoners and we can learn a lot from how we build and deploy models of the world. - &lt;strong&gt;Motor control&lt;/strong&gt;: forward kinematics models in the cerebellum. We have a lot of motor systems that are making predictions about how our muscles are going to affect the kinematics of our bodies.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Language comprehension&lt;/strong&gt;: we build models of what is being communicated in order to understand.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pragmatics&lt;/strong&gt;: we construct models of listener &amp;amp; speaker beliefs in order to try to understand what is tryingto be communicated.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Theory of mind&lt;/strong&gt;: we construct models of other agents beliefs and behavior in order to predict what they are going to do.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decision making&lt;/strong&gt;: model-based reinforcement learning&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Intuitive physics&lt;/strong&gt;: forward models of physical dynamics&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scientific reasoning&lt;/strong&gt;: mental models of scientific phenomena&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Creativity&lt;/strong&gt;: being able to imagine novel combinations of things&lt;/li&gt;
&lt;li&gt; and much more!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more you can see the following reference:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Markman, Klein, &amp;amp; Suhr (2008). Handbook of Imagination and Mental Simulation.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Abraham (2020). The Cambridge Handbook of the Imagination.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;If you look at the mentioned different domains, where people are engaging a model based reasoning, a few themes emerge that could be really useful in thinking about how to continue to develop our models in MBRL.&lt;/p&gt;
&lt;p&gt;Humans use their models of the world in ways that are compositional, causal, incomplete, adaptive, efficient, and abstract. Taking these ideas and trying to distill them into MBRL enables us to do&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;faster planning&lt;/li&gt;
&lt;li&gt;have systems with higher tolerance to model error&lt;/li&gt;
&lt;li&gt;can be scaled to much much harder problems.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This will lead us to more robust real-world applications and better common sense reasoning.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis1.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Compositionality&quot;&gt;Compositionality&lt;a class=&quot;anchor-link&quot; href=&quot;#Compositionality&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Humans are much much stronger than MBRL algorithms that we have in compositionality.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis2.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&quot;Causality&quot;&gt;Causality&lt;a class=&quot;anchor-link&quot; href=&quot;#Causality&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis3.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&quot;Incompleteness&quot;&gt;Incompleteness&lt;a class=&quot;anchor-link&quot; href=&quot;#Incompleteness&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Another facet of human model based reasoning in the fact that we can reason about incomplete models, but reason about them in very tich ways. This is in contrast to model-based RL which if we have model error, it would be a huge deal and are very far from human capabilities.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis4.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&quot;Adaptivity&quot;&gt;Adaptivity&lt;a class=&quot;anchor-link&quot; href=&quot;#Adaptivity&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The way that we (humans) use our models is also incredibly adaptive. We can rapidly assemble our compositional knowledge into on-the-fly models that are adapted to the current task. Then we quickly solve these models, leveraging both mental simulation &amp;amp; (carefully chosen) real experience&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/adapt.gif&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Allen, Smith, &amp;amp; Tenenbaum (2019). The tools challenge: Rapid trial-and-error learning in physical problem solving. CogSci 2019.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Dasgupta, Smith, Schulz, Tenenbaum, &amp;amp; Gershman (2018). Learning to act by integrating mental simulations and physical experiments. CogSci 2018.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Efficiency&quot;&gt;Efficiency&lt;a class=&quot;anchor-link&quot; href=&quot;#Efficiency&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Humans' model-based reasoning is also very efficient. Figure below illustrates how much of an improvement Alpha-zero was over former state of the art chess engine which requires a tens of millions of moves during simulation. Whereas Alpha-zero only needs tens of thousands. But again it is not comparable to human grandmaster, which only requires hundreds of moves. So we need to continue to develop planners that are able to sort of leverage our models as quickly and as efficiently as possible towards this type of efficiency.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis5.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&quot;Abstraction&quot;&gt;Abstraction&lt;a class=&quot;anchor-link&quot; href=&quot;#Abstraction&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The final feature of humans' ability to use models of the world is abstraction. We go through all of different levels of abstraction as we're planning over multiple timescales, over multiple forms of state abstraction, and we move up and down different forms of abstraction as needed and so we ideally want integrated agents that could do the same.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis6.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Conclusion&quot;&gt;Conclusion&lt;a class=&quot;anchor-link&quot; href=&quot;#Conclusion&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In this tutorial, we discussed what it means to have a model of the world and different types of models that you can learn. We also talked about where the model fits into the RL loop. We talked about landscape of model-based methods and some practical considerations that we care about when integrating models into the loop. We also saw how we can try to improve models by looking towards human cognition.&lt;/p&gt;
&lt;h3 id=&quot;Ethical-and-broader-impacts&quot;&gt;Ethical and broader impacts&lt;a class=&quot;anchor-link&quot; href=&quot;#Ethical-and-broader-impacts&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Because MBRL inherits methods both from model-free RL and model learning in general, it inherits the problems from both of them too.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis7.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" /><media:content medium="image" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Online Reinforcement Learning</title><link href="https://kargarisaac.github.io/blog/rl/2020/06/15/online-rl.html" rel="alternate" type="text/html" title="Online Reinforcement Learning" /><published>2020-06-15T00:00:00-05:00</published><updated>2020-06-15T00:00:00-05:00</updated><id>https://kargarisaac.github.io/blog/rl/2020/06/15/online-rl</id><content type="html" xml:base="https://kargarisaac.github.io/blog/rl/2020/06/15/online-rl.html">&lt;h1 id=&quot;online-reinforcement-learning&quot;&gt;Online Reinforcement Learning&lt;/h1&gt;

&lt;p&gt;In this post I will overview different single and multi-agent online Reinforcement Learning (RL) algorithms. By &lt;strong&gt;online&lt;/strong&gt; I mean the algorithms that can interact with an environment and collect data, in contrast to offline RL. I will update this post and add algorithms periodically.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/rl-diagram.png&quot; alt=&quot;RL diagram&quot; /&gt; &lt;em&gt;RL diagram&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Here are some resources to learn more about RL!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;David Silvers &lt;a href=&quot;https://www.youtube.com/playlist?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&quot;&gt;course&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CS287 at UC Berkeley - Advanced Robotics &lt;a href=&quot;https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNBPJdt8WamRAt4XKc639wF&quot;&gt;course&lt;/a&gt; - Instructor: Pieter Abbeel&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CS 285 at UC Berkeley - Deep Reinforcement Learning &lt;a href=&quot;http://rail.eecs.berkeley.edu/deeprlcourse/&quot;&gt;course&lt;/a&gt; - Instructor: Sergey Levine&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CS234 at Stanford - Reinforcement Learning &lt;a href=&quot;http://web.stanford.edu/class/cs234/index.html&quot;&gt;course&lt;/a&gt; - Instructor: Emma Brunskill&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CS885 at University of Waterloo - Reinforcement Learning &lt;a href=&quot;https://www.youtube.com/playlist?list=PLdAoL1zKcqTXFJniO3Tqqn6xMBBL07EDc&quot;&gt;course&lt;/a&gt; - Instructor: Pascal Poupart&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Arthur Julianis &lt;a href=&quot;https://medium.com/@awjuliani&quot;&gt;posts&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Jonathan Huis &lt;a href=&quot;https://medium.com/@jonathan_hui/rl-deep-reinforcement-learning-series-833319a95530&quot;&gt;posts&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A Free &lt;a href=&quot;https://simoninithomas.github.io/deep-rl-course/&quot;&gt;course&lt;/a&gt; in Deep Reinforcement Learning from beginner to expert by Thomas Simonni&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;single-agent&quot;&gt;Single agent&lt;/h1&gt;

&lt;h3 id=&quot;dqn&quot;&gt;DQN&lt;/h3&gt;

&lt;p&gt;We will take a look at DQN with experience replay buffer and the target network.&lt;/p&gt;

&lt;p&gt;DQN is a value-based method. It means that we try to learn a value function and then use it to achieve the policy. In DQN we use a neural network as a function approximator for our value function. It gets the state as input and outputs the value for different actions in that state. These values are not limited to be between zero and one, like probabilities, and can have other values based on the environment and the reward function we define.&lt;/p&gt;

&lt;p&gt;DQN is an off-policy method which means that we are using data from old policies, the data that we gather in every interaction with the environment and save it in the experience replay buffer, to sample from it later and train the network. The size of the replay buffer should be large enough to reduce the $i.i.d$ property between data that we sample from it.&lt;/p&gt;

&lt;p&gt;To use DQN, the action should be discrete. We can use it for continuous action spaces by discretizing the action space, but its better to use other techniques that can handle continuous action spaces such as Policy Gradients.
First, lets see the algorithms sudo code:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/dqn.png&quot; alt=&quot;DQN algorithm&quot; title=&quot;DQN algorithm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this algorithm, we have experience replay buffer and a target network with a different set of parameters that will be updated every $C$ steps. These tricks help to get a better and more stable method rather than pure DQN. There are a lot of improvements for DQN and we will see some of them in the next posts too.&lt;/p&gt;

&lt;p&gt;First, we initialize the weights of both networks and then start from the initial state s and take action a with epsilon-greedy policy. In the epsilon-greedy policy, we select an action a randomly or using the Q-network. Then we execute the selected action and get the next state, reward, and the done values from the environment and save them in our replay buffer. Then we sample a random batch from the replay buffer and calculate target based on the Bellman equation in the above picture and use MSE loss and gradient descent to update the network weights. We will update the weights of our target network every $C$ steps.&lt;/p&gt;

&lt;p&gt;In the training procedure, we use epsilon decay. It means that we consider a big value for epsilon, such as $1$. Then during the training procedure, as we go forward, we reduce its value to something like $0.02$ or $0.05$, based on the environment. It will help the agent to do more exploration in the first steps and learn more about the environment. Its better to have some exploration always. Thats a trade-off between exploration-exploitation.
In test time, we have to use a greedy policy. It means we have to select the action with the highest value, not randomly anymore (set epsilon to zero actually).&lt;/p&gt;

&lt;h3 id=&quot;reinforce&quot;&gt;REINFORCE&lt;/h3&gt;

&lt;p&gt;REINFORCE is a Monte-Carlo Policy Gradient (PG) method. In PGs, we try to find a policy to map the state into action directly.&lt;/p&gt;

&lt;p&gt;In value-based methods, we find a value function and use it to find the optimal policy. Policy gradient methods can be used for stochastic policies and continuous action spaces. If you want to use DQN for continuous action spaces, you have to discretize your action space. This will reduce the performance and if the number of actions is high, it will be difficult and impossible. But REINFORCE algorithms can be used for discrete or continuous action spaces. They are on-policy because they use the samples gathered from the current policy.&lt;/p&gt;

&lt;p&gt;There are different versions of REINFORCE. The first one is without a baseline. It is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/reinforce.png&quot; alt=&quot;reinforce algorithm&quot; title=&quot;from Sutton Barto book: Introduction to Reinforcement Learning&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this version, we consider a policy (here a neural network) and initialize it with some random weights. Then we play for one episode and after that, we calculate discounted reward from each time step towards the end of the episode. This discounted reward (G in the above sudo code) will be multiplied by the gradient. This G is different based on the environment and the reward function we define. For example, consider that we have three actions. The first action is a bad action and the other two actions are some good actions that will cause more future discounted rewards. If we have three positive G values for three different actions, we are pushing the network towards all of them. Actually, we push the network towards action number one slightly and towards others more. Now consider we have one negative G value for the first action and two G values for the other two actions. Here we are pushing the network far from the first action and towards the other two actions. You see?! the value of G and its sign is important. It guides our gradient direction and its step size. To solve such problems, one way is to use baseline. This will reduce the variance and accelerate the learning procedure. For example, subtract the value of the state from it, or normalize it with the mean and variance of the discounted reward of the current episode. You can see the sudo code for REINFORCE with baseline in the following picture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/reinforce2.png&quot; alt=&quot;reinforce algorithm&quot; title=&quot;from Sutton Barto book: Introduction to Reinforcement Learning&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this version, first, we initialize the policy and value networks. It is possible to use two separate networks or a multi-head network with a shared part. Then we play an episode and calculate the discounted reward from every step until the end of the episode (reward to go). Then subtract the value (from the learned neural net) for that state from the discounted reward (REINFORCE with baseline) and use it to update the weights of value and policy networks. Then generate another episode and repeat the loop.&lt;/p&gt;

&lt;p&gt;In the Sutton&amp;amp;Barto book, they do not consider the above algorithm as actor-critic (another RL algorithm that we will see in the next posts). It learns the value function but it is not used as a critic! I think it is because we do not use the learned value function (critic) in the first term of the policy gradient rescaler (for bootstrapping) to tell us how good is our policy or action in every step or in a batch of actions (in A2C and A3C we do the update every t_max step). In REINFORCE we update the network at the end of each episode.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The REINFORCE method follows directly from the policy gradient theorem. Adding a state-value function as a baseline reduces REINFORCEs variance without introducing bias. Using the state-value function for bootstrapping introduces bias but is often desirable for the same reason that bootstrapping TD methods are often superior to Monte Carlo methods (substantially reduced variance). The state-value function assigns credit to  critizes  the policys action selections, and accordingly the former is termed the critic and the latter the actor, and these overall methods are termed actorcritic methods.
Actorcritic methods are sometimes referred to as advantage actorcritic (A2C) methods in the literature.&lt;/em&gt;
[Sutton&amp;amp;Barto  second edition]&lt;/p&gt;

&lt;p&gt;I think Monte-Carlo policy gradient and Actor-Critic policy gradient are good names as I saw in the slides of David Silver course.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/reinforce3.png&quot; alt=&quot;reinforce algorithm&quot; title=&quot;source: https://www.youtube.com/watch?v=KHZVXao4qXs&amp;amp;list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&amp;amp;index=7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I also saw the following slide from the Deep Reinforcement Learning and Control course (CMU 10703) at Carnegie Mellon University:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/reinforce4.png&quot; alt=&quot;reinforce algorithm&quot; title=&quot;source: https://www.andrew.cmu.edu/course//10-703/slides/Lecture_PG-NatGrad-10-8-2018.pdf&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here they consider every method that uses value function (V or Q) as actor-critic and if you just consider reward to go in the policy gradient rescaler, it is REINFORCE. The policy evaluation by the value function can be TD or MC.&lt;/p&gt;

&lt;p&gt;Summary of the categorization:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Vanilla REINFORCE or Policy gradient  we use G as gradient rescaler.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;REINFORCE with baseline  we use $\frac{G-mean(G)}{std(G)}$ or $(G-V)$ as gradient rescaler. We do not use $V$ in $G$. $G$ is only the reward to go for every step in the episode  $G_t = r_t + \gamma r_{t+1} +  $&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Actor-Critic  we use $V$ in the first term of gradient rescaler and call it Advantage ($A$):&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$A_t = Q(s_t, a_t) - V(s_t)$&lt;/p&gt;

&lt;p&gt;$A_t = r_t + \gamma V_{s_{t+1}} - V_{s_t}$  for one-step&lt;/p&gt;

&lt;p&gt;$A_t = r_t + \gamma r_{t+1} + \gamma^2 V_{s_{t+2}} - V_{s_t}$  for 2-step&lt;/p&gt;

&lt;p&gt;and so on.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In Actor-Critics you can do the update each $N$ step based on your task. This $N$ can be less than an episode.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Anyway, lets continue.&lt;/p&gt;

&lt;p&gt;This algorithm can be used for either discrete or continuous action spaces. In discrete action spaces, it will output a probability distribution over action, which means that the activation function of the output layer is a softmax. For exploration-exploitation, it samples from the actions based on their probabilities. Actions with higher probabilities have more chances to be selected.&lt;/p&gt;

&lt;p&gt;In continuous action spaces, the output will not have any softmax. Because the output is a mean for a normal distribution. We consider one neuron for each action and it can have any value. In fact, the policy is a normal distribution and we calculate its mean by a neural network. The variance can be fixed or decrease over time or can be learned. You can consider it as a function of the input state, or define it as a parameter that can be learned by gradient descent. If you want to learn the sigma too, you have to consider the number of actions. For example, if we want to map the front view image of a self-driving car into steering and throttle-brake, we have two continuous actions. So we have to have two mean and two variance for these two actions. During training, we sample from this normal distribution for exploration of the environment, but in the test, we only use the mean as action.&lt;/p&gt;

&lt;h3 id=&quot;a2c&quot;&gt;A2C&lt;/h3&gt;

&lt;h3 id=&quot;a3c&quot;&gt;A3C&lt;/h3&gt;

&lt;h3 id=&quot;ppo&quot;&gt;PPO&lt;/h3&gt;

&lt;h3 id=&quot;ddpg&quot;&gt;DDPG&lt;/h3&gt;

&lt;p&gt;This algorithm is from the &lt;em&gt;Continuous Control with Deep Reinforcement Learning&lt;/em&gt; &lt;a href=&quot;https://arxiv.org/pdf/1509.02971.pdf&quot;&gt;paper&lt;/a&gt; and uses the ideas from deep q-learning in the continuous action domain and is a model-free method based on the deterministic policy gradient.&lt;/p&gt;

&lt;p&gt;In Deterministic Policy Gradient (DPG), for each state, we have one clearly defined action to take (the output of policy is one value for action and for exploration we add a noise, normal noise for example, to the action). But in Stochastic Gradient Descent, we have a distribution over actions (the output of policy is mean and variance of a normal distribution) and sample from that distribution to get the action, for exploration. In another term, in stochastic policy gradient, we have a distribution with mean and variance and we draw a sample from that as an action. When we reduce the variance to zero, the policy will be deterministic.&lt;/p&gt;

&lt;p&gt;When the action space is discrete, such as q-learning, we get the max over q-values of all actions and select the best action. But in continuous action spaces, you cannot apply q-learning directly, because in continuous spaces finding the greedy policy requires optimization of $a_t$ at every time-step and would be too slow for large networks and continuous action spaces. Based on the proposed equation in the reference paper, here we approximate &lt;em&gt;max Q(s, a)&lt;/em&gt; over actions with &lt;em&gt;Q(a, (s))&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In DDPG, they used function approximators, neural nets, for both action-value function $Q$ and deterministic policy function $\mu$. In addition, DDPG uses some techniques for stabilizing training, such as updating the target networks using soft updating for both $\mu$ and $Q$. It also uses batch normalization layers, noise for exploration, and a replay buffer to break temporal correlations.&lt;/p&gt;

&lt;p&gt;This algorithm is an actor-critic method and the network structure is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/ddpg_post/ddpg_diagram.jpg&quot; alt=&quot;DDPG diagram&quot; title=&quot;DDPG diagram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;First, the policy network gets the state and outputs the action mean vector. This will be a vector of mean values for different actions. For example, in a self-driving car, there are two continuous actions: steering and acceleration&amp;amp;braking (one continuous value between $-x$ to $x$, the negative values are for braking and positive values are for acceleration). So we will have two mean for these two actions. To consider exploration, we can use Ornstein-Uhlenbeck or normal noise and add it to the action mean vector in the training phase. In the test phase, we can use the mean vector directly without any added noise. Then this action vector will be concatenated with observation and fed into the $Q$ network. The output of the $Q$ network will be one single value as a state-action value. In DQN, because it had discrete action space, we had multiple state-action values for each action, but here because the action space is continuous, we feed the actions into the $Q$ network and get one single value as the state-action value.&lt;/p&gt;

&lt;p&gt;Finally, the sudo code for DDPG is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/ddpg_post/ddpg_algorithm.jpg&quot; alt=&quot;DDPG algorithm&quot; title=&quot;DDPG algorithm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To understand the algorithm better, its good to try to implement it and play with its parameters and test it in different environments. Here is a good implementation in PyTorch that you can start with &lt;a href=&quot;https://github.com/higgsfield/RL-Adventure-2/blob/master/5.ddpg.ipynb&quot;&gt;this&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also found the Spinningup implementation of DDPG very clear and understandable too. You can find it &lt;a href=&quot;https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ddpg/ddpg.py&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For POMDP problems, it is possible to use LSTMs or any other RNN layers to get a sequence of observations. It needs a different type of replay buffer for sequential data.&lt;/p&gt;

&lt;h3 id=&quot;sac&quot;&gt;SAC&lt;/h3&gt;

&lt;h3 id=&quot;ape-x&quot;&gt;Ape-X&lt;/h3&gt;

&lt;h3 id=&quot;r2d2&quot;&gt;R2D2&lt;/h3&gt;

&lt;h3 id=&quot;impala&quot;&gt;IMPALA&lt;/h3&gt;

&lt;h3 id=&quot;never-give-up&quot;&gt;Never Give-Up&lt;/h3&gt;

&lt;h3 id=&quot;agent57&quot;&gt;Agent57&lt;/h3&gt;

&lt;h1 id=&quot;multi-agent&quot;&gt;Multi-Agent&lt;/h1&gt;

&lt;p&gt;In Multi-Agent Reinforcement Learning (MARL)problems, there are several agents who usually have their own private observation and want to take an action based on that observation. This observation is local and different from the full state of the environment in that time-step. The other problem that we face in such environments is the non-stationary problem because all agents are learning and their behavior would be different during training as they learn to act differently.&lt;/p&gt;

&lt;p&gt;To solve this problem, the most naive approach is to use single-agent RL algorithms for each agent and treat other agents as part of the environment. Some methods like Independent Q-Learning (IQL) work fine in some multi-agent RL problems in practice but there is no guarantee for them to converge. In IQL, each agent has one separate action-value function that gets the agents local observation to select its action based on that. It is also possible to use additional inputs like previous actions as input. Usually, in partially observable environments, we use RNNs to consider a history of several sequential observation-actions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/marl1.png&quot; alt=&quot;MARL&quot; title=&quot; source: https://arxiv.org/pdf/1706.05296.pdf &quot; /&gt;&lt;/p&gt;

&lt;p&gt;The other approach is to have a fully centralised method to learn and act in a centralised fashion. We can consider this type as a big single-agent problem. This approach is also valid in some problems that you dont need decentralised execution. For example for traffic management or traffic light management, it is possible to use such approaches.&lt;/p&gt;

&lt;p&gt;There is one more case that is somewhere between the previous two ones: centralised training and decentralised execution. Usually in the training procedure, because we train agents in a simulation environment or in a lab, we have access to the full state and information in the training phase. So it is better to use this knowledge. On the other hand, the learned policy should be decentralised in some environments and agents cannot have access to the full state during the execution phase. So having algorithms to use the available knowledge in the training phase and learn a policy that is not dependent on the full state in the execution time is necessary. Here we focus on the last case.&lt;/p&gt;

&lt;p&gt;There are several works that try to propose such an algorithm and can be divided into two groups:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;value-based methods like Value Decomposition Networks (VDN) and QMIX&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;actor-critic methods like MADDPG and COMA&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;value-based-methods&quot;&gt;VALUE-BASED METHODS&lt;/h2&gt;

&lt;p&gt;These approaches try to propose a way to be able to use value-based methods like Q-learning and train them in a centralised way and use them for decentralised execution.&lt;/p&gt;

&lt;h3 id=&quot;vdn&quot;&gt;VDN&lt;/h3&gt;

&lt;p&gt;This work proposes a way to have separate action-value functions for multiple agents and learn them by just one shared team reward signal. The joint action-value function is a linear summation of all action-value functions of all agents. Actually, by using a single shared reward signal, it tries to learn decomposed value functions for each agent and use it for decentralised execution.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/vdn1.png&quot; alt=&quot;VDN&quot; title=&quot; source: https://arxiv.org/pdf/1706.05296.pdf &quot; /&gt;&lt;/p&gt;

&lt;p&gt;Consider a case with 2 agents, the reward would be:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/vdn2.png&quot; alt=&quot;VDN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then the total $Q$ function is:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/vdn3.png&quot; alt=&quot;VDN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is using the same Bellman equation to standard q-learning approach and just replaces $Q$ in that equation with the new $Q$ value.&lt;/p&gt;

&lt;h3 id=&quot;qmix&quot;&gt;QMIX&lt;/h3&gt;

&lt;p&gt;QMIX is somehow an extension to value decomposition networks (VDN) but tries to mix the Q-value of different agents in a nonlinear way. They use global state $s_t$ as input to hypernetworks to generate weights and biases of the mixing network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/qmix1.png&quot; alt=&quot;QMIX&quot; title=&quot; source: https://arxiv.org/pdf/1803.11485.pdf&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here again, the equation to update the weights is the standard Bellman equation in which the $Q$ is replaced with $Q_tot$ in the above figure.&lt;/p&gt;

&lt;h2 id=&quot;actor-critic-based-methods&quot;&gt;ACTOR-CRITIC BASED METHODS&lt;/h2&gt;

&lt;p&gt;This group of methods tries to use actor-critic architecture to do centralised training and decentralised execution. Usually, they use the full state and additional information which are available in the training phase in the critic network to generate a richer signal for the actor.&lt;/p&gt;

&lt;h3 id=&quot;maddpg&quot;&gt;MADDPG&lt;/h3&gt;

&lt;p&gt;Multi-Agent DDPG (MADDPG) is a method to use separate actors and critics for each agent and train the critic in a centralised way and use the actor in execution. So each agent has one actor and one critic. The actor has access to its own action-observation data and is trained by them and the critic has access to observation and action of all agents and is trained by all of them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/maddpg.png&quot; alt=&quot;MADDPG&quot; title=&quot; source: https://arxiv.org/pdf/1706.02275.pdf &quot; /&gt;&lt;/p&gt;

&lt;p&gt;The centralised action-value function for each agent can be written as:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/maddpg2.png&quot; alt=&quot;MADDPG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And the gradient can be written as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/maddpg3.png&quot; alt=&quot;MADDPG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you see, the policy is conditioned on the observation of the agent itself, o_i, and the critic is conditioned on the full state and actions of all agents.
This separate critic for each agent allows us to have agents with different rewards, cooperative or competitive behaviors.&lt;/p&gt;

&lt;h3 id=&quot;coma&quot;&gt;COMA&lt;/h3&gt;

&lt;p&gt;The talk can be found &lt;a href=&quot;https://www.youtube.com/watch?v=3OVvjE5B9LU&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Counterfactual Multi-Agent (COMA) policy gradient is a method for cooperative multi-agent systems and uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents policies. In addition, to address the problem of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agents action, while keeping the other agents actions fixed. The idea comes from difference rewards, in which each agent learns from a shaped reward $D_a = r(s, u)  r(s,(u^{-a}, c_a))$ that compares the global reward to the reward received when the action of agent $a$ is replaced with a default action $c_a$.&lt;/p&gt;

&lt;p&gt;COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/coma.png&quot; alt=&quot;COMA&quot; title=&quot; source: https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/17193 &quot; /&gt;&lt;/p&gt;

&lt;p&gt;For each agent $a$, we can then compute an advantage function that compares the Q-value for the current action $u^a$ to a counterfactual baseline that marginalizes out $u^a$, while keeping the other agents actions $u^{-a}$ fixed:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/coma2.png&quot; alt=&quot;COMA&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In contrast to MADDPG, COMA is an on-policy approach and has only one critic network.&lt;/p&gt;</content><author><name></name></author><summary type="html">Online Reinforcement Learning</summary></entry><entry><title type="html">AlphaGo - Mastering the game of Go with deep neural networks and tree search</title><link href="https://kargarisaac.github.io/blog/jupyter/2020/04/12/AlphaGo.html" rel="alternate" type="text/html" title="AlphaGo - Mastering the game of Go with deep neural networks and tree search" /><published>2020-04-12T00:00:00-05:00</published><updated>2020-04-12T00:00:00-05:00</updated><id>https://kargarisaac.github.io/blog/jupyter/2020/04/12/AlphaGo</id><content type="html" xml:base="https://kargarisaac.github.io/blog/jupyter/2020/04/12/AlphaGo.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-12-AlphaGo.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_0.png?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_1.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_2.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_3.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_4.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_5.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_6.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_7.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_8.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Thats it for the first one. In the next post, I will review the AlphaGo Zero paper.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Isaac Kargar</name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kargarisaac.github.io/blog/_notebooks/my_icons/alphago/alphago_0.png" /><media:content medium="image" url="https://kargarisaac.github.io/blog/_notebooks/my_icons/alphago/alphago_0.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>